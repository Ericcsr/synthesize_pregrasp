{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pytorch_kinematics as pk\n",
    "\n",
    "# Relative to the object\n",
    "def getRelativePose(tip_pose, obj_pose, obj_orn):\n",
    "    r = tip_pose - obj_pose\n",
    "    orn = torch.tensor([obj_orn[3], obj_orn[0], obj_orn[1], obj_orn[2]])\n",
    "    orn_inv = pk.quaternion_invert(orn)\n",
    "    r = pk.quaternion_apply(orn_inv, torch.from_numpy(r)).numpy()\n",
    "    return r\n",
    "\n",
    "def getWorldPose(rel_pos, obj_pose, obj_orn):\n",
    "    orn = torch.tensor([obj_orn[3], obj_orn[0], obj_orn[1], obj_orn[2]])\n",
    "    r = pk.quaternion_apply(orn, torch.from_numpy(rel_pos)).numpy()\n",
    "    return r + obj_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4, 3)\n",
      "(150, 7)\n"
     ]
    }
   ],
   "source": [
    "fig_tip_pose = np.load(\"../data/tip_data/redo_2_50_0.8_tip_poses.npy\")\n",
    "obj_poses = np.load(\"../data/object_poses/redo_2_50_0.8_object_poses.npy\")\n",
    "print(fig_tip_pose.shape)\n",
    "print(obj_poses.shape)\n",
    "fig_tip_pose[:50,3] += 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_vec = np.array([0, 0, -0.05])\n",
    "for i in range(50):\n",
    "    rel_pose = getRelativePose(fig_tip_pose[i,1], obj_poses[i,:3], obj_poses[i,3:])\n",
    "    rel_pose += delta_vec\n",
    "    fig_tip_pose[i,1] = getWorldPose(rel_pose, obj_poses[i,:3], obj_poses[i,3:])\n",
    "\n",
    "rel_pose = getRelativePose(fig_tip_pose[49,1], obj_poses[49,:3], obj_poses[49,3:])\n",
    "fig_tip_pose[50,1] = getWorldPose(rel_pose, obj_poses[50,:3], obj_poses[50,3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../data/tip_data/redo_2_50_0.8_tip_poses.npy\", fig_tip_pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_tip_pose = np.load(\"../data/tip_data/warm_start_80_0.8_tip_poses.npy\")\n",
    "obj_poses = np.load(\"../data/object_poses/warm_start_80_0.8_object_poses.npy\")\n",
    "rel_pose = getRelativePose(fig_tip_pose[99,2], obj_poses[99,:3], obj_poses[99,3:])\n",
    "fig_tip_pose[100,2] = getWorldPose(rel_pose, obj_poses[100,:3], obj_poses[100,3:])\n",
    "np.save(\"../data/tip_data/warm_start_80_0.8_tip_poses_mod.npy\", fig_tip_pose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "07c80fb706c8613e3e993fd79155accb8ca952c883d2b3c2120d0d77d7b52932"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('nimble_dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
