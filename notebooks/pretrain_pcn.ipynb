{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: May 20 2022 19:44:17\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from neurals.network import PCN, PointNet2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcds = []\n",
    "pcds.append(o3d.io.read_point_cloud(\"data/seeds/pointclouds/pose_0_pcd.ply\"))\n",
    "pcds.append(o3d.io.read_point_cloud(\"data/seeds/pointclouds/pose_1_pcd.ply\"))\n",
    "pcds.append(o3d.io.read_point_cloud(\"data/seeds/pointclouds/pose_2_pcd.ply\"))\n",
    "pcds.append(o3d.io.read_point_cloud(\"data/seeds/pointclouds/pose_3_pcd.ply\"))\n",
    "pcds.append(o3d.io.read_point_cloud(\"data/seeds/pointclouds/pose_4_pcd.ply\"))\n",
    "pcds.append(o3d.io.read_point_cloud(\"data/seeds/pointclouds/pose_5_pcd.ply\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n"
     ]
    }
   ],
   "source": [
    "points_list = []\n",
    "for pcd in pcds:\n",
    "    for i in range(100):\n",
    "        points = np.asarray(pcd.points)\n",
    "        points_index = np.random.choice(len(points), 1024, replace=False)\n",
    "        points = points[points_index]\n",
    "        points_list.append(points)\n",
    "print(len(points_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, points_list):\n",
    "        self.pcds = np.asarray(points_list)\n",
    "        self.mapping = []\n",
    "        for i in range(6):\n",
    "            self.mapping += [i] * 100\n",
    "        self.mapping = np.array(self.mapping).astype(np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mapping)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        mapping = self.mapping[index]\n",
    "        pcd = self.pcds[index]\n",
    "        ret_pcd = pcd + np.random.normal(size=pcd.shape, scale = 0.005)\n",
    "        return mapping, ret_pcd\n",
    "\n",
    "dataset = TestDataset(points_list)\n",
    "(training_dataset, test_dataset) = \\\n",
    "        torch.utils.data.random_split(\n",
    "            dataset, (550, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TestNN, self).__init__()\n",
    "        self.pcn = PCN(latent_dim=10)\n",
    "        #self.pcn = PointNet2(feat_dim=0)\n",
    "        self.fc1 = nn.Linear(10, 64)\n",
    "        self.out = nn.Linear(64, 6)\n",
    "\n",
    "    def forward(self, pcd):\n",
    "        latent = self.pcn(pcd)\n",
    "        x = self.fc1(latent).relu()\n",
    "        x = torch.nn.functional.log_softmax(self.out(x), dim=1)\n",
    "        return x\n",
    "net = TestNN().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "loss_fn = torch.nn.NLLLoss()\n",
    "dataloader = DataLoader(training_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss: 1.7964445352554321\n",
      "Total Loss: 1.7928923765818279\n",
      "Total Loss: 1.7919674846861098\n",
      "Total Loss: 1.791657222641839\n",
      "Total Loss: 1.791511336962382\n",
      "Total Loss: 1.7914344469706218\n",
      "Total Loss: 1.7913611200120714\n",
      "Total Loss: 1.7913244432873197\n",
      "Total Loss: 1.79126328892178\n",
      "Total Loss: 1.791198558277554\n",
      "Total Loss: 1.7911457353168063\n",
      "Total Loss: 1.7910645008087158\n",
      "Total Loss: 1.7909821536805894\n",
      "Total Loss: 1.790849208831787\n",
      "Total Loss: 1.7907042900721233\n",
      "Total Loss: 1.7905460596084595\n",
      "Total Loss: 1.7902708583407931\n",
      "Total Loss: 1.7898679706785414\n",
      "Total Loss: 1.7894263797336154\n",
      "Total Loss: 1.7886549234390259\n",
      "Total Loss: 1.7875872453053792\n",
      "Total Loss: 1.7856053908665974\n",
      "Total Loss: 1.7821884287728205\n",
      "Total Loss: 1.7769216696421306\n",
      "Total Loss: 1.7686369684007432\n",
      "Total Loss: 1.7500575648413763\n",
      "Total Loss: 1.7258691390355427\n",
      "Total Loss: 1.6791015598509047\n",
      "Total Loss: 1.617857853571574\n",
      "Total Loss: 1.5742187632454767\n",
      "Total Loss: 1.4961806535720825\n",
      "Total Loss: 1.3926108015908136\n",
      "Total Loss: 1.2986622916327581\n",
      "Total Loss: 1.235818240377638\n",
      "Total Loss: 1.2295984029769897\n",
      "Total Loss: 1.2247634198930528\n",
      "Total Loss: 1.2561445236206055\n",
      "Total Loss: 1.2394615411758423\n",
      "Total Loss: 1.1682610114415486\n",
      "Total Loss: 1.1129003763198853\n",
      "Total Loss: 1.087778713968065\n",
      "Total Loss: 1.073490367995368\n",
      "Total Loss: 1.0630249977111816\n",
      "Total Loss: 1.0498453511132135\n",
      "Total Loss: 1.0501717858844333\n",
      "Total Loss: 1.0380114316940308\n",
      "Total Loss: 0.9713150726424323\n",
      "Total Loss: 0.9975791838434007\n",
      "Total Loss: 0.9479557275772095\n",
      "Total Loss: 0.9315733644697402\n",
      "Total Loss: 0.8942012588183085\n",
      "Total Loss: 0.9153628216849433\n",
      "Total Loss: 0.8673396706581116\n",
      "Total Loss: 0.9095646010504829\n",
      "Total Loss: 0.8968701495064629\n",
      "Total Loss: 0.8727005190319486\n",
      "Total Loss: 0.9317195812861124\n",
      "Total Loss: 0.9045250945621066\n",
      "Total Loss: 0.833172963725196\n",
      "Total Loss: 0.9049922029177347\n",
      "Total Loss: 0.9119398262765672\n",
      "Total Loss: 0.8485802014668783\n",
      "Total Loss: 0.7848045163684421\n",
      "Total Loss: 0.768177847067515\n",
      "Total Loss: 0.7776751385794746\n",
      "Total Loss: 0.7449599769380357\n",
      "Total Loss: 0.7459774216016134\n",
      "Total Loss: 0.7298273841540018\n",
      "Total Loss: 0.7078196936183505\n",
      "Total Loss: 0.6987903912862142\n",
      "Total Loss: 0.7760632038116455\n",
      "Total Loss: 0.7417508297496371\n",
      "Total Loss: 0.795429633723365\n",
      "Total Loss: 1.0501748124758403\n",
      "Total Loss: 0.8261207143465678\n",
      "Total Loss: 0.7258609003490872\n",
      "Total Loss: 0.6555257108476427\n",
      "Total Loss: 0.629382398393419\n",
      "Total Loss: 0.6300131877263387\n",
      "Total Loss: 0.6268911494149102\n",
      "Total Loss: 0.6047699318991767\n",
      "Total Loss: 0.538212451669905\n",
      "Total Loss: 0.5572464995914035\n",
      "Total Loss: 0.5568019052346548\n",
      "Total Loss: 0.5151843263043298\n",
      "Total Loss: 0.5193960600429111\n",
      "Total Loss: 0.4670520954661899\n",
      "Total Loss: 0.46879369682735866\n",
      "Total Loss: 0.4608493712213304\n",
      "Total Loss: 0.42439742220772636\n",
      "Total Loss: 0.4348513119750553\n",
      "Total Loss: 0.38168008459938896\n",
      "Total Loss: 0.42297908663749695\n",
      "Total Loss: 0.3959535426563687\n",
      "Total Loss: 0.3959130413002438\n",
      "Total Loss: 0.3837607171800401\n",
      "Total Loss: 0.3369644151793586\n",
      "Total Loss: 0.34842068288061356\n",
      "Total Loss: 0.3809090985192193\n",
      "Total Loss: 0.41182975471019745\n",
      "Total Loss: 0.4088207450177934\n",
      "Total Loss: 0.36501333779758877\n",
      "Total Loss: 0.30455125371615094\n",
      "Total Loss: 0.299906419383155\n",
      "Total Loss: 0.360347631904814\n",
      "Total Loss: 0.34573232796457076\n",
      "Total Loss: 0.31162940131293404\n",
      "Total Loss: 0.34052278267012703\n",
      "Total Loss: 0.3351735638247596\n",
      "Total Loss: 0.3709546857410007\n",
      "Total Loss: 0.3796655966175927\n",
      "Total Loss: 0.3218786352210575\n",
      "Total Loss: 0.2753872573375702\n",
      "Total Loss: 0.27767956588003373\n",
      "Total Loss: 0.2655885054005517\n",
      "Total Loss: 0.29566946460141075\n",
      "Total Loss: 0.30197465419769287\n",
      "Total Loss: 0.25530364778306747\n",
      "Total Loss: 0.24284039272202385\n",
      "Total Loss: 0.23132154014375475\n",
      "Total Loss: 0.23448370397090912\n",
      "Total Loss: 0.3229533102777269\n",
      "Total Loss: 0.25039375325043994\n",
      "Total Loss: 0.26992474165227676\n",
      "Total Loss: 0.19331066972679561\n",
      "Total Loss: 0.2041582746638192\n",
      "Total Loss: 0.2304609715938568\n",
      "Total Loss: 0.270554855465889\n",
      "Total Loss: 0.20463052226437461\n",
      "Total Loss: 0.2556937386592229\n",
      "Total Loss: 0.2161126948065228\n",
      "Total Loss: 0.261207456390063\n",
      "Total Loss: 0.22200203935305277\n",
      "Total Loss: 0.15952970335880914\n",
      "Total Loss: 0.19575018518500858\n",
      "Total Loss: 0.19102481173144448\n",
      "Total Loss: 0.1581658778919114\n",
      "Total Loss: 0.13510305020544264\n",
      "Total Loss: 0.15383252004782358\n",
      "Total Loss: 0.1591871993409263\n",
      "Total Loss: 0.14336319350534016\n",
      "Total Loss: 0.12229036622577244\n",
      "Total Loss: 0.17639420181512833\n",
      "Total Loss: 0.1255977658761872\n",
      "Total Loss: 0.11870452513297398\n",
      "Total Loss: 0.14819680319892037\n",
      "Total Loss: 0.17877747284041512\n",
      "Total Loss: 0.13560198578569624\n",
      "Total Loss: 0.15000207970539728\n",
      "Total Loss: 0.14558193998204338\n",
      "Total Loss: 0.09941228727499644\n",
      "Total Loss: 0.07382155437436369\n",
      "Total Loss: 0.09933557568324937\n",
      "Total Loss: 0.1077213365998533\n",
      "Total Loss: 0.0930132203631931\n",
      "Total Loss: 0.09261467928687732\n",
      "Total Loss: 0.09981396339005894\n",
      "Total Loss: 0.13950251787900925\n",
      "Total Loss: 0.09997944368256463\n",
      "Total Loss: 0.08476592683129841\n",
      "Total Loss: 0.08681388902995321\n",
      "Total Loss: 0.10220015545686086\n",
      "Total Loss: 0.16510159646471342\n",
      "Total Loss: 0.08434341020054287\n",
      "Total Loss: 0.07338578005631764\n",
      "Total Loss: 0.06416181019610828\n",
      "Total Loss: 0.08189327932066387\n",
      "Total Loss: 0.09897167277004984\n",
      "Total Loss: 0.11447727556029956\n",
      "Total Loss: 0.08386861739887132\n",
      "Total Loss: 0.06210984869135751\n",
      "Total Loss: 0.06154197599324915\n",
      "Total Loss: 0.05626437999308109\n",
      "Total Loss: 0.05542859497169653\n",
      "Total Loss: 0.0797160959078206\n",
      "Total Loss: 0.0857038584848245\n",
      "Total Loss: 0.10811033451722728\n",
      "Total Loss: 0.12914851266476843\n",
      "Total Loss: 0.11293857130739424\n",
      "Total Loss: 0.09111630130145285\n",
      "Total Loss: 0.05485046820508109\n",
      "Total Loss: 0.04184462585382991\n",
      "Total Loss: 0.04137557693239716\n",
      "Total Loss: 0.03556915890011522\n",
      "Total Loss: 0.04332938210831748\n",
      "Total Loss: 0.030161112546920776\n",
      "Total Loss: 0.03067043734093507\n",
      "Total Loss: 0.03942341460949845\n",
      "Total Loss: 0.02227776776999235\n",
      "Total Loss: 0.03218418794373671\n",
      "Total Loss: 0.03324563832332691\n",
      "Total Loss: 0.0427944159342183\n",
      "Total Loss: 0.03723464119765493\n",
      "Total Loss: 0.04949273086256451\n",
      "Total Loss: 0.05954179478188356\n",
      "Total Loss: 0.04598737394230233\n",
      "Total Loss: 0.047677219120992556\n",
      "Total Loss: 0.035195298906829625\n",
      "Total Loss: 0.05146716245346599\n",
      "Total Loss: 0.04889011486536927\n",
      "Total Loss: 0.0385865124149455\n",
      "Total Loss: 0.029845978133380413\n",
      "Total Loss: 0.03280526430656513\n",
      "Total Loss: 0.026240925821993086\n",
      "Total Loss: 0.03143972624093294\n",
      "Total Loss: 0.04021845426824358\n",
      "Total Loss: 0.023941469689210255\n",
      "Total Loss: 0.020198546970884006\n",
      "Total Loss: 0.04945921463270982\n",
      "Total Loss: 0.037738130117456116\n",
      "Total Loss: 0.033337766925493874\n",
      "Total Loss: 0.027323178429570463\n",
      "Total Loss: 0.04865747131407261\n",
      "Total Loss: 0.06310983374714851\n",
      "Total Loss: 0.04604865496771203\n",
      "Total Loss: 0.049088665594657264\n",
      "Total Loss: 0.050861842619876065\n",
      "Total Loss: 0.0220459902452098\n",
      "Total Loss: 0.01923628430813551\n",
      "Total Loss: 0.015356787842594914\n",
      "Total Loss: 0.016914121703141265\n",
      "Total Loss: 0.024006233861049015\n",
      "Total Loss: 0.01574058851434125\n",
      "Total Loss: 0.023412775713950396\n",
      "Total Loss: 0.025432991277840402\n",
      "Total Loss: 0.019518645894196298\n",
      "Total Loss: 0.02937205373826954\n",
      "Total Loss: 0.028866336060067017\n",
      "Total Loss: 0.019562287049161062\n",
      "Total Loss: 0.03349941927525732\n",
      "Total Loss: 0.01309118326753378\n",
      "Total Loss: 0.010038238950073719\n",
      "Total Loss: 0.010584823011110226\n",
      "Total Loss: 0.01390974223613739\n",
      "Total Loss: 0.009005737335731586\n",
      "Total Loss: 0.016310063294238515\n",
      "Total Loss: 0.02570050194238623\n",
      "Total Loss: 0.02846825997241669\n",
      "Total Loss: 0.015814036027424865\n",
      "Total Loss: 0.02724522098691927\n",
      "Total Loss: 0.0715498118661344\n",
      "Total Loss: 0.03990478958520624\n",
      "Total Loss: 0.01403632289212611\n",
      "Total Loss: 0.026024366211560037\n",
      "Total Loss: 0.017871008202847507\n",
      "Total Loss: 0.015398662796037065\n",
      "Total Loss: 0.03515938327958187\n",
      "Total Loss: 0.06182260557802187\n",
      "Total Loss: 0.02501164304299487\n",
      "Total Loss: 0.0136600894232591\n",
      "Total Loss: 0.01448787386632628\n",
      "Total Loss: 0.056548360786918134\n",
      "Total Loss: 0.03056297544389963\n",
      "Total Loss: 0.029416065611359146\n",
      "Total Loss: 0.09180833782172865\n",
      "Total Loss: 0.13365455489191744\n",
      "Total Loss: 0.06455976707446906\n",
      "Total Loss: 0.028575560078024864\n",
      "Total Loss: 0.021883272772861853\n",
      "Total Loss: 0.01724794889903731\n",
      "Total Loss: 0.013556801300081942\n",
      "Total Loss: 0.020807067449722026\n",
      "Total Loss: 0.051841330248862505\n",
      "Total Loss: 0.027952685072604153\n",
      "Total Loss: 0.04588809066141645\n",
      "Total Loss: 0.0383043447509408\n",
      "Total Loss: 0.027702476932770677\n",
      "Total Loss: 0.012350912180004848\n",
      "Total Loss: 0.006591520918947127\n",
      "Total Loss: 0.01510363282997989\n",
      "Total Loss: 0.004857099304596583\n",
      "Total Loss: 0.013487045808384815\n",
      "Total Loss: 0.009388776884103814\n",
      "Total Loss: 0.011661460286834173\n",
      "Total Loss: 0.007356164769993888\n",
      "Total Loss: 0.009539407677948475\n",
      "Total Loss: 0.016264746198430657\n",
      "Total Loss: 0.010449911286640499\n",
      "Total Loss: 0.009148426974813143\n",
      "Total Loss: 0.015938378197865352\n",
      "Total Loss: 0.01143654254782531\n",
      "Total Loss: 0.010880234754747815\n",
      "Total Loss: 0.009949050300444165\n",
      "Total Loss: 0.009023334029027157\n",
      "Total Loss: 0.006925461283471022\n",
      "Total Loss: 0.11229404030988614\n",
      "Total Loss: 1.7367445381565227\n",
      "Total Loss: 1.1561993385354679\n",
      "Total Loss: 0.22420961927208635\n",
      "Total Loss: 0.07254439261224535\n",
      "Total Loss: 0.047738361068897776\n",
      "Total Loss: 0.02657043406118949\n",
      "Total Loss: 0.02233137811223666\n",
      "Total Loss: 0.016544738163550694\n",
      "Total Loss: 0.021397577184769843\n",
      "Total Loss: 0.014701869752671983\n",
      "Total Loss: 0.013382456731051207\n",
      "Total Loss: 0.014045805152919557\n",
      "Total Loss: 0.011364770173612568\n",
      "Total Loss: 0.011896792850974534\n",
      "Total Loss: 0.01648014173325565\n",
      "Total Loss: 0.009738176957600646\n",
      "Total Loss: 0.00801775987363524\n",
      "Total Loss: 0.010910329098502794\n",
      "Total Loss: 0.01586883675513996\n",
      "Total Loss: 0.012745424070292048\n",
      "Total Loss: 0.011741143392605914\n",
      "Total Loss: 0.011028468608856201\n",
      "Total Loss: 0.01383155837862028\n",
      "Total Loss: 0.01192000513482425\n",
      "Total Loss: 0.007097783912387159\n",
      "Total Loss: 0.007578317593369219\n",
      "Total Loss: 0.006537767230636544\n",
      "Total Loss: 0.00930451925119592\n",
      "Total Loss: 0.0075482954271137714\n",
      "Total Loss: 0.00971450460039907\n",
      "Total Loss: 0.00877630715775821\n",
      "Total Loss: 0.010139393516712718\n",
      "Total Loss: 0.009579316376604967\n",
      "Total Loss: 0.010268057386080423\n",
      "Total Loss: 0.01443939914719926\n",
      "Total Loss: 0.0075909888837486506\n",
      "Total Loss: 0.006633907220222884\n",
      "Total Loss: 0.005996199790388346\n",
      "Total Loss: 0.005947049551953872\n",
      "Total Loss: 0.00730202305648062\n",
      "Total Loss: 0.006903709766144554\n",
      "Total Loss: 0.004352391335285372\n",
      "Total Loss: 0.006802684937914212\n",
      "Total Loss: 0.008783212138546838\n",
      "Total Loss: 0.011312532626713315\n",
      "Total Loss: 0.010284641886957817\n",
      "Total Loss: 0.008323010315911638\n",
      "Total Loss: 0.00982474160587622\n",
      "Total Loss: 0.005585671919915412\n",
      "Total Loss: 0.007360886420226759\n",
      "Total Loss: 0.004309817424250973\n",
      "Total Loss: 0.006455653864476416\n",
      "Total Loss: 0.00870664334959454\n",
      "Total Loss: 0.013717211653581925\n",
      "Total Loss: 0.008693799609318376\n",
      "Total Loss: 0.014852849456171194\n",
      "Total Loss: 0.008522527137150368\n",
      "Total Loss: 0.007900974480435252\n",
      "Total Loss: 0.0071085057635274194\n",
      "Total Loss: 0.007215351642419894\n",
      "Total Loss: 0.006563044799905684\n",
      "Total Loss: 0.006454686375541819\n",
      "Total Loss: 0.005508923747887214\n",
      "Total Loss: 0.004558283499338561\n",
      "Total Loss: 0.0037334412336349487\n",
      "Total Loss: 0.005046875720533232\n",
      "Total Loss: 0.0035469961456126636\n",
      "Total Loss: 0.005086977435793314\n",
      "Total Loss: 0.006909705491529571\n",
      "Total Loss: 0.005098165918348564\n",
      "Total Loss: 0.006668235847933425\n",
      "Total Loss: 0.003570246406727367\n",
      "Total Loss: 0.0030183648794061607\n",
      "Total Loss: 0.003930178225143916\n",
      "Total Loss: 0.004349343310524192\n",
      "Total Loss: 0.004032696636083226\n",
      "Total Loss: 0.004527831993376215\n",
      "Total Loss: 0.004563824868657523\n",
      "Total Loss: 0.005731663395029803\n",
      "Total Loss: 0.004743771731025643\n",
      "Total Loss: 0.007371052827996512\n",
      "Total Loss: 0.0054860049082587166\n",
      "Total Loss: 0.004267061168017487\n",
      "Total Loss: 0.006309764537339409\n",
      "Total Loss: 0.0070028123704509605\n",
      "Total Loss: 0.003598148274856309\n",
      "Total Loss: 0.005728366888231701\n",
      "Total Loss: 0.004854440857242379\n",
      "Total Loss: 0.0039305909174597925\n",
      "Total Loss: 0.004185244699733125\n",
      "Total Loss: 0.003651879533814887\n",
      "Total Loss: 0.0033652771445405153\n",
      "Total Loss: 0.004616837482899427\n",
      "Total Loss: 0.003943946296607869\n",
      "Total Loss: 0.007471674910953475\n",
      "Total Loss: 0.005101682579455276\n",
      "Total Loss: 0.00525819248933759\n",
      "Total Loss: 0.003829024382866919\n",
      "Total Loss: 0.005913390402889086\n",
      "Total Loss: 0.004258098701635997\n",
      "Total Loss: 0.005402818968933489\n",
      "Total Loss: 0.006470657315933042\n",
      "Total Loss: 0.00278702671898322\n",
      "Total Loss: 0.00571210521997677\n",
      "Total Loss: 0.007520523452613916\n",
      "Total Loss: 0.0035590955263210666\n",
      "Total Loss: 0.006298472833198805\n",
      "Total Loss: 0.0020737232407554984\n",
      "Total Loss: 0.004610563033363885\n",
      "Total Loss: 0.004690394995527135\n",
      "Total Loss: 0.004056027818781634\n",
      "Total Loss: 0.003564749288165735\n",
      "Total Loss: 0.004642215415111018\n",
      "Total Loss: 0.005401128967706528\n",
      "Total Loss: 0.0037490770660547745\n",
      "Total Loss: 0.002304328890103433\n",
      "Total Loss: 0.004640183114032779\n",
      "Total Loss: 0.0031903556793824667\n",
      "Total Loss: 0.005688695120625198\n",
      "Total Loss: 0.004210484278802242\n",
      "Total Loss: 0.00585984699945483\n",
      "Total Loss: 0.0032545699634485776\n",
      "Total Loss: 0.006752351357135922\n",
      "Total Loss: 0.006190539803355932\n",
      "Total Loss: 0.00453902796531717\n",
      "Total Loss: 0.005226640987934338\n",
      "Total Loss: 0.0032723083859309554\n",
      "Total Loss: 0.004417059102302624\n",
      "Total Loss: 0.0033019989754797686\n",
      "Total Loss: 0.0027613083042928744\n",
      "Total Loss: 0.0035694288348572124\n",
      "Total Loss: 0.016136928254531488\n",
      "Total Loss: 0.013578221188961632\n",
      "Total Loss: 0.011001603703738915\n",
      "Total Loss: 0.009477199171669781\n",
      "Total Loss: 0.007760779117234051\n",
      "Total Loss: 0.00662726620470898\n",
      "Total Loss: 0.004160176302927236\n",
      "Total Loss: 0.0027836868410102194\n",
      "Total Loss: 0.004148399974736903\n",
      "Total Loss: 0.003784598291127218\n",
      "Total Loss: 0.0021031770286046797\n",
      "Total Loss: 0.00551796722639766\n",
      "Total Loss: 0.0026035341951582166\n",
      "Total Loss: 0.004413355064267914\n",
      "Total Loss: 0.0026539971392291286\n",
      "Total Loss: 0.004780810546233422\n",
      "Total Loss: 0.004005277839799722\n",
      "Total Loss: 0.007945311967180006\n",
      "Total Loss: 0.0025654935096907946\n",
      "Total Loss: 0.0017361918774743874\n",
      "Total Loss: 0.001999429336542057\n",
      "Total Loss: 0.008160733501426876\n",
      "Total Loss: 0.004567961674183607\n",
      "Total Loss: 0.0018832517865424354\n",
      "Total Loss: 0.001992761338543561\n",
      "Total Loss: 0.004381548865543057\n",
      "Total Loss: 0.0024798477340179184\n",
      "Total Loss: 0.00253985304152593\n",
      "Total Loss: 0.0024600045160493916\n",
      "Total Loss: 0.0034629571665492323\n",
      "Total Loss: 0.0019842636021268037\n",
      "Total Loss: 0.0027652353358765445\n",
      "Total Loss: 0.00219307543658134\n",
      "Total Loss: 0.004140396647724426\n",
      "Total Loss: 0.003984793606731627\n",
      "Total Loss: 0.003758335072133276\n",
      "Total Loss: 0.0031982976684553754\n",
      "Total Loss: 0.0023319225680703917\n",
      "Total Loss: 0.001203804820155104\n",
      "Total Loss: 0.002462577876738376\n",
      "Total Loss: 0.0016824947346726225\n",
      "Total Loss: 0.0025982899824157357\n",
      "Total Loss: 0.0013845249648309415\n",
      "Total Loss: 0.001717511422207786\n",
      "Total Loss: 0.0029623969652069113\n",
      "Total Loss: 0.006390151807055291\n",
      "Total Loss: 0.004137922155981262\n",
      "Total Loss: 0.00667931211905347\n",
      "Total Loss: 0.007964769307161786\n",
      "Total Loss: 0.0066208682320494615\n",
      "Total Loss: 0.0020566509338095784\n",
      "Total Loss: 0.004452946479432285\n",
      "Total Loss: 0.003548525299669968\n",
      "Total Loss: 0.0030594495021634633\n",
      "Total Loss: 0.0014755500596947968\n",
      "Total Loss: 0.005613024553491009\n",
      "Total Loss: 0.0015429580816999078\n",
      "Total Loss: 0.0027995094230088093\n",
      "Total Loss: 0.0037321618421831066\n",
      "Total Loss: 0.00312310738566642\n",
      "Total Loss: 0.0012218397395271394\n",
      "Total Loss: 0.0012232413282617927\n",
      "Total Loss: 0.0014561529646420644\n",
      "Total Loss: 0.0022478162621458373\n",
      "Total Loss: 0.002873739706248873\n",
      "Total Loss: 0.00429235596028674\n",
      "Total Loss: 0.00374774562401904\n",
      "Total Loss: 0.0020550627959892154\n",
      "Total Loss: 0.0037280919301944473\n",
      "Total Loss: 0.007805696805007756\n",
      "Total Loss: 0.002947716236424943\n",
      "Total Loss: 0.004199672600306157\n",
      "Total Loss: 0.00435134798883357\n",
      "Total Loss: 0.002397149348528021\n",
      "Total Loss: 0.0020329258663372863\n",
      "Total Loss: 0.0018087143479432496\n",
      "Total Loss: 0.0019226454896852374\n",
      "Total Loss: 0.0014537020405340525\n",
      "Total Loss: 0.0010942293770818247\n",
      "Total Loss: 0.0025709746340807113\n",
      "Total Loss: 0.0014690080748146607\n",
      "Total Loss: 0.001677990923376961\n",
      "Total Loss: 0.0014663128337512414\n"
     ]
    }
   ],
   "source": [
    "for i in range(500):\n",
    "    batch_cnt = 0\n",
    "    total_loss = 0\n",
    "    for label, pointcloud in dataloader:\n",
    "        optim.zero_grad()\n",
    "        label = label.cuda()\n",
    "        pointcloud = pointcloud.cuda().float()\n",
    "        pred = net(pointcloud)\n",
    "        loss = loss_fn(pred, label)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        batch_cnt += 1\n",
    "        total_loss += float(loss)\n",
    "    print(f\"Total Loss: {total_loss/batch_cnt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Test Loss: 0.001626654759324424\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(test_dataset, batch_size=1)\n",
    "batch_cnt = 0\n",
    "total_loss = 0\n",
    "net.eval()\n",
    "labels = []\n",
    "for label, pointcloud in dataloader:\n",
    "    label = label.cuda()\n",
    "    pointcloud = pointcloud.cuda().float()\n",
    "    pred = net(pointcloud)\n",
    "    labels.append(label)\n",
    "    loss = loss_fn(pred, label)\n",
    "    batch_cnt += 1\n",
    "    total_loss += float(loss)\n",
    "print(f\"Total Test Loss: {total_loss/batch_cnt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.pcn.state_dict(), \"neurals/pcn_model/pcn_10.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries([o3d.geometry.PointCloud(o3d.utility.Vector3dVector(training_dataset[10][1]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.04219856, -0.01532705,  0.03520654],\n",
       "       [-0.03941568,  0.10910279, -0.05009749],\n",
       "       [ 0.03348964, -0.15582641,  0.04298963],\n",
       "       ...,\n",
       "       [-0.20387647, -0.08532091,  0.06352441],\n",
       "       [-0.06860191,  0.14903795,  0.05420021],\n",
       "       [ 0.10703852, -0.02935053,  0.06121993]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[10][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('dex_manip')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a13924603cf124e169c00f51f55efb178979cb98f2833cd534c16d33f0a3fe3d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
