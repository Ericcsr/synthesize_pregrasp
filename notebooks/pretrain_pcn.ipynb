{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: May 20 2022 19:44:17\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from neurals.network import PCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcds = []\n",
    "pcds.append(o3d.io.read_point_cloud(\"../data/seeds/pointclouds/pose_0_pcd.ply\"))\n",
    "pcds.append(o3d.io.read_point_cloud(\"../data/seeds/pointclouds/pose_1_pcd.ply\"))\n",
    "pcds.append(o3d.io.read_point_cloud(\"../data/seeds/pointclouds/pose_2_pcd.ply\"))\n",
    "pcds.append(o3d.io.read_point_cloud(\"../data/seeds/pointclouds/pose_3_pcd.ply\"))\n",
    "pcds.append(o3d.io.read_point_cloud(\"../data/seeds/pointclouds/pose_4_pcd.ply\"))\n",
    "pcds.append(o3d.io.read_point_cloud(\"../data/seeds/pointclouds/pose_5_pcd.ply\"))\n",
    "dfs = []\n",
    "dfs.append(np.load(\"../data/seeds/pointclouds_df/pose_0_pcd.npy\"))\n",
    "dfs.append(np.load(\"../data/seeds/pointclouds_df/pose_1_pcd.npy\"))\n",
    "dfs.append(np.load(\"../data/seeds/pointclouds_df/pose_2_pcd.npy\"))\n",
    "dfs.append(np.load(\"../data/seeds/pointclouds_df/pose_3_pcd.npy\"))\n",
    "dfs.append(np.load(\"../data/seeds/pointclouds_df/pose_4_pcd.npy\"))\n",
    "dfs.append(np.load(\"../data/seeds/pointclouds_df/pose_5_pcd.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600 600\n"
     ]
    }
   ],
   "source": [
    "points_list = []\n",
    "dfs_list = []\n",
    "for pcd, df in zip(pcds, dfs):\n",
    "    for i in range(100):\n",
    "        points = np.asarray(pcd.points)\n",
    "        points_index = np.random.choice(len(points), 1024, replace=False)\n",
    "        points = points[points_index]\n",
    "        points_list.append(points)\n",
    "        dfs_list.append(df[points_index])\n",
    "print(len(points_list), len(dfs_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, points_list, dfs_list):\n",
    "        self.pcds = np.asarray(points_list)\n",
    "        self.dfs = np.asarray(dfs_list)\n",
    "        self.mapping = []\n",
    "        for i in range(6):\n",
    "            self.mapping += [i] * 100\n",
    "        self.mapping = np.array(self.mapping).astype(np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mapping)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        mapping = self.mapping[index]\n",
    "        pcd = self.pcds[index]\n",
    "        df = self.dfs[index]\n",
    "        ret_pcd = pcd + np.random.normal(size=pcd.shape, scale = 0.005)\n",
    "        ret_df = df + np.random.normal(size=df.shape, scale=0.005)\n",
    "        return mapping, ret_pcd, ret_df\n",
    "\n",
    "dataset = TestDataset(points_list, dfs_list)\n",
    "(training_dataset, test_dataset) = \\\n",
    "        torch.utils.data.random_split(\n",
    "            dataset, (550, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TestNN, self).__init__()\n",
    "        self.pcn = PCN(4,latent_dim=20)\n",
    "        #self.pcn = PointNet2(feat_dim=0)\n",
    "        self.fc1 = nn.Linear(20, 64)\n",
    "        self.out = nn.Linear(64, 6)\n",
    "\n",
    "    def forward(self, pcd, df):\n",
    "        df = df.view([df.shape[0], df.shape[1],1])\n",
    "        pcd = torch.cat([pcd, df], axis=2)\n",
    "        latent = self.pcn(pcd)\n",
    "        x = self.fc1(latent).relu()\n",
    "        x = torch.nn.functional.log_softmax(self.out(x), dim=1)\n",
    "        return x\n",
    "net = TestNN().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "loss_fn = torch.nn.NLLLoss()\n",
    "dataloader = DataLoader(training_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss: 1.7964346806208293\n",
      "Total Loss: 1.7920001745224\n",
      "Total Loss: 1.7902297841178045\n",
      "Total Loss: 1.7878453466627333\n",
      "Total Loss: 1.7840404907862346\n",
      "Total Loss: 1.7761289013756647\n",
      "Total Loss: 1.7575562927458022\n",
      "Total Loss: 1.7158650954564412\n",
      "Total Loss: 1.6288912296295166\n",
      "Total Loss: 1.471411108970642\n",
      "Total Loss: 1.2505370643403795\n",
      "Total Loss: 1.030300219853719\n",
      "Total Loss: 0.8030530876583524\n",
      "Total Loss: 0.5726111829280853\n",
      "Total Loss: 0.39522971709569293\n",
      "Total Loss: 0.2851282109816869\n",
      "Total Loss: 0.2144550151295132\n",
      "Total Loss: 0.1579073957271046\n",
      "Total Loss: 0.10997884058290058\n",
      "Total Loss: 0.07510660505957073\n",
      "Total Loss: 0.04472229422794448\n",
      "Total Loss: 0.0342108156118128\n",
      "Total Loss: 0.02036127344601684\n",
      "Total Loss: 0.014587666114999188\n",
      "Total Loss: 0.01177131746792131\n",
      "Total Loss: 0.008796499628159735\n",
      "Total Loss: 0.007127902884450223\n",
      "Total Loss: 0.006618354055616591\n",
      "Total Loss: 0.005758540032224523\n",
      "Total Loss: 0.004540259535941813\n",
      "Total Loss: 0.0034265331147859492\n",
      "Total Loss: 0.0030615062763293586\n",
      "Total Loss: 0.00285738670370645\n",
      "Total Loss: 0.0027722497600027258\n",
      "Total Loss: 0.002373605656127135\n",
      "Total Loss: 0.002201672397657401\n",
      "Total Loss: 0.0017576093086972833\n",
      "Total Loss: 0.001611461746506393\n",
      "Total Loss: 0.0015360795789294774\n",
      "Total Loss: 0.0012627634892447127\n",
      "Total Loss: 0.0012136838373003735\n",
      "Total Loss: 0.0009631316319832371\n",
      "Total Loss: 0.0009982307739038435\n",
      "Total Loss: 0.0008826284837495121\n",
      "Total Loss: 0.0009160357553304897\n",
      "Total Loss: 0.0007703261281777588\n",
      "Total Loss: 0.0007636723522510794\n",
      "Total Loss: 0.0006580030700812737\n",
      "Total Loss: 0.0005815488014680644\n",
      "Total Loss: 0.0005793401538135691\n",
      "Total Loss: 0.0005365352036379692\n",
      "Total Loss: 0.00047426158885678486\n",
      "Total Loss: 0.0004458470050142043\n",
      "Total Loss: 0.00043861014031184215\n",
      "Total Loss: 0.0005026394598341236\n",
      "Total Loss: 0.00043380331286850077\n",
      "Total Loss: 0.00042825648496444855\n",
      "Total Loss: 0.0003695773743351714\n",
      "Total Loss: 0.00032723804340801306\n",
      "Total Loss: 0.00033965046491680876\n",
      "Total Loss: 0.0003418948876464532\n",
      "Total Loss: 0.0003149528686966126\n",
      "Total Loss: 0.0002885626874760621\n",
      "Total Loss: 0.00028278334260297316\n",
      "Total Loss: 0.00029163131774920557\n",
      "Total Loss: 0.0002631974914240547\n",
      "Total Loss: 0.00022410931220899025\n",
      "Total Loss: 0.0002427232506710829\n",
      "Total Loss: 0.00022352984524331987\n",
      "Total Loss: 0.00021429074210269997\n",
      "Total Loss: 0.0002019611646472994\n",
      "Total Loss: 0.0002066182415647846\n",
      "Total Loss: 0.0002094178521979807\n",
      "Total Loss: 0.00018313822996181747\n",
      "Total Loss: 0.00018197161100235663\n",
      "Total Loss: 0.00016337308948600871\n",
      "Total Loss: 0.00016139139350141503\n",
      "Total Loss: 0.00016487450355068885\n",
      "Total Loss: 0.00016219285397609282\n",
      "Total Loss: 0.00014226422677489204\n",
      "Total Loss: 0.00014530857879435644\n",
      "Total Loss: 0.0001477413356446454\n",
      "Total Loss: 0.00015300186108409738\n",
      "Total Loss: 0.00022836226950554797\n",
      "Total Loss: 0.0001475906537962146\n",
      "Total Loss: 0.00012592423607000254\n",
      "Total Loss: 0.00012564194184960797\n",
      "Total Loss: 0.00011787072435254231\n",
      "Total Loss: 0.00011421842807774535\n",
      "Total Loss: 0.00011170160966382052\n",
      "Total Loss: 0.00011652541191627581\n",
      "Total Loss: 0.00010576260683592409\n",
      "Total Loss: 0.0003535764828686499\n",
      "Total Loss: 0.00016108963528596278\n",
      "Total Loss: 0.00010438327202185367\n",
      "Total Loss: 0.00011943592431230677\n",
      "Total Loss: 0.00010964795688374175\n",
      "Total Loss: 9.116287401411682e-05\n",
      "Total Loss: 9.795296743201713e-05\n",
      "Total Loss: 8.174536035059848e-05\n",
      "Total Loss: 8.015212612614657e-05\n",
      "Total Loss: 8.256751607405022e-05\n",
      "Total Loss: 0.00010147396258414826\n",
      "Total Loss: 7.325854393356066e-05\n",
      "Total Loss: 0.00011334417407245685\n",
      "Total Loss: 7.265133212462792e-05\n",
      "Total Loss: 7.88943248658648e-05\n",
      "Total Loss: 6.884567402367893e-05\n",
      "Total Loss: 6.72416980604693e-05\n",
      "Total Loss: 7.125261537213292e-05\n",
      "Total Loss: 6.497906886377475e-05\n",
      "Total Loss: 7.748415373498574e-05\n",
      "Total Loss: 6.498371537115115e-05\n",
      "Total Loss: 5.7814936250603445e-05\n",
      "Total Loss: 7.49368959481621e-05\n",
      "Total Loss: 6.518611108832475e-05\n",
      "Total Loss: 6.762245538993739e-05\n",
      "Total Loss: 6.387102257576771e-05\n",
      "Total Loss: 6.042340505195575e-05\n",
      "Total Loss: 5.458694633691468e-05\n",
      "Total Loss: 5.7919586753188116e-05\n",
      "Total Loss: 5.1968420141040246e-05\n",
      "Total Loss: 5.9059732949309466e-05\n",
      "Total Loss: 5.2357706105491765e-05\n",
      "Total Loss: 5.750950367655605e-05\n",
      "Total Loss: 5.8706124238799224e-05\n",
      "Total Loss: 5.19612306864777e-05\n",
      "Total Loss: 4.7750277089006784e-05\n",
      "Total Loss: 4.1126436675161436e-05\n",
      "Total Loss: 4.964037523475579e-05\n",
      "Total Loss: 4.4859164012854715e-05\n",
      "Total Loss: 4.348240124980091e-05\n",
      "Total Loss: 5.331498282935677e-05\n",
      "Total Loss: 4.350243681175117e-05\n",
      "Total Loss: 4.722899150995848e-05\n",
      "Total Loss: 5.022575917084598e-05\n",
      "Total Loss: 4.910090784606938e-05\n",
      "Total Loss: 4.324598093161411e-05\n",
      "Total Loss: 5.605900433794078e-05\n",
      "Total Loss: 3.739709467885809e-05\n",
      "Total Loss: 3.858135823975317e-05\n",
      "Total Loss: 3.82531616196502e-05\n",
      "Total Loss: 3.74061613304851e-05\n",
      "Total Loss: 3.529602832309643e-05\n",
      "Total Loss: 4.489769708015956e-05\n",
      "Total Loss: 3.969394027889292e-05\n",
      "Total Loss: 3.471892664998045e-05\n",
      "Total Loss: 3.756475517649152e-05\n",
      "Total Loss: 4.158902164716791e-05\n",
      "Total Loss: 3.394227658443722e-05\n",
      "Total Loss: 3.127819117152184e-05\n",
      "Total Loss: 3.3573116727186265e-05\n",
      "Total Loss: 3.037732130906079e-05\n",
      "Total Loss: 3.5205268836256844e-05\n",
      "Total Loss: 3.38634520934041e-05\n",
      "Total Loss: 3.405641412731105e-05\n",
      "Total Loss: 5.6247739444693754e-05\n",
      "Total Loss: 4.4414717396850596e-05\n",
      "Total Loss: 3.0441234533403378e-05\n",
      "Total Loss: 3.0794547354970644e-05\n",
      "Total Loss: 3.07646710603472e-05\n",
      "Total Loss: 3.64009059315625e-05\n",
      "Total Loss: 2.8110748947559234e-05\n",
      "Total Loss: 3.082037397285199e-05\n",
      "Total Loss: 2.5660276151029393e-05\n",
      "Total Loss: 2.4804792701615952e-05\n",
      "Total Loss: 2.913552513443089e-05\n",
      "Total Loss: 2.718586175534357e-05\n",
      "Total Loss: 2.35283621603028e-05\n",
      "Total Loss: 2.4134354640005364e-05\n",
      "Total Loss: 3.645956187230897e-05\n",
      "Total Loss: 2.669449774354386e-05\n",
      "Total Loss: 2.9183105855028974e-05\n",
      "Total Loss: 2.1873737043481218e-05\n",
      "Total Loss: 2.5423969216515412e-05\n",
      "Total Loss: 2.6108493734176995e-05\n",
      "Total Loss: 2.2099486967716883e-05\n",
      "Total Loss: 2.3430689098960203e-05\n",
      "Total Loss: 2.386124601293381e-05\n",
      "Total Loss: 2.4974513912133665e-05\n",
      "Total Loss: 2.2625272524439628e-05\n",
      "Total Loss: 2.171444955618224e-05\n",
      "Total Loss: 2.2135144616994592e-05\n",
      "Total Loss: 2.2644004073097473e-05\n",
      "Total Loss: 2.624392214379946e-05\n",
      "Total Loss: 2.747124037543674e-05\n",
      "Total Loss: 2.0531990028555607e-05\n",
      "Total Loss: 1.958933454362624e-05\n",
      "Total Loss: 2.3785519361909894e-05\n",
      "Total Loss: 1.9743352317568173e-05\n",
      "Total Loss: 2.0617595206810318e-05\n",
      "Total Loss: 1.833786558967808e-05\n",
      "Total Loss: 1.902040528673549e-05\n",
      "Total Loss: 3.578060214850767e-05\n",
      "Total Loss: 2.2729313160299273e-05\n",
      "Total Loss: 2.2019593517244277e-05\n",
      "Total Loss: 1.9562572257806703e-05\n",
      "Total Loss: 2.146447695849929e-05\n",
      "Total Loss: 2.445851775216094e-05\n",
      "Total Loss: 2.1051862151782392e-05\n",
      "Total Loss: 1.536735109968706e-05\n",
      "Total Loss: 1.5576761244220608e-05\n",
      "Total Loss: 2.0240355196518874e-05\n",
      "Total Loss: 1.7034161424413065e-05\n",
      "Total Loss: 1.8047557306191367e-05\n",
      "Total Loss: 1.9352398541943534e-05\n",
      "Total Loss: 1.6486485037603416e-05\n",
      "Total Loss: 1.6341680950265276e-05\n",
      "Total Loss: 1.7725496440511455e-05\n",
      "Total Loss: 1.6423994364029364e-05\n",
      "Total Loss: 1.4306653206909283e-05\n",
      "Total Loss: 1.7270133968041693e-05\n",
      "Total Loss: 1.6399918624503902e-05\n",
      "Total Loss: 1.4504713362839539e-05\n",
      "Total Loss: 1.4114318016961786e-05\n",
      "Total Loss: 1.4580695379764722e-05\n",
      "Total Loss: 1.4523973327919117e-05\n",
      "Total Loss: 1.510654035680798e-05\n",
      "Total Loss: 1.3666892098424594e-05\n",
      "Total Loss: 1.4257666407502256e-05\n",
      "Total Loss: 1.8341846043767873e-05\n",
      "Total Loss: 1.7189404388773255e-05\n",
      "Total Loss: 1.540958328405395e-05\n",
      "Total Loss: 1.3139344951519484e-05\n",
      "Total Loss: 1.5601341045920788e-05\n",
      "Total Loss: 1.2450799557781364e-05\n",
      "Total Loss: 1.2783106184441647e-05\n",
      "Total Loss: 1.3919808427292284e-05\n",
      "Total Loss: 1.1397366506571416e-05\n",
      "Total Loss: 1.1783499758166727e-05\n",
      "Total Loss: 1.2979114116104836e-05\n",
      "Total Loss: 1.351853512056146e-05\n",
      "Total Loss: 2.408515117874938e-05\n",
      "Total Loss: 1.6019291352778157e-05\n",
      "Total Loss: 1.4014252378223722e-05\n",
      "Total Loss: 1.3297475460502836e-05\n",
      "Total Loss: 1.2731923864824543e-05\n",
      "Total Loss: 1.2516919014160521e-05\n",
      "Total Loss: 1.079741974131644e-05\n",
      "Total Loss: 1.121087643696228e-05\n",
      "Total Loss: 1.4699117147958734e-05\n",
      "Total Loss: 3.2990615737718246e-05\n",
      "Total Loss: 3.152166229685665e-05\n",
      "Total Loss: 3.147397274005925e-05\n",
      "Total Loss: 1.2478171205051089e-05\n",
      "Total Loss: 1.6059147734874084e-05\n",
      "Total Loss: 1.0192850923987055e-05\n",
      "Total Loss: 1.339534941911956e-05\n",
      "Total Loss: 9.969032311750601e-06\n",
      "Total Loss: 1.1862905416314283e-05\n",
      "Total Loss: 1.1557829465244948e-05\n",
      "Total Loss: 1.0538997634689116e-05\n",
      "Total Loss: 1.2795221209671581e-05\n",
      "Total Loss: 1.045319297392982e-05\n",
      "Total Loss: 9.42704051946445e-06\n",
      "Total Loss: 1.0091566612876099e-05\n",
      "Total Loss: 1.078252444131067e-05\n",
      "Total Loss: 1.1231808861743451e-05\n",
      "Total Loss: 1.0179199610623376e-05\n",
      "Total Loss: 1.025348274197313e-05\n",
      "Total Loss: 1.1048685488882862e-05\n",
      "Total Loss: 9.7756852584199e-06\n",
      "Total Loss: 1.3785102712265345e-05\n",
      "Total Loss: 9.949810949668366e-06\n",
      "Total Loss: 8.506550178329538e-06\n",
      "Total Loss: 3.1623007392530177e-05\n",
      "Total Loss: 2.3465783872072483e-05\n",
      "Total Loss: 1.2436139816903355e-05\n",
      "Total Loss: 9.899046340756791e-06\n",
      "Total Loss: 1.1630926110228756e-05\n",
      "Total Loss: 8.455675217394148e-06\n",
      "Total Loss: 8.47896939780589e-06\n",
      "Total Loss: 8.770031147125539e-06\n",
      "Total Loss: 8.230917349768182e-06\n",
      "Total Loss: 9.860944070775682e-06\n",
      "Total Loss: 8.037400069345798e-06\n",
      "Total Loss: 7.683665292764394e-06\n",
      "Total Loss: 1.036064784582575e-05\n",
      "Total Loss: 8.002848870900278e-06\n",
      "Total Loss: 8.393142454426399e-06\n",
      "Total Loss: 7.390939420777916e-06\n",
      "Total Loss: 1.029051024185416e-05\n",
      "Total Loss: 8.212176049306032e-06\n",
      "Total Loss: 7.949325461393326e-06\n",
      "Total Loss: 7.296004342707521e-06\n",
      "Total Loss: 6.7659637655500165e-06\n",
      "Total Loss: 8.455897639376215e-06\n",
      "Total Loss: 7.79707534093177e-06\n",
      "Total Loss: 7.583389156530353e-06\n",
      "Total Loss: 6.43694374957704e-06\n",
      "Total Loss: 9.386158884202208e-06\n",
      "Total Loss: 7.686054131126082e-06\n",
      "Total Loss: 6.782968840626482e-06\n",
      "Total Loss: 7.17799381770116e-06\n",
      "Total Loss: 1.1399271999026598e-05\n",
      "Total Loss: 6.57065639744461e-06\n",
      "Total Loss: 7.720950836503309e-06\n",
      "Total Loss: 6.552953537821951e-06\n",
      "Total Loss: 8.853405385404281e-06\n",
      "Total Loss: 6.879177564971744e-06\n",
      "Total Loss: 6.88576614670132e-06\n",
      "Total Loss: 9.639473167529408e-06\n",
      "Total Loss: 7.132756460729676e-06\n",
      "Total Loss: 6.271759704961571e-06\n",
      "Total Loss: 6.403919441153347e-06\n",
      "Total Loss: 6.437654090202866e-06\n",
      "Total Loss: 7.09598548888203e-06\n",
      "Total Loss: 7.621181591983057e-06\n",
      "Total Loss: 7.427478117784227e-06\n",
      "Total Loss: 9.031965520181176e-06\n",
      "Total Loss: 7.206157281163743e-06\n",
      "Total Loss: 6.718675775927194e-06\n",
      "Total Loss: 5.818878030873344e-06\n",
      "Total Loss: 5.464931973619969e-06\n",
      "Total Loss: 6.864949884604559e-06\n",
      "Total Loss: 6.986400876485277e-06\n",
      "Total Loss: 5.9950311373945115e-06\n",
      "Total Loss: 5.427674396843132e-06\n",
      "Total Loss: 5.512025533486546e-06\n",
      "Total Loss: 6.341264149038277e-06\n",
      "Total Loss: 6.6021541695388605e-06\n",
      "Total Loss: 4.934249621631655e-06\n",
      "Total Loss: 5.696790544283835e-06\n",
      "Total Loss: 6.820987942369862e-06\n",
      "Total Loss: 5.971622234331638e-06\n",
      "Total Loss: 5.633318829091473e-06\n",
      "Total Loss: 5.096349493922187e-06\n",
      "Total Loss: 5.7491588955195366e-06\n",
      "Total Loss: 4.637434383564849e-06\n",
      "Total Loss: 5.256848857647532e-06\n",
      "Total Loss: 4.988052624968178e-06\n",
      "Total Loss: 5.4655152123612224e-06\n",
      "Total Loss: 6.615902495670727e-06\n",
      "Total Loss: 5.578494538996084e-06\n",
      "Total Loss: 5.565661821391485e-06\n",
      "Total Loss: 4.843683528734901e-06\n",
      "Total Loss: 4.525646520253051e-06\n",
      "Total Loss: 5.72637731642721e-06\n",
      "Total Loss: 5.8026711637163926e-06\n",
      "Total Loss: 4.307026428149483e-06\n",
      "Total Loss: 4.63383296139202e-06\n",
      "Total Loss: 4.241454992855627e-06\n",
      "Total Loss: 4.590439680719606e-06\n",
      "Total Loss: 7.045968130922928e-06\n",
      "Total Loss: 4.601783050222568e-06\n",
      "Total Loss: 4.3414179850110995e-06\n",
      "Total Loss: 5.964050286719511e-06\n",
      "Total Loss: 5.905968515031014e-06\n",
      "Total Loss: 4.679851523784843e-06\n",
      "Total Loss: 4.700652728691542e-06\n",
      "Total Loss: 4.5050453789170005e-06\n",
      "Total Loss: 4.4649390878880186e-06\n",
      "Total Loss: 6.042017932688597e-06\n",
      "Total Loss: 5.6815753800442e-06\n",
      "Total Loss: 4.498841286072598e-06\n",
      "Total Loss: 5.040657545881307e-06\n",
      "Total Loss: 4.0722325896543525e-06\n",
      "Total Loss: 4.451797419985976e-06\n",
      "Total Loss: 4.3054159151425765e-06\n",
      "Total Loss: 5.8153546989261586e-06\n",
      "Total Loss: 4.82791438116692e-06\n",
      "Total Loss: 4.999092425350682e-06\n",
      "Total Loss: 4.4662417874658586e-06\n",
      "Total Loss: 4.844399073691521e-06\n",
      "Total Loss: 4.524052883425611e-06\n",
      "Total Loss: 4.731974513560999e-06\n",
      "Total Loss: 3.797471486500904e-06\n",
      "Total Loss: 4.156249057511256e-06\n",
      "Total Loss: 4.14142815922484e-06\n",
      "Total Loss: 4.310489329226483e-06\n",
      "Total Loss: 3.871647853278167e-06\n",
      "Total Loss: 6.625550465994618e-06\n",
      "Total Loss: 3.920673003045118e-06\n",
      "Total Loss: 4.385929085199476e-06\n",
      "Total Loss: 3.7806505841015478e-06\n",
      "Total Loss: 4.142470061196946e-06\n",
      "Total Loss: 4.1803152220786106e-06\n",
      "Total Loss: 4.290936052105583e-06\n",
      "Total Loss: 3.834348566063757e-06\n",
      "Total Loss: 3.2332738757961324e-06\n",
      "Total Loss: 3.329124385547604e-06\n",
      "Total Loss: 3.2677811405948078e-06\n",
      "Total Loss: 4.203042711499923e-06\n",
      "Total Loss: 3.748740558269977e-06\n",
      "Total Loss: 4.056594156686009e-06\n",
      "Total Loss: 3.054642219366441e-06\n",
      "Total Loss: 3.384495408277467e-06\n",
      "Total Loss: 3.7044472062714503e-06\n",
      "Total Loss: 2.8347691467287304e-06\n",
      "Total Loss: 3.852860648597319e-06\n",
      "Total Loss: 2.95528244957192e-06\n",
      "Total Loss: 3.244965202813748e-06\n",
      "Total Loss: 3.6123569013094916e-06\n",
      "Total Loss: 3.3815310871432303e-06\n",
      "Total Loss: 3.2010908752353216e-06\n",
      "Total Loss: 3.4260014369162186e-06\n",
      "Total Loss: 3.563684989179213e-06\n",
      "Total Loss: 3.9124403075321525e-06\n",
      "Total Loss: 3.7058456806941345e-06\n",
      "Total Loss: 3.389234077783638e-06\n",
      "Total Loss: 3.3118560875462008e-06\n",
      "Total Loss: 3.2074729511401124e-06\n",
      "Total Loss: 3.0294973688190416e-06\n",
      "Total Loss: 3.120951027489759e-06\n",
      "Total Loss: 3.0626419579170438e-06\n",
      "Total Loss: 2.9834081942681223e-06\n",
      "Total Loss: 2.9492346382337728e-06\n",
      "Total Loss: 2.7146666954851953e-06\n",
      "Total Loss: 4.310279261214116e-06\n",
      "Total Loss: 3.5098543220455112e-06\n",
      "Total Loss: 3.288857038165184e-06\n",
      "Total Loss: 3.1213211665696385e-06\n",
      "Total Loss: 3.893996386573741e-06\n",
      "Total Loss: 3.8055156387094435e-06\n",
      "Total Loss: 3.458007995504886e-06\n",
      "Total Loss: 3.06558772283299e-06\n",
      "Total Loss: 2.6519562905175717e-06\n",
      "Total Loss: 2.767032330868662e-06\n",
      "Total Loss: 3.934589535169329e-06\n",
      "Total Loss: 3.398577695811077e-06\n",
      "Total Loss: 3.000208633036689e-06\n",
      "Total Loss: 2.7599606811337353e-06\n",
      "Total Loss: 3.1810618565941695e-06\n",
      "Total Loss: 2.9922761213027925e-06\n",
      "Total Loss: 3.5495693282427965e-06\n",
      "Total Loss: 3.840062580214483e-06\n",
      "Total Loss: 2.791170471836166e-06\n",
      "Total Loss: 2.544740482335328e-06\n",
      "Total Loss: 3.85394693896362e-06\n",
      "Total Loss: 2.4801336293320573e-06\n",
      "Total Loss: 2.6178526645202915e-06\n",
      "Total Loss: 2.949002944458496e-06\n",
      "Total Loss: 3.2901813382421906e-06\n",
      "Total Loss: 2.5881809935728798e-06\n",
      "Total Loss: 2.6544287518643414e-06\n",
      "Total Loss: 2.7923370251098986e-06\n",
      "Total Loss: 2.5237720567222114e-06\n",
      "Total Loss: 2.8618945862035616e-06\n",
      "Total Loss: 2.2395974181361253e-06\n",
      "Total Loss: 2.6805035481124328e-06\n",
      "Total Loss: 2.499090101082402e-06\n",
      "Total Loss: 2.2207609238951895e-06\n",
      "Total Loss: 2.469710074769359e-06\n",
      "Total Loss: 3.4836666726631747e-06\n",
      "Total Loss: 2.3664363703169834e-06\n",
      "Total Loss: 2.4510261356327746e-06\n",
      "Total Loss: 3.081708036410823e-06\n",
      "Total Loss: 2.1479454390929377e-06\n",
      "Total Loss: 2.778629828349545e-06\n",
      "Total Loss: 2.1772816833415873e-06\n",
      "Total Loss: 2.4780337702799848e-06\n",
      "Total Loss: 3.4155973480665125e-06\n",
      "Total Loss: 3.1107573350810626e-06\n",
      "Total Loss: 2.2353506842086467e-06\n",
      "Total Loss: 2.5319266746414037e-06\n",
      "Total Loss: 2.340728921505312e-06\n",
      "Total Loss: 2.0322027643891893e-06\n",
      "Total Loss: 2.558684375091818e-06\n",
      "Total Loss: 2.610334212881248e-06\n",
      "Total Loss: 4.018719500385891e-06\n",
      "Total Loss: 3.2330842461508128e-06\n",
      "Total Loss: 2.5699160409607834e-06\n",
      "Total Loss: 2.8822467912606144e-06\n",
      "Total Loss: 2.133488502901552e-06\n",
      "Total Loss: 2.0816190979328694e-06\n",
      "Total Loss: 1.845091585689968e-06\n",
      "Total Loss: 2.323998134872656e-06\n",
      "Total Loss: 2.2938312314282584e-06\n",
      "Total Loss: 2.1884370274468288e-06\n",
      "Total Loss: 1.8287958912171437e-06\n",
      "Total Loss: 2.7048797366862546e-06\n",
      "Total Loss: 1.867366096222314e-06\n",
      "Total Loss: 2.1919482833254733e-06\n",
      "Total Loss: 2.5034112493611046e-06\n",
      "Total Loss: 2.172594349758583e-06\n",
      "Total Loss: 1.8121924085789942e-06\n",
      "Total Loss: 2.195513982567516e-06\n",
      "Total Loss: 2.1264986445304303e-06\n",
      "Total Loss: 1.402665237846021e-05\n",
      "Total Loss: 1.92006478856557e-05\n",
      "Total Loss: 4.6870328434225585e-06\n",
      "Total Loss: 3.6979853222065785e-06\n",
      "Total Loss: 2.1511336854018737e-06\n",
      "Total Loss: 2.087486614578261e-06\n",
      "Total Loss: 2.693000169529114e-06\n",
      "Total Loss: 1.8598047972773202e-06\n",
      "Total Loss: 1.8302846835164626e-06\n",
      "Total Loss: 3.219305338259599e-06\n",
      "Total Loss: 2.317249873663564e-06\n",
      "Total Loss: 1.7348891358576818e-06\n",
      "Total Loss: 1.6632055298436576e-06\n",
      "Total Loss: 1.6300043752279535e-06\n",
      "Total Loss: 1.590182920254948e-06\n",
      "Total Loss: 1.665722025892238e-06\n",
      "Total Loss: 1.3007337303052837e-06\n",
      "Total Loss: 1.7423228276432685e-06\n",
      "Total Loss: 1.712744455694014e-06\n",
      "Total Loss: 1.7411152839486022e-06\n",
      "Total Loss: 2.414336209868553e-06\n",
      "Total Loss: 1.636290158381194e-06\n"
     ]
    }
   ],
   "source": [
    "for i in range(500):\n",
    "    batch_cnt = 0\n",
    "    total_loss = 0\n",
    "    for label, pointcloud, df in dataloader:\n",
    "        optim.zero_grad()\n",
    "        label = label.cuda()\n",
    "        pointcloud = pointcloud.cuda().float()\n",
    "        df = df.cuda().float()\n",
    "        pred = net(pointcloud, df)\n",
    "        loss = loss_fn(pred, label)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        batch_cnt += 1\n",
    "        total_loss += float(loss)\n",
    "    print(f\"Total Loss: {total_loss/batch_cnt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Test Loss: 1.5425620058806544e-06\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(test_dataset, batch_size=1)\n",
    "batch_cnt = 0\n",
    "total_loss = 0\n",
    "net.eval()\n",
    "labels = []\n",
    "for label, pointcloud, df in dataloader:\n",
    "    label = label.cuda()\n",
    "    pointcloud = pointcloud.cuda().float()\n",
    "    df = df.cuda().float()\n",
    "    pred = net(pointcloud, df)\n",
    "    labels.append(label)\n",
    "    loss = loss_fn(pred, label)\n",
    "    batch_cnt += 1\n",
    "    total_loss += float(loss)\n",
    "print(f\"Total Test Loss: {total_loss/batch_cnt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.pcn.state_dict(), \"../neurals/pcn_model/pcn_df_20.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries([o3d.geometry.PointCloud(o3d.utility.Vector3dVector(training_dataset[10][1]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.04219856, -0.01532705,  0.03520654],\n",
       "       [-0.03941568,  0.10910279, -0.05009749],\n",
       "       [ 0.03348964, -0.15582641,  0.04298963],\n",
       "       ...,\n",
       "       [-0.20387647, -0.08532091,  0.06352441],\n",
       "       [-0.06860191,  0.14903795,  0.05420021],\n",
       "       [ 0.10703852, -0.02935053,  0.06121993]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[10][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('dex_manip')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a13924603cf124e169c00f51f55efb178979cb98f2833cd534c16d33f0a3fe3d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
