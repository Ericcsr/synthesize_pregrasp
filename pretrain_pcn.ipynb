{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: May 20 2022 19:44:17\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from neurals.network import PCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pcds = 40\n",
    "pcds = []\n",
    "dfs = []\n",
    "for i in range(num_pcds):\n",
    "    pcds.append(o3d.io.read_point_cloud(f\"data/seeds_scale/pointclouds/pose_{i:02}_pcd.ply\"))\n",
    "    dfs.append(np.load(f\"data/seeds_scale/pointclouds_df/pose_{i:02}_pcd.npy\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400 2400\n"
     ]
    }
   ],
   "source": [
    "points_list = []\n",
    "dfs_list = []\n",
    "for pcd, df in zip(pcds, dfs):\n",
    "    for i in range(60):\n",
    "        points = np.asarray(pcd.points)\n",
    "        points_index = np.random.choice(len(points), 1024, replace=False)\n",
    "        points = points[points_index]\n",
    "        points_list.append(points)\n",
    "        dfs_list.append(df[points_index])\n",
    "print(len(points_list), len(dfs_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, points_list, dfs_list):\n",
    "        self.pcds = np.asarray(points_list)\n",
    "        self.dfs = np.asarray(dfs_list)\n",
    "        self.mapping = []\n",
    "        for i in range(num_pcds):\n",
    "            self.mapping += [i] * 60\n",
    "        self.mapping = np.array(self.mapping).astype(np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mapping)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        mapping = self.mapping[index]\n",
    "        pcd = self.pcds[index]\n",
    "        df = self.dfs[index]\n",
    "        ret_pcd = pcd + np.random.normal(size=pcd.shape, scale = 0.005)\n",
    "        ret_df = df + np.random.normal(size=df.shape, scale=0.005)\n",
    "        return mapping, ret_pcd, ret_df\n",
    "\n",
    "dataset = TestDataset(points_list, dfs_list)\n",
    "(training_dataset, test_dataset) = \\\n",
    "        torch.utils.data.random_split(\n",
    "            dataset, (2100, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TestNN, self).__init__()\n",
    "        self.pcn = PCN(4,latent_dim=20)\n",
    "        #self.pcn = PointNet2(feat_dim=0)\n",
    "        self.fc1 = nn.Linear(20, 64)\n",
    "        self.out = nn.Linear(64, num_pcds)\n",
    "\n",
    "    def forward(self, pcd, df):\n",
    "        df = df.view([df.shape[0], df.shape[1],1])\n",
    "        pcd = torch.cat([pcd, df], axis=2)\n",
    "        latent = self.pcn(pcd)\n",
    "        x = self.fc1(latent).relu()\n",
    "        x = torch.nn.functional.log_softmax(self.out(x), dim=1)\n",
    "        return x\n",
    "net = TestNN().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "loss_fn = torch.nn.NLLLoss()\n",
    "dataloader = DataLoader(training_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss: 3.693950320735122\n",
      "Total Loss: 3.6900267528765127\n",
      "Total Loss: 3.6841722112713438\n",
      "Total Loss: 3.5747226079305015\n",
      "Total Loss: 3.009791547601873\n",
      "Total Loss: 2.4957367651390308\n",
      "Total Loss: 2.206693613167965\n",
      "Total Loss: 2.011864748868075\n",
      "Total Loss: 1.8696260994130915\n",
      "Total Loss: 1.769778605663415\n",
      "Total Loss: 1.6740519458597356\n",
      "Total Loss: 1.5657452525514546\n",
      "Total Loss: 1.4723020466891201\n",
      "Total Loss: 1.3520053588982783\n",
      "Total Loss: 1.2541904521710945\n",
      "Total Loss: 1.1624847686651982\n",
      "Total Loss: 1.055526650313175\n",
      "Total Loss: 0.9744395234368064\n",
      "Total Loss: 0.9025665124257406\n",
      "Total Loss: 0.8367806293747642\n",
      "Total Loss: 0.7829410415707212\n",
      "Total Loss: 0.7229352575359922\n",
      "Total Loss: 0.733105896097241\n",
      "Total Loss: 0.6748475929101309\n",
      "Total Loss: 0.6430729130903879\n",
      "Total Loss: 0.6409707548040332\n",
      "Total Loss: 0.5595250915397297\n",
      "Total Loss: 0.5181558818528147\n",
      "Total Loss: 0.48966885516137787\n",
      "Total Loss: 0.4768274589018388\n",
      "Total Loss: 0.4483107364538944\n",
      "Total Loss: 0.41718383810736914\n",
      "Total Loss: 0.42485103417526593\n",
      "Total Loss: 0.3753746468009371\n",
      "Total Loss: 0.36447618675954413\n",
      "Total Loss: 0.35010723905129865\n",
      "Total Loss: 0.3548045939568317\n",
      "Total Loss: 0.31757294951063214\n",
      "Total Loss: 0.319292147954305\n",
      "Total Loss: 0.35765571801951435\n",
      "Total Loss: 0.30630574623743695\n",
      "Total Loss: 0.2821106721054424\n",
      "Total Loss: 0.28364858573133295\n",
      "Total Loss: 0.3006387023311673\n",
      "Total Loss: 0.2849504360646913\n",
      "Total Loss: 0.2805787491979021\n",
      "Total Loss: 0.2455337711355903\n",
      "Total Loss: 0.2589093600258683\n",
      "Total Loss: 0.2960145798596469\n",
      "Total Loss: 0.24367959436142084\n",
      "Total Loss: 0.23539956072063156\n",
      "Total Loss: 0.25018868324431504\n",
      "Total Loss: 0.24280358399405624\n",
      "Total Loss: 0.27260711698821094\n",
      "Total Loss: 0.24076966354341217\n",
      "Total Loss: 0.2119359543377703\n",
      "Total Loss: 0.2225505176818732\n",
      "Total Loss: 0.23076623768517465\n",
      "Total Loss: 0.2174102465311686\n",
      "Total Loss: 0.20238336649808017\n",
      "Total Loss: 0.25335388562896033\n",
      "Total Loss: 0.21838190158208212\n",
      "Total Loss: 0.18968602495663095\n",
      "Total Loss: 0.22922085654554944\n",
      "Total Loss: 0.22226748005910354\n",
      "Total Loss: 0.1973573746103229\n",
      "Total Loss: 0.20795614791638922\n",
      "Total Loss: 0.17164374007420105\n",
      "Total Loss: 0.17803157595070926\n",
      "Total Loss: 0.1900161522807497\n",
      "Total Loss: 0.19096481438839075\n",
      "Total Loss: 0.15810257106116324\n",
      "Total Loss: 0.17890979507655808\n",
      "Total Loss: 0.15819775448604065\n",
      "Total Loss: 0.16110385638294797\n",
      "Total Loss: 0.18613135611469095\n",
      "Total Loss: 0.19650227260408978\n",
      "Total Loss: 0.2463174694866845\n",
      "Total Loss: 0.2482412222659949\n",
      "Total Loss: 0.16612903254501749\n",
      "Total Loss: 0.16123503424001462\n",
      "Total Loss: 0.1593038302027818\n",
      "Total Loss: 0.15293329163934244\n",
      "Total Loss: 0.15462392523433222\n",
      "Total Loss: 0.18297225643288006\n",
      "Total Loss: 0.18131846991000752\n",
      "Total Loss: 0.1926781555468386\n",
      "Total Loss: 0.14780726645028952\n",
      "Total Loss: 0.17652899207490863\n",
      "Total Loss: 0.1902426905704267\n",
      "Total Loss: 0.2284434295513413\n",
      "Total Loss: 0.1652731102975932\n",
      "Total Loss: 0.14823180030692706\n",
      "Total Loss: 0.1767975211595044\n",
      "Total Loss: 0.22321800684387033\n",
      "Total Loss: 0.21646542632670113\n",
      "Total Loss: 0.15622332985654022\n",
      "Total Loss: 0.13573364834442284\n",
      "Total Loss: 0.15587940315405527\n",
      "Total Loss: 0.2069139410600518\n",
      "Total Loss: 0.16200421231262613\n",
      "Total Loss: 0.1850337627710718\n",
      "Total Loss: 0.15213964450539966\n",
      "Total Loss: 0.137754937916091\n",
      "Total Loss: 0.12601593475450168\n",
      "Total Loss: 0.12386033537261414\n",
      "Total Loss: 0.16612167758020488\n",
      "Total Loss: 0.1749159614696647\n",
      "Total Loss: 0.1600663440697121\n",
      "Total Loss: 0.13812946686238953\n",
      "Total Loss: 0.16634976051070474\n",
      "Total Loss: 0.18865448936368479\n",
      "Total Loss: 0.15296759659593756\n",
      "Total Loss: 0.16309736669063568\n",
      "Total Loss: 0.20752161899299332\n",
      "Total Loss: 0.1599126776511019\n",
      "Total Loss: 0.16977276675628894\n",
      "Total Loss: 0.12645599020249915\n",
      "Total Loss: 0.13616425306959587\n",
      "Total Loss: 0.12006705423647707\n",
      "Total Loss: 0.15917004317496763\n",
      "Total Loss: 0.18161108954386276\n",
      "Total Loss: 0.19950705179662415\n",
      "Total Loss: 0.1427719464356249\n",
      "Total Loss: 0.1578749975923336\n",
      "Total Loss: 0.18304583904418079\n",
      "Total Loss: 0.12806460893515384\n",
      "Total Loss: 0.12382663628368666\n",
      "Total Loss: 0.13395274836908688\n",
      "Total Loss: 0.11424704110532095\n",
      "Total Loss: 0.12243933445124915\n",
      "Total Loss: 0.17425630106167359\n",
      "Total Loss: 0.16522418865651795\n",
      "Total Loss: 0.1105782635735743\n",
      "Total Loss: 0.13188189448732318\n",
      "Total Loss: 0.14438515438726454\n",
      "Total Loss: 0.13270163852157016\n",
      "Total Loss: 0.12261191902287079\n",
      "Total Loss: 0.14543425765904514\n",
      "Total Loss: 0.140551299195398\n",
      "Total Loss: 0.1419485036396619\n",
      "Total Loss: 0.11848055932557944\n",
      "Total Loss: 0.1521595438772982\n",
      "Total Loss: 0.11128368872133168\n",
      "Total Loss: 0.14188946862563942\n",
      "Total Loss: 0.12546632066369057\n",
      "Total Loss: 0.1462022957928253\n",
      "Total Loss: 0.19641589802322965\n",
      "Total Loss: 0.1919537638172959\n",
      "Total Loss: 0.11675563996488397\n",
      "Total Loss: 0.11246825054739461\n",
      "Total Loss: 0.10339914307449803\n",
      "Total Loss: 0.12763364138928326\n",
      "Total Loss: 0.1050622280348431\n",
      "Total Loss: 0.12820363056027528\n",
      "Total Loss: 0.12295494849483173\n",
      "Total Loss: 0.1533023358294458\n",
      "Total Loss: 0.12943117376981358\n",
      "Total Loss: 0.2117514612548279\n",
      "Total Loss: 0.1665146957066926\n",
      "Total Loss: 0.13144870644265955\n",
      "Total Loss: 0.1268515426552657\n",
      "Total Loss: 0.12955028067032495\n",
      "Total Loss: 0.11280111426656897\n",
      "Total Loss: 0.1086225945389632\n",
      "Total Loss: 0.11144578964872794\n",
      "Total Loss: 0.09897984632036903\n",
      "Total Loss: 0.10911908361947897\n",
      "Total Loss: 0.12437020230925444\n",
      "Total Loss: 0.1945322578152021\n",
      "Total Loss: 0.17024787673444458\n",
      "Total Loss: 0.12276799581719167\n",
      "Total Loss: 0.12049275546362906\n",
      "Total Loss: 0.11427558089296024\n",
      "Total Loss: 0.11781018441825201\n",
      "Total Loss: 0.11041446967106877\n",
      "Total Loss: 0.12884920033993144\n",
      "Total Loss: 0.14661502555915804\n",
      "Total Loss: 0.1538234461437572\n",
      "Total Loss: 0.13774715126915413\n",
      "Total Loss: 0.1814219176091931\n",
      "Total Loss: 0.15931311735149586\n",
      "Total Loss: 0.1097363309200966\n",
      "Total Loss: 0.13182337785309012\n",
      "Total Loss: 0.17347207055850464\n",
      "Total Loss: 0.188747664405541\n",
      "Total Loss: 0.09718687911376808\n",
      "Total Loss: 0.10700700488505942\n",
      "Total Loss: 0.10804742495670464\n",
      "Total Loss: 0.09964348691882509\n",
      "Total Loss: 0.10816909474405376\n",
      "Total Loss: 0.13028637816508612\n",
      "Total Loss: 0.1261103945699605\n",
      "Total Loss: 0.10441560794909795\n",
      "Total Loss: 0.1357705599882386\n",
      "Total Loss: 0.17681943692944266\n",
      "Total Loss: 0.20456118407574567\n",
      "Total Loss: 0.17854743337992465\n",
      "Total Loss: 0.10764629608302405\n",
      "Total Loss: 0.09724795750596306\n",
      "Total Loss: 0.11466848522876248\n",
      "Total Loss: 0.10824766089067314\n",
      "Total Loss: 0.13751428596901172\n",
      "Total Loss: 0.13483648893959593\n",
      "Total Loss: 0.10766069229805109\n",
      "Total Loss: 0.10617059627265642\n",
      "Total Loss: 0.13628827362801088\n",
      "Total Loss: 0.19761372888178536\n",
      "Total Loss: 0.3485482990967505\n",
      "Total Loss: 0.12620940589995094\n",
      "Total Loss: 0.1129467368803241\n",
      "Total Loss: 0.09542300631151054\n",
      "Total Loss: 0.09836749924403249\n",
      "Total Loss: 0.10794176375775626\n",
      "Total Loss: 0.11998817166595747\n",
      "Total Loss: 0.12519574492718233\n",
      "Total Loss: 0.10819366269490936\n",
      "Total Loss: 0.10519050095569003\n",
      "Total Loss: 0.10667868432673541\n",
      "Total Loss: 0.11498194402365973\n",
      "Total Loss: 0.21643418762268443\n",
      "Total Loss: 0.13214560812621406\n",
      "Total Loss: 0.11945253362258275\n",
      "Total Loss: 0.09889047017151659\n",
      "Total Loss: 0.09504871792865521\n",
      "Total Loss: 0.0903282780764681\n",
      "Total Loss: 0.10706315223466266\n",
      "Total Loss: 0.1133176518434828\n",
      "Total Loss: 0.11264483619368437\n",
      "Total Loss: 0.22469196592768034\n",
      "Total Loss: 0.19578993726860394\n",
      "Total Loss: 0.1180798216073802\n",
      "Total Loss: 0.09168775289347678\n",
      "Total Loss: 0.10324006098689455\n",
      "Total Loss: 0.12067637965083122\n",
      "Total Loss: 0.1258629145044269\n",
      "Total Loss: 0.13712266979343962\n",
      "Total Loss: 0.1138533293975122\n",
      "Total Loss: 0.11695865846493027\n",
      "Total Loss: 0.11378921444217364\n",
      "Total Loss: 0.09491046992215244\n",
      "Total Loss: 0.1130210038161639\n",
      "Total Loss: 0.11160909802173123\n",
      "Total Loss: 0.10835750423597568\n",
      "Total Loss: 0.10423408189054692\n",
      "Total Loss: 0.09401230536627048\n",
      "Total Loss: 0.09852571006525647\n",
      "Total Loss: 0.10476008734919807\n",
      "Total Loss: 0.09437792375683784\n",
      "Total Loss: 0.10831484119548943\n",
      "Total Loss: 0.13007619299671866\n",
      "Total Loss: 0.11242738975719972\n",
      "Total Loss: 0.12000410044283578\n",
      "Total Loss: 0.12740723438786739\n",
      "Total Loss: 0.1211320087313652\n",
      "Total Loss: 0.11869465695186095\n",
      "Total Loss: 0.1057622375136072\n",
      "Total Loss: 0.11857259104197676\n",
      "Total Loss: 0.1029297641732476\n",
      "Total Loss: 0.10277045202074629\n",
      "Total Loss: 0.15838565903179574\n",
      "Total Loss: 0.21819371741377946\n",
      "Total Loss: 0.15967595915902744\n",
      "Total Loss: 0.11609577015042305\n",
      "Total Loss: 0.09921721594803261\n",
      "Total Loss: 0.08818029589725263\n",
      "Total Loss: 0.09268885286468448\n",
      "Total Loss: 0.10005249986142824\n",
      "Total Loss: 0.10759262469681827\n",
      "Total Loss: 0.09452150152488188\n",
      "Total Loss: 0.09366898712786761\n",
      "Total Loss: 0.10009319459398587\n",
      "Total Loss: 0.11623167675552946\n",
      "Total Loss: 0.11024873545675566\n",
      "Total Loss: 0.1091407449408011\n",
      "Total Loss: 0.14799624387965057\n",
      "Total Loss: 0.08954799130107417\n",
      "Total Loss: 0.10293075622934283\n",
      "Total Loss: 0.10649498755281622\n",
      "Total Loss: 0.14187410914085127\n",
      "Total Loss: 0.10312132119680896\n",
      "Total Loss: 0.11675124739607175\n",
      "Total Loss: 0.1067603092753526\n",
      "Total Loss: 0.09200091806776596\n",
      "Total Loss: 0.112992565345132\n",
      "Total Loss: 0.10953013057058508\n",
      "Total Loss: 0.104969520347588\n",
      "Total Loss: 0.09743342796961467\n",
      "Total Loss: 0.09918561126246597\n",
      "Total Loss: 0.09963257226980093\n",
      "Total Loss: 0.09538181047096397\n",
      "Total Loss: 0.10006957896279567\n",
      "Total Loss: 0.16225264740712714\n",
      "Total Loss: 0.1425579306528424\n",
      "Total Loss: 0.15812584057901846\n",
      "Total Loss: 0.09594474450656862\n",
      "Total Loss: 0.08710276674140584\n",
      "Total Loss: 0.09913008942297011\n",
      "Total Loss: 0.09135760457226724\n",
      "Total Loss: 0.10214671876394388\n",
      "Total Loss: 0.09506239753329393\n",
      "Total Loss: 0.08758251171446207\n",
      "Total Loss: 0.13376204929117\n",
      "Total Loss: 0.19868785696047725\n",
      "Total Loss: 0.1187932902213299\n",
      "Total Loss: 0.0963929808049491\n",
      "Total Loss: 0.08807001477389624\n",
      "Total Loss: 0.09339655709989143\n",
      "Total Loss: 0.12903814525766807\n",
      "Total Loss: 0.1536961044325973\n",
      "Total Loss: 0.11313458441784888\n",
      "Total Loss: 0.10635094897765102\n",
      "Total Loss: 0.09196892030762904\n",
      "Total Loss: 0.08185473423112523\n",
      "Total Loss: 0.09176022751313268\n",
      "Total Loss: 0.08859973028302193\n",
      "Total Loss: 0.12852812468102484\n",
      "Total Loss: 0.1627200266177004\n",
      "Total Loss: 0.13602658462795345\n",
      "Total Loss: 0.10123211721127684\n",
      "Total Loss: 0.10647440430792895\n",
      "Total Loss: 0.10054278272119435\n",
      "Total Loss: 0.09071220593018965\n",
      "Total Loss: 0.103210122409192\n",
      "Total Loss: 0.09024535774281531\n",
      "Total Loss: 0.08235743156436717\n",
      "Total Loss: 0.08358979721864064\n",
      "Total Loss: 0.10310588111028526\n",
      "Total Loss: 0.0885308575675343\n",
      "Total Loss: 0.08542992270579844\n",
      "Total Loss: 0.08650700081930016\n",
      "Total Loss: 0.12828114532837362\n",
      "Total Loss: 0.28132770165349497\n",
      "Total Loss: 0.11240926204305707\n",
      "Total Loss: 0.0985874543813142\n",
      "Total Loss: 0.08351090747975942\n",
      "Total Loss: 0.09288931197740814\n",
      "Total Loss: 0.10523742279320052\n",
      "Total Loss: 0.09327255674835408\n",
      "Total Loss: 0.0827471087827827\n",
      "Total Loss: 0.08132245837513244\n",
      "Total Loss: 0.11671816151250493\n",
      "Total Loss: 0.08041477767807065\n",
      "Total Loss: 0.09217830927986087\n",
      "Total Loss: 0.10087632455609062\n",
      "Total Loss: 0.0868411459254496\n",
      "Total Loss: 0.10494446122285092\n",
      "Total Loss: 0.12854993704593543\n",
      "Total Loss: 0.07819426415318792\n",
      "Total Loss: 0.09030380274987582\n",
      "Total Loss: 0.07990157993679697\n",
      "Total Loss: 0.09825639279954361\n",
      "Total Loss: 0.10805405467522866\n",
      "Total Loss: 0.08416512156977798\n",
      "Total Loss: 0.1309388019708973\n",
      "Total Loss: 0.08730043363616322\n",
      "Total Loss: 0.08515454512653929\n",
      "Total Loss: 0.09043256870724938\n",
      "Total Loss: 0.08153626695275307\n",
      "Total Loss: 0.08548401126807387\n",
      "Total Loss: 0.09102008175669295\n",
      "Total Loss: 0.11477869410406459\n",
      "Total Loss: 0.0923648893155835\n",
      "Total Loss: 0.08818015580375989\n",
      "Total Loss: 0.10833878205581145\n",
      "Total Loss: 0.17216907610947435\n",
      "Total Loss: 0.1448546303718379\n",
      "Total Loss: 0.09790884003494725\n",
      "Total Loss: 0.07603273489935831\n",
      "Total Loss: 0.07233532320595149\n",
      "Total Loss: 0.07847414922082063\n",
      "Total Loss: 0.08758102413831335\n",
      "Total Loss: 0.12521288764070382\n",
      "Total Loss: 0.13565573045475918\n",
      "Total Loss: 0.08360636409259203\n",
      "Total Loss: 0.07332606331417055\n",
      "Total Loss: 0.08183333971960978\n",
      "Total Loss: 0.08954510730550144\n",
      "Total Loss: 0.08350929420328501\n",
      "Total Loss: 0.06674072253658916\n",
      "Total Loss: 0.0660920198442358\n",
      "Total Loss: 0.07552005085303928\n",
      "Total Loss: 0.07981405119326981\n",
      "Total Loss: 0.10134368779307062\n",
      "Total Loss: 0.10553032577489362\n",
      "Total Loss: 0.11087352195472429\n",
      "Total Loss: 0.07131563087530209\n",
      "Total Loss: 0.07536655269337422\n",
      "Total Loss: 0.08742515440804488\n",
      "Total Loss: 0.06632770281849486\n",
      "Total Loss: 0.09522736924841549\n",
      "Total Loss: 0.06826040809127418\n",
      "Total Loss: 0.07408951310384454\n",
      "Total Loss: 0.05876136356682488\n",
      "Total Loss: 0.07772233492384355\n",
      "Total Loss: 0.0777726432477886\n",
      "Total Loss: 0.09996749737271757\n",
      "Total Loss: 0.0906339729712768\n",
      "Total Loss: 0.07163122736594894\n",
      "Total Loss: 0.06702860676203713\n",
      "Total Loss: 0.10260388909867316\n",
      "Total Loss: 0.13017595373094082\n",
      "Total Loss: 0.1866843282160434\n",
      "Total Loss: 0.16512912940798383\n",
      "Total Loss: 0.11157470066664797\n",
      "Total Loss: 0.07196171683343974\n",
      "Total Loss: 0.06343462577823436\n",
      "Total Loss: 0.06608143025501208\n",
      "Total Loss: 0.06174715292273146\n",
      "Total Loss: 0.06643450008987477\n",
      "Total Loss: 0.06537458012727174\n",
      "Total Loss: 0.08272813356509714\n",
      "Total Loss: 0.060717796511722336\n",
      "Total Loss: 0.05853658750879042\n",
      "Total Loss: 0.06740193243956927\n",
      "Total Loss: 0.07793205482603023\n",
      "Total Loss: 0.07422419479399016\n",
      "Total Loss: 0.07195448779472799\n",
      "Total Loss: 0.06877388957549226\n",
      "Total Loss: 0.06871657186385358\n",
      "Total Loss: 0.0643216162694223\n",
      "Total Loss: 0.18362705262772966\n",
      "Total Loss: 0.2261390478322\n",
      "Total Loss: 0.07652725764747823\n",
      "Total Loss: 0.0768369177187031\n",
      "Total Loss: 0.0634080542985237\n",
      "Total Loss: 0.07070098241621797\n",
      "Total Loss: 0.05698017911477522\n",
      "Total Loss: 0.06715043538221807\n",
      "Total Loss: 0.04998910269050887\n",
      "Total Loss: 0.055396248530032055\n",
      "Total Loss: 0.05551301100940415\n",
      "Total Loss: 0.052326168469858894\n",
      "Total Loss: 0.05438971945620848\n",
      "Total Loss: 0.045915421042026894\n",
      "Total Loss: 0.06917068379169161\n",
      "Total Loss: 0.06660405828645735\n",
      "Total Loss: 0.06731506775725972\n",
      "Total Loss: 0.09262658441157053\n",
      "Total Loss: 0.08503027113549637\n",
      "Total Loss: 0.11136214559276898\n",
      "Total Loss: 0.1429551329576608\n",
      "Total Loss: 0.05976278848494544\n",
      "Total Loss: 0.06893878915544713\n",
      "Total Loss: 0.05853989703411406\n",
      "Total Loss: 0.052857155431852196\n",
      "Total Loss: 0.06998819936857079\n",
      "Total Loss: 0.06272090779560985\n",
      "Total Loss: 0.09089584180125684\n",
      "Total Loss: 0.05295039634361411\n",
      "Total Loss: 0.05103551037609577\n",
      "Total Loss: 0.051603684540499344\n",
      "Total Loss: 0.05556661517105319\n",
      "Total Loss: 0.061030415930982795\n",
      "Total Loss: 0.053731626689885605\n",
      "Total Loss: 0.04895185639686657\n",
      "Total Loss: 0.04841573654927991\n",
      "Total Loss: 0.05984242491875634\n",
      "Total Loss: 0.04632144079854091\n",
      "Total Loss: 0.05590004984740958\n",
      "Total Loss: 0.049186627408771805\n",
      "Total Loss: 0.06753799319267273\n",
      "Total Loss: 0.11822806806726889\n",
      "Total Loss: 0.10406742069983121\n",
      "Total Loss: 0.05056279683203408\n",
      "Total Loss: 0.057760308045103695\n",
      "Total Loss: 0.05498572512332237\n",
      "Total Loss: 0.05252812359707825\n",
      "Total Loss: 0.0497256668397423\n",
      "Total Loss: 0.04574729327225324\n",
      "Total Loss: 0.08403490873223002\n",
      "Total Loss: 0.10995406663101731\n",
      "Total Loss: 0.05927776974259001\n",
      "Total Loss: 0.046102926854721525\n",
      "Total Loss: 0.04140040323589787\n",
      "Total Loss: 0.04303681991542831\n",
      "Total Loss: 0.0564388811136737\n",
      "Total Loss: 0.04391497709421498\n",
      "Total Loss: 0.05587411748076027\n",
      "Total Loss: 0.048445423331224556\n",
      "Total Loss: 0.05356007046771772\n",
      "Total Loss: 0.04526004872538827\n",
      "Total Loss: 0.048320535316386005\n",
      "Total Loss: 0.05319754451964841\n",
      "Total Loss: 0.049051052975383674\n",
      "Total Loss: 0.040091173610452446\n",
      "Total Loss: 0.040860500102016056\n",
      "Total Loss: 0.04277800593638059\n",
      "Total Loss: 0.0589747495497718\n",
      "Total Loss: 0.042761241791374756\n",
      "Total Loss: 0.09920661501360661\n",
      "Total Loss: 0.14220888760279526\n",
      "Total Loss: 0.12153156773384774\n",
      "Total Loss: 0.04654969012534076\n",
      "Total Loss: 0.04573650296890375\n",
      "Total Loss: 0.04582049068289273\n",
      "Total Loss: 0.047659161980405\n",
      "Total Loss: 0.039443036103903345\n",
      "Total Loss: 0.040839874354953114\n",
      "Total Loss: 0.044182063404922235\n"
     ]
    }
   ],
   "source": [
    "for i in range(500):\n",
    "    batch_cnt = 0\n",
    "    total_loss = 0\n",
    "    for label, pointcloud, df in dataloader:\n",
    "        optim.zero_grad()\n",
    "        label = label.cuda()\n",
    "        pointcloud = pointcloud.cuda().float()\n",
    "        df = df.cuda().float()\n",
    "        pred = net(pointcloud, df)\n",
    "        loss = loss_fn(pred, label)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        batch_cnt += 1\n",
    "        total_loss += float(loss)\n",
    "    print(f\"Total Loss: {total_loss/batch_cnt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Test Loss: 0.05825124947567583\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(test_dataset, batch_size=1)\n",
    "batch_cnt = 0\n",
    "total_loss = 0\n",
    "net.eval()\n",
    "labels = []\n",
    "for label, pointcloud, df in dataloader:\n",
    "    label = label.cuda()\n",
    "    pointcloud = pointcloud.cuda().float()\n",
    "    df = df.cuda().float()\n",
    "    pred = net(pointcloud, df)\n",
    "    labels.append(label)\n",
    "    loss = loss_fn(pred, label)\n",
    "    batch_cnt += 1\n",
    "    total_loss += float(loss)\n",
    "print(f\"Total Test Loss: {total_loss/batch_cnt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "torch.save(net.pcn.state_dict(), \"neurals/pcn_model/pcn_scale_20.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries([o3d.geometry.PointCloud(o3d.utility.Vector3dVector(training_dataset[10][1]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.04219856, -0.01532705,  0.03520654],\n",
       "       [-0.03941568,  0.10910279, -0.05009749],\n",
       "       [ 0.03348964, -0.15582641,  0.04298963],\n",
       "       ...,\n",
       "       [-0.20387647, -0.08532091,  0.06352441],\n",
       "       [-0.06860191,  0.14903795,  0.05420021],\n",
       "       [ 0.10703852, -0.02935053,  0.06121993]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[10][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('dex')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "00b8f4a69a0ca14138cc33727ee41215cd45af553930c27cfc01041823663372"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
