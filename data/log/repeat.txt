nohup: ignoring input
pybullet build time: May 20 2022 19:44:17
Iteration 1 took 2.88 seconds (mean sampled reward: -4445.86). Current reward after update: -2020.01, Optimal reward -2020.01
Iteration 2 took 2.86 seconds (mean sampled reward: -4428.55). Current reward after update: -1661.77, Optimal reward -1661.77
Iteration 3 took 2.76 seconds (mean sampled reward: -4350.46). Current reward after update: -1268.22, Optimal reward -1268.22
Iteration 4 took 2.95 seconds (mean sampled reward: -4358.49). Current reward after update: -1324.97, Optimal reward -1268.22
Iteration 5 took 2.84 seconds (mean sampled reward: -4308.96). Current reward after update: -1250.81, Optimal reward -1250.81
Iteration 6 took 3.12 seconds (mean sampled reward: -4245.43). Current reward after update: -1144.20, Optimal reward -1144.20
Iteration 7 took 3.23 seconds (mean sampled reward: -4255.94). Current reward after update: -1013.66, Optimal reward -1013.66
Iteration 8 took 3.20 seconds (mean sampled reward: -4324.39). Current reward after update: -1222.61, Optimal reward -1013.66
Iteration 9 took 2.89 seconds (mean sampled reward: -4299.85). Current reward after update: -1185.87, Optimal reward -1013.66
Iteration 10 took 3.04 seconds (mean sampled reward: -4372.21). Current reward after update: -1098.82, Optimal reward -1013.66
Iteration 11 took 2.80 seconds (mean sampled reward: -4326.72). Current reward after update: -1205.36, Optimal reward -1013.66
Iteration 12 took 3.69 seconds (mean sampled reward: -4261.47). Current reward after update: -1078.42, Optimal reward -1013.66
Iteration 13 took 3.44 seconds (mean sampled reward: -4405.70). Current reward after update: -1343.87, Optimal reward -1013.66
Iteration 14 took 3.37 seconds (mean sampled reward: -4315.90). Current reward after update: -1216.47, Optimal reward -1013.66
Iteration 15 took 3.60 seconds (mean sampled reward: -4199.26). Current reward after update: -1064.66, Optimal reward -1013.66
Iteration 16 took 3.31 seconds (mean sampled reward: -4193.19). Current reward after update: -1154.79, Optimal reward -1013.66
Iteration 17 took 3.25 seconds (mean sampled reward: -4257.75). Current reward after update: -1136.21, Optimal reward -1013.66
Iteration 18 took 3.21 seconds (mean sampled reward: -4239.23). Current reward after update: -1097.13, Optimal reward -1013.66
Iteration 19 took 2.67 seconds (mean sampled reward: -4310.99). Current reward after update: -941.95, Optimal reward -941.95
Iteration 20 took 2.77 seconds (mean sampled reward: -4114.60). Current reward after update: -887.77, Optimal reward -887.77
Iteration 21 took 2.62 seconds (mean sampled reward: -4139.90). Current reward after update: -983.62, Optimal reward -887.77
Iteration 22 took 2.74 seconds (mean sampled reward: -4136.14). Current reward after update: -933.64, Optimal reward -887.77
Iteration 23 took 2.64 seconds (mean sampled reward: -4215.88). Current reward after update: -883.09, Optimal reward -883.09
Iteration 24 took 2.74 seconds (mean sampled reward: -4262.41). Current reward after update: -1049.33, Optimal reward -883.09
Iteration 25 took 2.79 seconds (mean sampled reward: -4265.13). Current reward after update: -1143.12, Optimal reward -883.09
Iteration 26 took 2.73 seconds (mean sampled reward: -4128.33). Current reward after update: -1132.61, Optimal reward -883.09
Iteration 27 took 2.98 seconds (mean sampled reward: -4267.34). Current reward after update: -1410.59, Optimal reward -883.09
Iteration 28 took 2.73 seconds (mean sampled reward: -4282.75). Current reward after update: -1213.10, Optimal reward -883.09
Iteration 29 took 2.97 seconds (mean sampled reward: -4099.16). Current reward after update: -1060.26, Optimal reward -883.09
Iteration 30 took 3.03 seconds (mean sampled reward: -4066.75). Current reward after update: -1028.40, Optimal reward -883.09
Iteration 31 took 2.76 seconds (mean sampled reward: -4227.38). Current reward after update: -1051.97, Optimal reward -883.09
Iteration 32 took 2.66 seconds (mean sampled reward: -4185.17). Current reward after update: -633.83, Optimal reward -633.83
Iteration 33 took 2.86 seconds (mean sampled reward: -4315.08). Current reward after update: -1086.92, Optimal reward -633.83
Iteration 34 took 2.77 seconds (mean sampled reward: -4205.99). Current reward after update: -802.65, Optimal reward -633.83
Iteration 35 took 2.65 seconds (mean sampled reward: -4194.81). Current reward after update: -833.76, Optimal reward -633.83
Iteration 36 took 2.61 seconds (mean sampled reward: -4317.97). Current reward after update: -748.74, Optimal reward -633.83
Iteration 37 took 2.76 seconds (mean sampled reward: -4346.24). Current reward after update: -1048.06, Optimal reward -633.83
Iteration 38 took 2.66 seconds (mean sampled reward: -4345.49). Current reward after update: -436.66, Optimal reward -436.66
Iteration 39 took 2.57 seconds (mean sampled reward: -4227.18). Current reward after update: -907.56, Optimal reward -436.66
Iteration 40 took 2.57 seconds (mean sampled reward: -4222.99). Current reward after update: -716.72, Optimal reward -436.66
Iteration 41 took 2.69 seconds (mean sampled reward: -4285.48). Current reward after update: -1609.64, Optimal reward -436.66
Iteration 42 took 2.90 seconds (mean sampled reward: -4221.07). Current reward after update: -1501.78, Optimal reward -436.66
Iteration 43 took 2.68 seconds (mean sampled reward: -4156.32). Current reward after update: -1482.12, Optimal reward -436.66
Iteration 44 took 2.83 seconds (mean sampled reward: -4322.12). Current reward after update: -1264.01, Optimal reward -436.66
Iteration 45 took 2.73 seconds (mean sampled reward: -4249.81). Current reward after update: -877.03, Optimal reward -436.66
Iteration 46 took 2.67 seconds (mean sampled reward: -4321.01). Current reward after update: -1267.50, Optimal reward -436.66
Iteration 47 took 2.67 seconds (mean sampled reward: -4369.88). Current reward after update: -1058.75, Optimal reward -436.66
Iteration 48 took 2.56 seconds (mean sampled reward: -4356.57). Current reward after update: -794.13, Optimal reward -436.66
Iteration 49 took 2.77 seconds (mean sampled reward: -4354.91). Current reward after update: -827.25, Optimal reward -436.66
Iteration 50 took 2.77 seconds (mean sampled reward: -4437.93). Current reward after update: -776.85, Optimal reward -436.66
Iteration 51 took 2.91 seconds (mean sampled reward: -4408.14). Current reward after update: -817.56, Optimal reward -436.66
Iteration 52 took 2.94 seconds (mean sampled reward: -4379.74). Current reward after update: -798.41, Optimal reward -436.66
Iteration 53 took 2.64 seconds (mean sampled reward: -4392.18). Current reward after update: -721.02, Optimal reward -436.66
Iteration 54 took 2.79 seconds (mean sampled reward: -4375.96). Current reward after update: -736.06, Optimal reward -436.66
Iteration 55 took 2.98 seconds (mean sampled reward: -4304.41). Current reward after update: -1113.47, Optimal reward -436.66
Iteration 56 took 3.03 seconds (mean sampled reward: -4391.36). Current reward after update: -1107.83, Optimal reward -436.66
Iteration 57 took 2.78 seconds (mean sampled reward: -4382.51). Current reward after update: -1113.13, Optimal reward -436.66
Iteration 58 took 2.71 seconds (mean sampled reward: -4414.45). Current reward after update: -1507.19, Optimal reward -436.66
Iteration 59 took 2.52 seconds (mean sampled reward: -4385.86). Current reward after update: -1302.44, Optimal reward -436.66
Iteration 60 took 2.93 seconds (mean sampled reward: -4397.62). Current reward after update: -1385.73, Optimal reward -436.66
Iteration 61 took 2.93 seconds (mean sampled reward: -4364.00). Current reward after update: -823.94, Optimal reward -436.66
Iteration 62 took 3.03 seconds (mean sampled reward: -4325.57). Current reward after update: -695.41, Optimal reward -436.66
Iteration 63 took 2.72 seconds (mean sampled reward: -4325.15). Current reward after update: -731.77, Optimal reward -436.66
Iteration 64 took 2.98 seconds (mean sampled reward: -4349.38). Current reward after update: -803.04, Optimal reward -436.66
Iteration 65 took 2.76 seconds (mean sampled reward: -4319.64). Current reward after update: -795.87, Optimal reward -436.66
Iteration 66 took 3.06 seconds (mean sampled reward: -4369.86). Current reward after update: -777.12, Optimal reward -436.66
Iteration 67 took 2.96 seconds (mean sampled reward: -4310.12). Current reward after update: -1082.48, Optimal reward -436.66
Iteration 68 took 2.97 seconds (mean sampled reward: -4384.10). Current reward after update: -1358.38, Optimal reward -436.66
Iteration 69 took 2.82 seconds (mean sampled reward: -4372.79). Current reward after update: -772.67, Optimal reward -436.66
Iteration 70 took 2.75 seconds (mean sampled reward: -4329.41). Current reward after update: -750.93, Optimal reward -436.66
Iteration 71 took 2.85 seconds (mean sampled reward: -4385.11). Current reward after update: -767.71, Optimal reward -436.66
Iteration 72 took 2.73 seconds (mean sampled reward: -4414.09). Current reward after update: -807.59, Optimal reward -436.66
Iteration 73 took 2.79 seconds (mean sampled reward: -4424.91). Current reward after update: -714.43, Optimal reward -436.66
Iteration 74 took 2.82 seconds (mean sampled reward: -4394.46). Current reward after update: -698.65, Optimal reward -436.66
Iteration 75 took 2.89 seconds (mean sampled reward: -4384.17). Current reward after update: -917.15, Optimal reward -436.66
Iteration 76 took 2.83 seconds (mean sampled reward: -4494.25). Current reward after update: -1473.26, Optimal reward -436.66
Iteration 77 took 2.88 seconds (mean sampled reward: -4480.05). Current reward after update: -788.63, Optimal reward -436.66
Iteration 78 took 2.96 seconds (mean sampled reward: -4518.02). Current reward after update: -798.52, Optimal reward -436.66
Iteration 79 took 2.91 seconds (mean sampled reward: -4499.46). Current reward after update: -1558.75, Optimal reward -436.66
Iteration 80 took 3.22 seconds (mean sampled reward: -4487.39). Current reward after update: -1380.16, Optimal reward -436.66
Iteration 81 took 2.74 seconds (mean sampled reward: -4386.96). Current reward after update: -1350.99, Optimal reward -436.66
Iteration 82 took 2.82 seconds (mean sampled reward: -4378.41). Current reward after update: -1482.38, Optimal reward -436.66
Iteration 83 took 2.76 seconds (mean sampled reward: -4393.63). Current reward after update: -1035.64, Optimal reward -436.66
Iteration 84 took 2.62 seconds (mean sampled reward: -4346.74). Current reward after update: -1045.72, Optimal reward -436.66
Iteration 85 took 2.82 seconds (mean sampled reward: -4316.68). Current reward after update: -1000.34, Optimal reward -436.66
Iteration 86 took 2.75 seconds (mean sampled reward: -4275.81). Current reward after update: -1075.43, Optimal reward -436.66
Iteration 87 took 3.03 seconds (mean sampled reward: -4306.13). Current reward after update: -1084.77, Optimal reward -436.66
Iteration 88 took 2.85 seconds (mean sampled reward: -4358.97). Current reward after update: -965.52, Optimal reward -436.66
Iteration 89 took 2.68 seconds (mean sampled reward: -4256.55). Current reward after update: -912.08, Optimal reward -436.66
Iteration 90 took 2.83 seconds (mean sampled reward: -4300.43). Current reward after update: -1266.74, Optimal reward -436.66
Iteration 91 took 2.86 seconds (mean sampled reward: -4332.45). Current reward after update: -892.97, Optimal reward -436.66
Iteration 92 took 2.87 seconds (mean sampled reward: -4431.75). Current reward after update: -824.22, Optimal reward -436.66
Iteration 93 took 2.70 seconds (mean sampled reward: -4464.41). Current reward after update: -1537.18, Optimal reward -436.66
Iteration 94 took 2.86 seconds (mean sampled reward: -4425.97). Current reward after update: -1376.26, Optimal reward -436.66
Iteration 95 took 2.83 seconds (mean sampled reward: -4363.80). Current reward after update: -1371.76, Optimal reward -436.66
Iteration 96 took 2.83 seconds (mean sampled reward: -4324.46). Current reward after update: -1247.08, Optimal reward -436.66
Iteration 97 took 2.85 seconds (mean sampled reward: -4348.24). Current reward after update: -1352.45, Optimal reward -436.66
Iteration 98 took 3.08 seconds (mean sampled reward: -4317.76). Current reward after update: -1026.31, Optimal reward -436.66
Iteration 99 took 2.87 seconds (mean sampled reward: -4337.61). Current reward after update: -1092.28, Optimal reward -436.66
Iteration 100 took 3.13 seconds (mean sampled reward: -4332.62). Current reward after update: -1173.53, Optimal reward -436.66
Iteration 101 took 3.16 seconds (mean sampled reward: -4321.51). Current reward after update: -1236.81, Optimal reward -436.66
Iteration 102 took 2.93 seconds (mean sampled reward: -4323.98). Current reward after update: -1222.60, Optimal reward -436.66
Iteration 103 took 2.82 seconds (mean sampled reward: -4298.42). Current reward after update: -1054.08, Optimal reward -436.66
Iteration 104 took 3.03 seconds (mean sampled reward: -4230.82). Current reward after update: -1228.49, Optimal reward -436.66
Iteration 105 took 2.76 seconds (mean sampled reward: -4256.58). Current reward after update: -2855.37, Optimal reward -436.66
Iteration 106 took 2.88 seconds (mean sampled reward: -4272.87). Current reward after update: -1298.04, Optimal reward -436.66
Iteration 107 took 3.14 seconds (mean sampled reward: -4268.47). Current reward after update: -1192.00, Optimal reward -436.66
Iteration 108 took 3.17 seconds (mean sampled reward: -4221.99). Current reward after update: -1162.20, Optimal reward -436.66
Iteration 109 took 2.83 seconds (mean sampled reward: -4329.04). Current reward after update: -1057.59, Optimal reward -436.66
Iteration 110 took 2.95 seconds (mean sampled reward: -4288.53). Current reward after update: -1156.66, Optimal reward -436.66
Iteration 111 took 2.94 seconds (mean sampled reward: -4340.30). Current reward after update: -1307.41, Optimal reward -436.66
Iteration 112 took 2.83 seconds (mean sampled reward: -4321.50). Current reward after update: -1328.02, Optimal reward -436.66
Iteration 113 took 3.03 seconds (mean sampled reward: -4215.68). Current reward after update: -1191.68, Optimal reward -436.66
Iteration 114 took 2.79 seconds (mean sampled reward: -4319.24). Current reward after update: -1168.84, Optimal reward -436.66
Iteration 115 took 2.82 seconds (mean sampled reward: -4278.19). Current reward after update: -1126.01, Optimal reward -436.66
Iteration 116 took 2.80 seconds (mean sampled reward: -4319.95). Current reward after update: -1066.54, Optimal reward -436.66
Iteration 117 took 3.07 seconds (mean sampled reward: -4369.20). Current reward after update: -1063.92, Optimal reward -436.66
Iteration 118 took 3.20 seconds (mean sampled reward: -4352.47). Current reward after update: -1093.88, Optimal reward -436.66
Iteration 119 took 2.79 seconds (mean sampled reward: -4326.34). Current reward after update: -1179.50, Optimal reward -436.66
Iteration 120 took 3.23 seconds (mean sampled reward: -4269.98). Current reward after update: -1049.05, Optimal reward -436.66
Iteration 121 took 2.84 seconds (mean sampled reward: -4272.90). Current reward after update: -1193.57, Optimal reward -436.66
Iteration 122 took 2.94 seconds (mean sampled reward: -4246.85). Current reward after update: -1021.85, Optimal reward -436.66
Iteration 123 took 3.13 seconds (mean sampled reward: -4199.03). Current reward after update: -1082.71, Optimal reward -436.66
Iteration 124 took 2.82 seconds (mean sampled reward: -4312.27). Current reward after update: -1098.78, Optimal reward -436.66
Iteration 125 took 3.06 seconds (mean sampled reward: -4197.76). Current reward after update: -1148.88, Optimal reward -436.66
Iteration 126 took 2.82 seconds (mean sampled reward: -4268.31). Current reward after update: -1100.76, Optimal reward -436.66
Iteration 127 took 2.80 seconds (mean sampled reward: -4318.74). Current reward after update: -1245.93, Optimal reward -436.66
Iteration 128 took 2.95 seconds (mean sampled reward: -4037.65). Current reward after update: -1074.58, Optimal reward -436.66
Iteration 129 took 2.80 seconds (mean sampled reward: -3853.85). Current reward after update: -1048.16, Optimal reward -436.66
Iteration 130 took 2.85 seconds (mean sampled reward: -3755.32). Current reward after update: -1043.36, Optimal reward -436.66
Iteration 131 took 2.96 seconds (mean sampled reward: -3700.78). Current reward after update: -1049.39, Optimal reward -436.66
Iteration 132 took 3.04 seconds (mean sampled reward: -3644.42). Current reward after update: -1192.00, Optimal reward -436.66
Iteration 133 took 2.99 seconds (mean sampled reward: -3576.96). Current reward after update: -1105.19, Optimal reward -436.66
Iteration 134 took 3.06 seconds (mean sampled reward: -3585.07). Current reward after update: -1319.18, Optimal reward -436.66
Iteration 135 took 3.20 seconds (mean sampled reward: -3591.95). Current reward after update: -1469.10, Optimal reward -436.66
Iteration 136 took 2.83 seconds (mean sampled reward: -3628.48). Current reward after update: -1122.26, Optimal reward -436.66
Iteration 137 took 2.96 seconds (mean sampled reward: -3680.88). Current reward after update: -1366.32, Optimal reward -436.66
Iteration 138 took 3.03 seconds (mean sampled reward: -3627.79). Current reward after update: -1297.31, Optimal reward -436.66
Iteration 139 took 3.13 seconds (mean sampled reward: -3620.00). Current reward after update: -1204.05, Optimal reward -436.66
Iteration 140 took 2.89 seconds (mean sampled reward: -3624.60). Current reward after update: -1093.56, Optimal reward -436.66
Iteration 141 took 2.83 seconds (mean sampled reward: -3631.62). Current reward after update: -1035.79, Optimal reward -436.66
Iteration 142 took 3.10 seconds (mean sampled reward: -3632.71). Current reward after update: -994.87, Optimal reward -436.66
Iteration 143 took 2.85 seconds (mean sampled reward: -3670.17). Current reward after update: -1188.46, Optimal reward -436.66
Iteration 144 took 2.79 seconds (mean sampled reward: -3588.36). Current reward after update: -1170.27, Optimal reward -436.66
Iteration 145 took 3.08 seconds (mean sampled reward: -3562.76). Current reward after update: -1361.71, Optimal reward -436.66
Iteration 146 took 3.03 seconds (mean sampled reward: -3624.93). Current reward after update: -1317.27, Optimal reward -436.66
Iteration 147 took 2.81 seconds (mean sampled reward: -3612.68). Current reward after update: -1165.14, Optimal reward -436.66
Iteration 148 took 2.80 seconds (mean sampled reward: -3642.65). Current reward after update: -1305.26, Optimal reward -436.66
Iteration 149 took 3.02 seconds (mean sampled reward: -3736.31). Current reward after update: -1457.76, Optimal reward -436.66
Iteration 150 took 2.88 seconds (mean sampled reward: -3739.20). Current reward after update: -1371.90, Optimal reward -436.66
Iteration 151 took 2.91 seconds (mean sampled reward: -3655.18). Current reward after update: -1573.27, Optimal reward -436.66
Iteration 152 took 2.98 seconds (mean sampled reward: -3754.51). Current reward after update: -1533.12, Optimal reward -436.66
Iteration 153 took 2.91 seconds (mean sampled reward: -3612.96). Current reward after update: -1595.50, Optimal reward -436.66
Iteration 154 took 2.74 seconds (mean sampled reward: -2920.96). Current reward after update: -1624.57, Optimal reward -436.66
Iteration 155 took 2.86 seconds (mean sampled reward: -2906.64). Current reward after update: -1670.32, Optimal reward -436.66
Iteration 156 took 3.02 seconds (mean sampled reward: -2983.76). Current reward after update: -1692.34, Optimal reward -436.66
Iteration 157 took 2.76 seconds (mean sampled reward: -2946.62). Current reward after update: -1670.27, Optimal reward -436.66
Iteration 158 took 2.64 seconds (mean sampled reward: -2888.70). Current reward after update: -1605.55, Optimal reward -436.66
Iteration 159 took 2.78 seconds (mean sampled reward: -2699.37). Current reward after update: -1561.74, Optimal reward -436.66
Iteration 160 took 2.79 seconds (mean sampled reward: -2721.17). Current reward after update: -1494.52, Optimal reward -436.66
Iteration 161 took 2.79 seconds (mean sampled reward: -2747.90). Current reward after update: -1463.03, Optimal reward -436.66
Iteration 162 took 2.87 seconds (mean sampled reward: -2781.44). Current reward after update: -1562.09, Optimal reward -436.66
Iteration 163 took 3.04 seconds (mean sampled reward: -2875.93). Current reward after update: -1588.67, Optimal reward -436.66
Iteration 164 took 2.67 seconds (mean sampled reward: -2686.52). Current reward after update: -1436.40, Optimal reward -436.66
Iteration 165 took 3.03 seconds (mean sampled reward: -2733.99). Current reward after update: -1448.85, Optimal reward -436.66
Iteration 166 took 2.75 seconds (mean sampled reward: -3563.08). Current reward after update: -1703.20, Optimal reward -436.66
Iteration 167 took 2.72 seconds (mean sampled reward: -3222.77). Current reward after update: -1542.42, Optimal reward -436.66
Iteration 168 took 2.97 seconds (mean sampled reward: -3617.34). Current reward after update: -1446.66, Optimal reward -436.66
Iteration 169 took 2.83 seconds (mean sampled reward: -3272.64). Current reward after update: -1539.23, Optimal reward -436.66
Iteration 170 took 2.71 seconds (mean sampled reward: -3278.28). Current reward after update: -1483.63, Optimal reward -436.66
Iteration 171 took 3.08 seconds (mean sampled reward: -3545.52). Current reward after update: -1527.30, Optimal reward -436.66
Iteration 172 took 3.08 seconds (mean sampled reward: -3400.87). Current reward after update: -1739.61, Optimal reward -436.66
Iteration 173 took 2.95 seconds (mean sampled reward: -3383.65). Current reward after update: -1505.60, Optimal reward -436.66
Iteration 174 took 2.84 seconds (mean sampled reward: -2684.52). Current reward after update: -1484.27, Optimal reward -436.66
Iteration 175 took 2.89 seconds (mean sampled reward: -2887.43). Current reward after update: -1583.93, Optimal reward -436.66
Iteration 176 took 2.71 seconds (mean sampled reward: -2758.44). Current reward after update: -1397.84, Optimal reward -436.66
Iteration 177 took 2.94 seconds (mean sampled reward: -2802.39). Current reward after update: -1528.39, Optimal reward -436.66
Iteration 178 took 2.76 seconds (mean sampled reward: -2811.23). Current reward after update: -1500.50, Optimal reward -436.66
Iteration 179 took 2.74 seconds (mean sampled reward: -2837.70). Current reward after update: -1477.27, Optimal reward -436.66
Iteration 180 took 3.09 seconds (mean sampled reward: -2970.98). Current reward after update: -1496.60, Optimal reward -436.66
Iteration 181 took 2.75 seconds (mean sampled reward: -2962.27). Current reward after update: -1614.36, Optimal reward -436.66
Iteration 182 took 2.88 seconds (mean sampled reward: -3166.23). Current reward after update: -1664.38, Optimal reward -436.66
Iteration 183 took 3.01 seconds (mean sampled reward: -2738.35). Current reward after update: -1612.81, Optimal reward -436.66
Iteration 184 took 3.09 seconds (mean sampled reward: -2818.50). Current reward after update: -1536.03, Optimal reward -436.66
Iteration 185 took 2.89 seconds (mean sampled reward: -2934.04). Current reward after update: -1506.08, Optimal reward -436.66
Iteration 186 took 2.98 seconds (mean sampled reward: -3038.98). Current reward after update: -1494.37, Optimal reward -436.66
Iteration 187 took 3.10 seconds (mean sampled reward: -3183.66). Current reward after update: -1376.40, Optimal reward -436.66
Iteration 188 took 2.89 seconds (mean sampled reward: -2969.17). Current reward after update: -1386.97, Optimal reward -436.66
Iteration 189 took 2.73 seconds (mean sampled reward: -2805.03). Current reward after update: -1365.41, Optimal reward -436.66
Iteration 190 took 2.95 seconds (mean sampled reward: -2756.37). Current reward after update: -1286.93, Optimal reward -436.66
Iteration 191 took 2.89 seconds (mean sampled reward: -2954.17). Current reward after update: -1410.81, Optimal reward -436.66
Iteration 192 took 2.75 seconds (mean sampled reward: -2985.57). Current reward after update: -1399.80, Optimal reward -436.66
Iteration 193 took 2.80 seconds (mean sampled reward: -3122.87). Current reward after update: -1412.59, Optimal reward -436.66
Iteration 194 took 2.67 seconds (mean sampled reward: -3174.28). Current reward after update: -1300.57, Optimal reward -436.66
Iteration 195 took 2.68 seconds (mean sampled reward: -2841.33). Current reward after update: -1254.17, Optimal reward -436.66
Iteration 196 took 2.97 seconds (mean sampled reward: -2889.69). Current reward after update: -1409.61, Optimal reward -436.66
Iteration 197 took 2.87 seconds (mean sampled reward: -2752.22). Current reward after update: -1393.10, Optimal reward -436.66
Iteration 198 took 2.75 seconds (mean sampled reward: -2541.08). Current reward after update: -1402.07, Optimal reward -436.66
Iteration 199 took 2.66 seconds (mean sampled reward: -2575.13). Current reward after update: -1333.34, Optimal reward -436.66
Iteration 200 took 2.66 seconds (mean sampled reward: -2681.46). Current reward after update: -1277.40, Optimal reward -436.66
Iteration 201 took 2.77 seconds (mean sampled reward: -2621.48). Current reward after update: -1224.14, Optimal reward -436.66
Iteration 202 took 2.89 seconds (mean sampled reward: -2707.31). Current reward after update: -1318.43, Optimal reward -436.66
Iteration 203 took 3.04 seconds (mean sampled reward: -2505.79). Current reward after update: -1282.64, Optimal reward -436.66
Iteration 204 took 2.73 seconds (mean sampled reward: -2678.54). Current reward after update: -1216.97, Optimal reward -436.66
Iteration 205 took 2.76 seconds (mean sampled reward: -2465.76). Current reward after update: -1315.00, Optimal reward -436.66
Iteration 206 took 2.86 seconds (mean sampled reward: -2463.31). Current reward after update: -1222.11, Optimal reward -436.66
Iteration 207 took 2.63 seconds (mean sampled reward: -2462.70). Current reward after update: -1375.61, Optimal reward -436.66
Iteration 208 took 2.66 seconds (mean sampled reward: -2526.91). Current reward after update: -1375.97, Optimal reward -436.66
Iteration 209 took 2.78 seconds (mean sampled reward: -2540.79). Current reward after update: -1395.43, Optimal reward -436.66
Iteration 210 took 2.81 seconds (mean sampled reward: -2508.35). Current reward after update: -1337.99, Optimal reward -436.66
Iteration 211 took 2.90 seconds (mean sampled reward: -2535.20). Current reward after update: -1340.64, Optimal reward -436.66
Iteration 212 took 2.86 seconds (mean sampled reward: -2716.30). Current reward after update: -1347.30, Optimal reward -436.66
Iteration 213 took 2.96 seconds (mean sampled reward: -2681.39). Current reward after update: -1387.87, Optimal reward -436.66
Iteration 214 took 2.96 seconds (mean sampled reward: -3411.91). Current reward after update: -1395.92, Optimal reward -436.66
Iteration 215 took 2.81 seconds (mean sampled reward: -2904.96). Current reward after update: -1358.55, Optimal reward -436.66
Iteration 216 took 2.84 seconds (mean sampled reward: -3083.83). Current reward after update: -1399.25, Optimal reward -436.66
Iteration 217 took 2.68 seconds (mean sampled reward: -2542.01). Current reward after update: -1275.36, Optimal reward -436.66
Iteration 218 took 2.74 seconds (mean sampled reward: -2524.53). Current reward after update: -1231.64, Optimal reward -436.66
Iteration 219 took 2.78 seconds (mean sampled reward: -2619.29). Current reward after update: -1312.82, Optimal reward -436.66
Iteration 220 took 2.67 seconds (mean sampled reward: -3136.05). Current reward after update: -1253.86, Optimal reward -436.66
Iteration 221 took 2.97 seconds (mean sampled reward: -2955.22). Current reward after update: -1263.27, Optimal reward -436.66
Iteration 222 took 2.82 seconds (mean sampled reward: -3133.92). Current reward after update: -1234.75, Optimal reward -436.66
Iteration 223 took 2.78 seconds (mean sampled reward: -2907.04). Current reward after update: -1357.44, Optimal reward -436.66
Iteration 224 took 2.69 seconds (mean sampled reward: -2932.72). Current reward after update: -1306.50, Optimal reward -436.66
Iteration 225 took 2.64 seconds (mean sampled reward: -2997.40). Current reward after update: -1455.66, Optimal reward -436.66
Iteration 226 took 2.78 seconds (mean sampled reward: -2621.06). Current reward after update: -1415.07, Optimal reward -436.66
Iteration 227 took 2.78 seconds (mean sampled reward: -2520.43). Current reward after update: -1408.93, Optimal reward -436.66
Iteration 228 took 2.92 seconds (mean sampled reward: -2490.38). Current reward after update: -1347.83, Optimal reward -436.66
Iteration 229 took 2.76 seconds (mean sampled reward: -2758.06). Current reward after update: -1376.83, Optimal reward -436.66
Iteration 230 took 2.88 seconds (mean sampled reward: -2812.52). Current reward after update: -1432.17, Optimal reward -436.66
Iteration 231 took 2.85 seconds (mean sampled reward: -2529.45). Current reward after update: -1325.78, Optimal reward -436.66
Iteration 232 took 3.04 seconds (mean sampled reward: -2791.76). Current reward after update: -1369.32, Optimal reward -436.66
Iteration 233 took 2.71 seconds (mean sampled reward: -2761.17). Current reward after update: -1349.91, Optimal reward -436.66
Iteration 234 took 2.77 seconds (mean sampled reward: -2486.37). Current reward after update: -2168.77, Optimal reward -436.66
Iteration 235 took 2.74 seconds (mean sampled reward: -2468.03). Current reward after update: -1389.44, Optimal reward -436.66
Iteration 236 took 2.86 seconds (mean sampled reward: -2464.90). Current reward after update: -1361.85, Optimal reward -436.66
Iteration 237 took 3.00 seconds (mean sampled reward: -2589.63). Current reward after update: -1362.25, Optimal reward -436.66
Iteration 238 took 2.88 seconds (mean sampled reward: -2486.68). Current reward after update: -1353.54, Optimal reward -436.66
Iteration 239 took 2.89 seconds (mean sampled reward: -2602.52). Current reward after update: -1376.05, Optimal reward -436.66
Iteration 240 took 2.79 seconds (mean sampled reward: -2543.38). Current reward after update: -1290.74, Optimal reward -436.66
Iteration 241 took 2.69 seconds (mean sampled reward: -2476.41). Current reward after update: -1341.27, Optimal reward -436.66
Iteration 242 took 2.70 seconds (mean sampled reward: -2461.62). Current reward after update: -1271.99, Optimal reward -436.66
Iteration 243 took 3.01 seconds (mean sampled reward: -2536.94). Current reward after update: -1349.13, Optimal reward -436.66
Iteration 244 took 2.78 seconds (mean sampled reward: -2473.25). Current reward after update: -1345.17, Optimal reward -436.66
Iteration 245 took 2.94 seconds (mean sampled reward: -2554.81). Current reward after update: -1369.89, Optimal reward -436.66
Iteration 246 took 2.84 seconds (mean sampled reward: -2578.29). Current reward after update: -1262.91, Optimal reward -436.66
Iteration 247 took 2.72 seconds (mean sampled reward: -2834.47). Current reward after update: -1437.88, Optimal reward -436.66
Iteration 248 took 2.78 seconds (mean sampled reward: -2702.81). Current reward after update: -1446.11, Optimal reward -436.66
Iteration 249 took 2.70 seconds (mean sampled reward: -2657.32). Current reward after update: -1380.59, Optimal reward -436.66
Iteration 250 took 2.99 seconds (mean sampled reward: -2651.89). Current reward after update: -1402.79, Optimal reward -436.66
Iteration 251 took 2.70 seconds (mean sampled reward: -2631.21). Current reward after update: -1354.99, Optimal reward -436.66
Iteration 252 took 2.82 seconds (mean sampled reward: -2727.65). Current reward after update: -1429.58, Optimal reward -436.66
Iteration 253 took 2.84 seconds (mean sampled reward: -3116.71). Current reward after update: -1349.43, Optimal reward -436.66
Iteration 254 took 2.91 seconds (mean sampled reward: -3973.75). Current reward after update: -1496.91, Optimal reward -436.66
Iteration 255 took 2.79 seconds (mean sampled reward: -4148.66). Current reward after update: -1142.61, Optimal reward -436.66
Iteration 256 took 3.23 seconds (mean sampled reward: -4137.09). Current reward after update: -1116.48, Optimal reward -436.66
Iteration 257 took 2.85 seconds (mean sampled reward: -4113.03). Current reward after update: -1059.72, Optimal reward -436.66
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
Iteration 258 took 3.14 seconds (mean sampled reward: -4114.93). Current reward after update: -1428.71, Optimal reward -436.66
Iteration 259 took 3.04 seconds (mean sampled reward: -4195.41). Current reward after update: -1261.59, Optimal reward -436.66
Iteration 260 took 2.78 seconds (mean sampled reward: -4147.81). Current reward after update: -1199.00, Optimal reward -436.66
Iteration 261 took 2.86 seconds (mean sampled reward: -4144.79). Current reward after update: -1483.60, Optimal reward -436.66
Iteration 262 took 2.77 seconds (mean sampled reward: -4141.67). Current reward after update: -1307.64, Optimal reward -436.66
Iteration 263 took 2.83 seconds (mean sampled reward: -4136.53). Current reward after update: -1055.46, Optimal reward -436.66
Iteration 264 took 2.77 seconds (mean sampled reward: -4106.82). Current reward after update: -1366.29, Optimal reward -436.66
Iteration 265 took 2.75 seconds (mean sampled reward: -4120.16). Current reward after update: -1219.37, Optimal reward -436.66
Iteration 266 took 2.98 seconds (mean sampled reward: -4138.71). Current reward after update: -1127.04, Optimal reward -436.66
Iteration 267 took 3.13 seconds (mean sampled reward: -4155.50). Current reward after update: -1223.26, Optimal reward -436.66
Iteration 268 took 3.00 seconds (mean sampled reward: -4005.60). Current reward after update: -1374.09, Optimal reward -436.66
Iteration 269 took 2.77 seconds (mean sampled reward: -4117.23). Current reward after update: -1380.22, Optimal reward -436.66
Iteration 270 took 3.04 seconds (mean sampled reward: -4109.51). Current reward after update: -1254.47, Optimal reward -436.66
Iteration 271 took 2.92 seconds (mean sampled reward: -4118.00). Current reward after update: -1309.61, Optimal reward -436.66
Iteration 272 took 3.15 seconds (mean sampled reward: -4159.27). Current reward after update: -1248.76, Optimal reward -436.66
Iteration 273 took 2.91 seconds (mean sampled reward: -4121.39). Current reward after update: -1345.91, Optimal reward -436.66
Iteration 274 took 2.76 seconds (mean sampled reward: -4104.68). Current reward after update: -1095.63, Optimal reward -436.66
Iteration 275 took 2.85 seconds (mean sampled reward: -4135.66). Current reward after update: -1282.19, Optimal reward -436.66
Iteration 276 took 2.98 seconds (mean sampled reward: -4172.51). Current reward after update: -1362.35, Optimal reward -436.66
Iteration 277 took 2.91 seconds (mean sampled reward: -4063.27). Current reward after update: -1319.68, Optimal reward -436.66
Iteration 278 took 2.91 seconds (mean sampled reward: -4156.71). Current reward after update: -1256.31, Optimal reward -436.66
Iteration 279 took 3.11 seconds (mean sampled reward: -4180.49). Current reward after update: -1302.21, Optimal reward -436.66
Iteration 280 took 2.96 seconds (mean sampled reward: -4198.72). Current reward after update: -1310.09, Optimal reward -436.66
Iteration 281 took 2.88 seconds (mean sampled reward: -4126.32). Current reward after update: -1286.33, Optimal reward -436.66
Iteration 282 took 2.91 seconds (mean sampled reward: -4159.46). Current reward after update: -1311.90, Optimal reward -436.66
Iteration 283 took 3.11 seconds (mean sampled reward: -4140.03). Current reward after update: -1332.07, Optimal reward -436.66
Iteration 284 took 2.78 seconds (mean sampled reward: -4082.35). Current reward after update: -1233.09, Optimal reward -436.66
Iteration 285 took 2.75 seconds (mean sampled reward: -4166.91). Current reward after update: -1388.55, Optimal reward -436.66
Iteration 286 took 2.96 seconds (mean sampled reward: -4135.01). Current reward after update: -1181.03, Optimal reward -436.66
Iteration 287 took 3.06 seconds (mean sampled reward: -4166.12). Current reward after update: -1348.36, Optimal reward -436.66
Iteration 288 took 2.87 seconds (mean sampled reward: -4221.75). Current reward after update: -1403.71, Optimal reward -436.66
Iteration 289 took 3.10 seconds (mean sampled reward: -4165.84). Current reward after update: -1267.15, Optimal reward -436.66
Iteration 290 took 2.96 seconds (mean sampled reward: -4108.60). Current reward after update: -1264.24, Optimal reward -436.66
Iteration 291 took 3.06 seconds (mean sampled reward: -4167.37). Current reward after update: -1291.18, Optimal reward -436.66
Iteration 292 took 3.22 seconds (mean sampled reward: -4133.11). Current reward after update: -1074.19, Optimal reward -436.66
Iteration 293 took 2.95 seconds (mean sampled reward: -4149.22). Current reward after update: -1332.13, Optimal reward -436.66
Iteration 294 took 2.79 seconds (mean sampled reward: -4089.53). Current reward after update: -1187.89, Optimal reward -436.66
Iteration 295 took 2.89 seconds (mean sampled reward: -4159.59). Current reward after update: -1128.46, Optimal reward -436.66
Iteration 296 took 2.85 seconds (mean sampled reward: -4092.18). Current reward after update: -1089.31, Optimal reward -436.66
Iteration 297 took 2.80 seconds (mean sampled reward: -4091.76). Current reward after update: -1142.24, Optimal reward -436.66
Iteration 298 took 2.92 seconds (mean sampled reward: -4124.35). Current reward after update: -1172.95, Optimal reward -436.66
Iteration 299 took 2.90 seconds (mean sampled reward: -4178.05). Current reward after update: -1211.77, Optimal reward -436.66
Iteration 300 took 2.78 seconds (mean sampled reward: -4085.93). Current reward after update: -1164.90, Optimal reward -436.66
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
Iteration 1 took 2.94 seconds (mean sampled reward: -4432.32). Current reward after update: -2014.80, Optimal reward -2014.80
Iteration 2 took 2.78 seconds (mean sampled reward: -4395.04). Current reward after update: -1552.81, Optimal reward -1552.81
Iteration 3 took 2.90 seconds (mean sampled reward: -4346.26). Current reward after update: -1719.45, Optimal reward -1552.81
Iteration 4 took 2.74 seconds (mean sampled reward: -4334.29). Current reward after update: -1539.86, Optimal reward -1539.86
Iteration 5 took 2.70 seconds (mean sampled reward: -4300.76). Current reward after update: -1331.98, Optimal reward -1331.98
Iteration 6 took 2.89 seconds (mean sampled reward: -4276.11). Current reward after update: -1435.94, Optimal reward -1331.98
Iteration 7 took 2.97 seconds (mean sampled reward: -4344.64). Current reward after update: -1518.65, Optimal reward -1331.98
Iteration 8 took 3.14 seconds (mean sampled reward: -4313.27). Current reward after update: -1655.59, Optimal reward -1331.98
Iteration 9 took 2.97 seconds (mean sampled reward: -4348.65). Current reward after update: -1667.27, Optimal reward -1331.98
Iteration 10 took 3.03 seconds (mean sampled reward: -4399.95). Current reward after update: -1344.47, Optimal reward -1331.98
Iteration 11 took 2.88 seconds (mean sampled reward: -4332.50). Current reward after update: -1306.33, Optimal reward -1306.33
Iteration 12 took 2.89 seconds (mean sampled reward: -4285.04). Current reward after update: -1300.63, Optimal reward -1300.63
Iteration 13 took 3.03 seconds (mean sampled reward: -4265.20). Current reward after update: -1380.43, Optimal reward -1300.63
Iteration 14 took 2.90 seconds (mean sampled reward: -4357.57). Current reward after update: -1431.41, Optimal reward -1300.63
Iteration 15 took 3.00 seconds (mean sampled reward: -4324.41). Current reward after update: -1351.40, Optimal reward -1300.63
Iteration 16 took 3.04 seconds (mean sampled reward: -4280.87). Current reward after update: -1236.44, Optimal reward -1236.44
Iteration 17 took 2.78 seconds (mean sampled reward: -4171.71). Current reward after update: -1380.12, Optimal reward -1236.44
Iteration 18 took 2.90 seconds (mean sampled reward: -4176.25). Current reward after update: -1503.32, Optimal reward -1236.44
Iteration 19 took 2.87 seconds (mean sampled reward: -4061.34). Current reward after update: -1322.75, Optimal reward -1236.44
Iteration 20 took 2.92 seconds (mean sampled reward: -4050.97). Current reward after update: -1289.56, Optimal reward -1236.44
Iteration 21 took 2.97 seconds (mean sampled reward: -4144.90). Current reward after update: -1208.76, Optimal reward -1208.76
Iteration 22 took 2.74 seconds (mean sampled reward: -4085.68). Current reward after update: -1475.88, Optimal reward -1208.76
Iteration 23 took 3.03 seconds (mean sampled reward: -4094.42). Current reward after update: -1240.37, Optimal reward -1208.76
Iteration 24 took 2.84 seconds (mean sampled reward: -4110.52). Current reward after update: -1200.83, Optimal reward -1200.83
Iteration 25 took 2.85 seconds (mean sampled reward: -4140.49). Current reward after update: -1259.55, Optimal reward -1200.83
Iteration 26 took 2.71 seconds (mean sampled reward: -4108.95). Current reward after update: -1303.28, Optimal reward -1200.83
Iteration 27 took 3.12 seconds (mean sampled reward: -4099.37). Current reward after update: -1300.14, Optimal reward -1200.83
Iteration 28 took 3.07 seconds (mean sampled reward: -4077.71). Current reward after update: -1331.66, Optimal reward -1200.83
Iteration 29 took 3.05 seconds (mean sampled reward: -4096.79). Current reward after update: -1285.55, Optimal reward -1200.83
Iteration 30 took 2.87 seconds (mean sampled reward: -4074.45). Current reward after update: -1309.61, Optimal reward -1200.83
Iteration 31 took 2.86 seconds (mean sampled reward: -4071.72). Current reward after update: -1159.46, Optimal reward -1159.46
Iteration 32 took 2.96 seconds (mean sampled reward: -4079.33). Current reward after update: -1177.11, Optimal reward -1159.46
Iteration 33 took 2.77 seconds (mean sampled reward: -4146.16). Current reward after update: -1235.39, Optimal reward -1159.46
Iteration 34 took 2.83 seconds (mean sampled reward: -4102.85). Current reward after update: -1393.19, Optimal reward -1159.46
Iteration 35 took 2.83 seconds (mean sampled reward: -4132.05). Current reward after update: -1259.60, Optimal reward -1159.46
Iteration 36 took 2.98 seconds (mean sampled reward: -4075.71). Current reward after update: -1263.64, Optimal reward -1159.46
Iteration 37 took 2.82 seconds (mean sampled reward: -4137.41). Current reward after update: -1184.23, Optimal reward -1159.46
Iteration 38 took 2.80 seconds (mean sampled reward: -4114.60). Current reward after update: -1203.31, Optimal reward -1159.46
Iteration 39 took 2.87 seconds (mean sampled reward: -4084.54). Current reward after update: -1315.57, Optimal reward -1159.46
Iteration 40 took 2.81 seconds (mean sampled reward: -4124.38). Current reward after update: -1252.10, Optimal reward -1159.46
Iteration 41 took 3.04 seconds (mean sampled reward: -4165.79). Current reward after update: -1189.29, Optimal reward -1159.46
Iteration 42 took 2.93 seconds (mean sampled reward: -4215.77). Current reward after update: -1280.41, Optimal reward -1159.46
Iteration 43 took 2.96 seconds (mean sampled reward: -4207.29). Current reward after update: -1184.75, Optimal reward -1159.46
Iteration 44 took 2.79 seconds (mean sampled reward: -4165.29). Current reward after update: -1240.45, Optimal reward -1159.46
Iteration 45 took 2.74 seconds (mean sampled reward: -4173.37). Current reward after update: -1302.42, Optimal reward -1159.46
Iteration 46 took 2.77 seconds (mean sampled reward: -4191.48). Current reward after update: -1303.21, Optimal reward -1159.46
Iteration 47 took 2.95 seconds (mean sampled reward: -4206.50). Current reward after update: -1226.06, Optimal reward -1159.46
Iteration 48 took 2.80 seconds (mean sampled reward: -4206.23). Current reward after update: -1294.60, Optimal reward -1159.46
Iteration 49 took 2.76 seconds (mean sampled reward: -4154.05). Current reward after update: -1335.58, Optimal reward -1159.46
Iteration 50 took 2.83 seconds (mean sampled reward: -4147.30). Current reward after update: -1485.05, Optimal reward -1159.46
Iteration 51 took 2.76 seconds (mean sampled reward: -4196.23). Current reward after update: -1409.17, Optimal reward -1159.46
Iteration 52 took 2.69 seconds (mean sampled reward: -4149.71). Current reward after update: -1285.85, Optimal reward -1159.46
Iteration 53 took 2.83 seconds (mean sampled reward: -4150.26). Current reward after update: -1419.63, Optimal reward -1159.46
Iteration 54 took 2.91 seconds (mean sampled reward: -4259.82). Current reward after update: -1657.34, Optimal reward -1159.46
Iteration 55 took 2.71 seconds (mean sampled reward: -4134.65). Current reward after update: -1369.57, Optimal reward -1159.46
Iteration 56 took 2.94 seconds (mean sampled reward: -4087.23). Current reward after update: -1101.98, Optimal reward -1101.98
Iteration 57 took 2.81 seconds (mean sampled reward: -4040.14). Current reward after update: -1318.11, Optimal reward -1101.98
Iteration 58 took 2.62 seconds (mean sampled reward: -4104.85). Current reward after update: -1331.38, Optimal reward -1101.98
Iteration 59 took 2.76 seconds (mean sampled reward: -4101.75). Current reward after update: -1261.89, Optimal reward -1101.98
Iteration 60 took 2.76 seconds (mean sampled reward: -3999.76). Current reward after update: -1250.23, Optimal reward -1101.98
Iteration 61 took 2.85 seconds (mean sampled reward: -4013.96). Current reward after update: -1244.88, Optimal reward -1101.98
Iteration 62 took 2.73 seconds (mean sampled reward: -4041.00). Current reward after update: -1191.17, Optimal reward -1101.98
Iteration 63 took 2.75 seconds (mean sampled reward: -3924.28). Current reward after update: -1073.47, Optimal reward -1073.47
Iteration 64 took 2.90 seconds (mean sampled reward: -3982.00). Current reward after update: -1176.11, Optimal reward -1073.47
Iteration 65 took 2.86 seconds (mean sampled reward: -4225.29). Current reward after update: -1240.19, Optimal reward -1073.47
Iteration 66 took 2.86 seconds (mean sampled reward: -4193.03). Current reward after update: -1293.87, Optimal reward -1073.47
Iteration 67 took 2.69 seconds (mean sampled reward: -4126.47). Current reward after update: -1190.83, Optimal reward -1073.47
Iteration 68 took 2.93 seconds (mean sampled reward: -4133.46). Current reward after update: -1040.20, Optimal reward -1040.20
Iteration 69 took 2.78 seconds (mean sampled reward: -3838.76). Current reward after update: -1035.73, Optimal reward -1035.73
Iteration 70 took 2.80 seconds (mean sampled reward: -3752.68). Current reward after update: -1009.77, Optimal reward -1009.77
Iteration 71 took 2.72 seconds (mean sampled reward: -3773.61). Current reward after update: -1089.45, Optimal reward -1009.77
Iteration 72 took 2.61 seconds (mean sampled reward: -3804.85). Current reward after update: -1058.60, Optimal reward -1009.77
Iteration 73 took 2.96 seconds (mean sampled reward: -3755.59). Current reward after update: -1085.70, Optimal reward -1009.77
Iteration 74 took 2.73 seconds (mean sampled reward: -3853.87). Current reward after update: -1102.84, Optimal reward -1009.77
Iteration 75 took 2.97 seconds (mean sampled reward: -3932.41). Current reward after update: -1178.03, Optimal reward -1009.77
Iteration 76 took 2.66 seconds (mean sampled reward: -3932.66). Current reward after update: -1169.94, Optimal reward -1009.77
Iteration 77 took 3.01 seconds (mean sampled reward: -4005.32). Current reward after update: -1284.90, Optimal reward -1009.77
Iteration 78 took 2.96 seconds (mean sampled reward: -3964.26). Current reward after update: -1188.35, Optimal reward -1009.77
Iteration 79 took 2.74 seconds (mean sampled reward: -4031.70). Current reward after update: -1126.85, Optimal reward -1009.77
Iteration 80 took 2.73 seconds (mean sampled reward: -4069.92). Current reward after update: -1416.74, Optimal reward -1009.77
Iteration 81 took 2.92 seconds (mean sampled reward: -3977.04). Current reward after update: -1216.07, Optimal reward -1009.77
Iteration 82 took 2.93 seconds (mean sampled reward: -4111.05). Current reward after update: -1188.77, Optimal reward -1009.77
Iteration 83 took 2.74 seconds (mean sampled reward: -4122.00). Current reward after update: -1297.43, Optimal reward -1009.77
Iteration 84 took 2.76 seconds (mean sampled reward: -4014.46). Current reward after update: -1179.25, Optimal reward -1009.77
Iteration 85 took 3.06 seconds (mean sampled reward: -4084.66). Current reward after update: -1181.36, Optimal reward -1009.77
Iteration 86 took 2.78 seconds (mean sampled reward: -4118.22). Current reward after update: -1195.06, Optimal reward -1009.77
Iteration 87 took 2.71 seconds (mean sampled reward: -4013.85). Current reward after update: -1099.67, Optimal reward -1009.77
Iteration 88 took 3.23 seconds (mean sampled reward: -4107.22). Current reward after update: -1410.77, Optimal reward -1009.77
Iteration 89 took 2.78 seconds (mean sampled reward: -4210.89). Current reward after update: -1181.17, Optimal reward -1009.77
Iteration 90 took 2.71 seconds (mean sampled reward: -4039.13). Current reward after update: -1293.01, Optimal reward -1009.77
Iteration 91 took 2.83 seconds (mean sampled reward: -4147.71). Current reward after update: -1196.59, Optimal reward -1009.77
Iteration 92 took 2.76 seconds (mean sampled reward: -4129.55). Current reward after update: -1255.23, Optimal reward -1009.77
Iteration 93 took 2.80 seconds (mean sampled reward: -4166.58). Current reward after update: -1253.17, Optimal reward -1009.77
Iteration 94 took 2.86 seconds (mean sampled reward: -4181.92). Current reward after update: -1211.43, Optimal reward -1009.77
Iteration 95 took 2.85 seconds (mean sampled reward: -4205.25). Current reward after update: -1451.88, Optimal reward -1009.77
Iteration 96 took 2.82 seconds (mean sampled reward: -4265.43). Current reward after update: -1275.42, Optimal reward -1009.77
Iteration 97 took 2.62 seconds (mean sampled reward: -4203.54). Current reward after update: -1321.35, Optimal reward -1009.77
Iteration 98 took 2.72 seconds (mean sampled reward: -4190.09). Current reward after update: -1169.27, Optimal reward -1009.77
Iteration 99 took 2.85 seconds (mean sampled reward: -4165.63). Current reward after update: -1191.38, Optimal reward -1009.77
Iteration 100 took 2.74 seconds (mean sampled reward: -4038.68). Current reward after update: -1156.97, Optimal reward -1009.77
Iteration 101 took 2.72 seconds (mean sampled reward: -4202.71). Current reward after update: -1311.55, Optimal reward -1009.77
Iteration 102 took 2.86 seconds (mean sampled reward: -4131.96). Current reward after update: -1150.06, Optimal reward -1009.77
Iteration 103 took 2.71 seconds (mean sampled reward: -4087.34). Current reward after update: -1296.88, Optimal reward -1009.77
Iteration 104 took 2.80 seconds (mean sampled reward: -4101.75). Current reward after update: -1180.96, Optimal reward -1009.77
Iteration 105 took 2.66 seconds (mean sampled reward: -4069.24). Current reward after update: -1152.59, Optimal reward -1009.77
Iteration 106 took 2.74 seconds (mean sampled reward: -4169.10). Current reward after update: -1319.58, Optimal reward -1009.77
Iteration 107 took 2.60 seconds (mean sampled reward: -4188.48). Current reward after update: -1264.36, Optimal reward -1009.77
Iteration 108 took 2.79 seconds (mean sampled reward: -4182.54). Current reward after update: -1282.22, Optimal reward -1009.77
Iteration 109 took 2.76 seconds (mean sampled reward: -4221.14). Current reward after update: -1300.77, Optimal reward -1009.77
Iteration 110 took 2.83 seconds (mean sampled reward: -4224.81). Current reward after update: -1269.29, Optimal reward -1009.77
Iteration 111 took 2.84 seconds (mean sampled reward: -4208.36). Current reward after update: -1291.55, Optimal reward -1009.77
Iteration 112 took 2.70 seconds (mean sampled reward: -4274.09). Current reward after update: -1292.92, Optimal reward -1009.77
Iteration 113 took 2.93 seconds (mean sampled reward: -4220.10). Current reward after update: -1183.94, Optimal reward -1009.77
Iteration 114 took 2.91 seconds (mean sampled reward: -4188.73). Current reward after update: -1333.04, Optimal reward -1009.77
Iteration 115 took 2.70 seconds (mean sampled reward: -4037.27). Current reward after update: -1297.02, Optimal reward -1009.77
Iteration 116 took 2.92 seconds (mean sampled reward: -3989.40). Current reward after update: -1261.02, Optimal reward -1009.77
Iteration 117 took 2.70 seconds (mean sampled reward: -4054.16). Current reward after update: -1262.50, Optimal reward -1009.77
Iteration 118 took 2.59 seconds (mean sampled reward: -3921.76). Current reward after update: -1154.12, Optimal reward -1009.77
Iteration 119 took 2.83 seconds (mean sampled reward: -3998.50). Current reward after update: -1111.37, Optimal reward -1009.77
Iteration 120 took 2.81 seconds (mean sampled reward: -4015.94). Current reward after update: -1410.00, Optimal reward -1009.77
Iteration 121 took 2.68 seconds (mean sampled reward: -4221.68). Current reward after update: -1282.28, Optimal reward -1009.77
Iteration 122 took 2.70 seconds (mean sampled reward: -4138.83). Current reward after update: -1257.34, Optimal reward -1009.77
Iteration 123 took 2.78 seconds (mean sampled reward: -4180.52). Current reward after update: -1361.64, Optimal reward -1009.77
Iteration 124 took 2.59 seconds (mean sampled reward: -4161.53). Current reward after update: -1319.73, Optimal reward -1009.77
Iteration 125 took 2.69 seconds (mean sampled reward: -4229.42). Current reward after update: -1406.10, Optimal reward -1009.77
Iteration 126 took 2.73 seconds (mean sampled reward: -4280.85). Current reward after update: -1257.27, Optimal reward -1009.77
Iteration 127 took 2.71 seconds (mean sampled reward: -4153.22). Current reward after update: -1283.49, Optimal reward -1009.77
Iteration 128 took 2.79 seconds (mean sampled reward: -4176.24). Current reward after update: -1343.88, Optimal reward -1009.77
Iteration 129 took 2.56 seconds (mean sampled reward: -4147.49). Current reward after update: -1301.37, Optimal reward -1009.77
Iteration 130 took 2.71 seconds (mean sampled reward: -4217.97). Current reward after update: -1298.17, Optimal reward -1009.77
Iteration 131 took 2.72 seconds (mean sampled reward: -4118.50). Current reward after update: -1296.74, Optimal reward -1009.77
Iteration 132 took 2.73 seconds (mean sampled reward: -4208.33). Current reward after update: -1661.58, Optimal reward -1009.77
Iteration 133 took 2.60 seconds (mean sampled reward: -4130.57). Current reward after update: -1319.74, Optimal reward -1009.77
Iteration 134 took 2.86 seconds (mean sampled reward: -4111.25). Current reward after update: -1296.58, Optimal reward -1009.77
Iteration 135 took 2.78 seconds (mean sampled reward: -4050.17). Current reward after update: -1260.94, Optimal reward -1009.77
Iteration 136 took 2.72 seconds (mean sampled reward: -4102.99). Current reward after update: -1279.83, Optimal reward -1009.77
Iteration 137 took 2.68 seconds (mean sampled reward: -4143.99). Current reward after update: -1464.28, Optimal reward -1009.77
Iteration 138 took 2.71 seconds (mean sampled reward: -4198.47). Current reward after update: -1197.46, Optimal reward -1009.77
Iteration 139 took 3.01 seconds (mean sampled reward: -4208.63). Current reward after update: -1238.44, Optimal reward -1009.77
Iteration 140 took 2.83 seconds (mean sampled reward: -4145.35). Current reward after update: -1354.15, Optimal reward -1009.77
Iteration 141 took 2.82 seconds (mean sampled reward: -4226.63). Current reward after update: -1294.27, Optimal reward -1009.77
Iteration 142 took 2.63 seconds (mean sampled reward: -4258.61). Current reward after update: -1320.26, Optimal reward -1009.77
Iteration 143 took 2.91 seconds (mean sampled reward: -4174.10). Current reward after update: -1201.98, Optimal reward -1009.77
Iteration 144 took 2.96 seconds (mean sampled reward: -3956.69). Current reward after update: -1238.00, Optimal reward -1009.77
Iteration 145 took 2.69 seconds (mean sampled reward: -4013.71). Current reward after update: -1313.14, Optimal reward -1009.77
Iteration 146 took 3.10 seconds (mean sampled reward: -4019.41). Current reward after update: -1279.22, Optimal reward -1009.77
Iteration 147 took 3.05 seconds (mean sampled reward: -4040.13). Current reward after update: -1293.98, Optimal reward -1009.77
Iteration 148 took 3.04 seconds (mean sampled reward: -4024.72). Current reward after update: -1295.53, Optimal reward -1009.77
Iteration 149 took 2.78 seconds (mean sampled reward: -3979.91). Current reward after update: -1259.96, Optimal reward -1009.77
Iteration 150 took 3.00 seconds (mean sampled reward: -3936.15). Current reward after update: -1241.36, Optimal reward -1009.77
Iteration 151 took 3.07 seconds (mean sampled reward: -4027.12). Current reward after update: -1377.09, Optimal reward -1009.77
Iteration 152 took 2.79 seconds (mean sampled reward: -4060.61). Current reward after update: -1251.48, Optimal reward -1009.77
Iteration 153 took 2.77 seconds (mean sampled reward: -4156.79). Current reward after update: -1319.65, Optimal reward -1009.77
Iteration 154 took 2.99 seconds (mean sampled reward: -4129.47). Current reward after update: -1209.78, Optimal reward -1009.77
Iteration 155 took 2.95 seconds (mean sampled reward: -4006.95). Current reward after update: -1246.31, Optimal reward -1009.77
Iteration 156 took 2.81 seconds (mean sampled reward: -4024.13). Current reward after update: -1228.35, Optimal reward -1009.77
Iteration 157 took 2.88 seconds (mean sampled reward: -3901.37). Current reward after update: -1217.91, Optimal reward -1009.77
Iteration 158 took 2.71 seconds (mean sampled reward: -4052.83). Current reward after update: -1305.73, Optimal reward -1009.77
Iteration 159 took 2.89 seconds (mean sampled reward: -4032.25). Current reward after update: -1280.09, Optimal reward -1009.77
Iteration 160 took 2.75 seconds (mean sampled reward: -3911.20). Current reward after update: -1270.82, Optimal reward -1009.77
Iteration 161 took 2.96 seconds (mean sampled reward: -3860.27). Current reward after update: -1244.21, Optimal reward -1009.77
Iteration 162 took 2.87 seconds (mean sampled reward: -3811.41). Current reward after update: -1441.55, Optimal reward -1009.77
Iteration 163 took 2.68 seconds (mean sampled reward: -3787.34). Current reward after update: -1220.09, Optimal reward -1009.77
Iteration 164 took 2.83 seconds (mean sampled reward: -3785.81). Current reward after update: -1221.50, Optimal reward -1009.77
Iteration 165 took 2.98 seconds (mean sampled reward: -3930.06). Current reward after update: -1194.79, Optimal reward -1009.77
Iteration 166 took 2.84 seconds (mean sampled reward: -3878.92). Current reward after update: -1182.24, Optimal reward -1009.77
Iteration 167 took 2.81 seconds (mean sampled reward: -3894.90). Current reward after update: -1277.09, Optimal reward -1009.77
Iteration 168 took 2.72 seconds (mean sampled reward: -3875.53). Current reward after update: -1182.11, Optimal reward -1009.77
Iteration 169 took 2.67 seconds (mean sampled reward: -3876.74). Current reward after update: -1201.39, Optimal reward -1009.77
Iteration 170 took 2.97 seconds (mean sampled reward: -4041.33). Current reward after update: -1284.38, Optimal reward -1009.77
Iteration 171 took 2.82 seconds (mean sampled reward: -4050.81). Current reward after update: -1192.05, Optimal reward -1009.77
Iteration 172 took 2.84 seconds (mean sampled reward: -4088.14). Current reward after update: -1279.04, Optimal reward -1009.77
Iteration 173 took 2.81 seconds (mean sampled reward: -4071.31). Current reward after update: -1190.97, Optimal reward -1009.77
Iteration 174 took 2.65 seconds (mean sampled reward: -4009.94). Current reward after update: -1221.46, Optimal reward -1009.77
Iteration 175 took 2.99 seconds (mean sampled reward: -4059.75). Current reward after update: -1202.15, Optimal reward -1009.77
Iteration 176 took 2.63 seconds (mean sampled reward: -4049.29). Current reward after update: -1211.09, Optimal reward -1009.77
Iteration 177 took 2.68 seconds (mean sampled reward: -4079.51). Current reward after update: -1183.65, Optimal reward -1009.77
Iteration 178 took 2.83 seconds (mean sampled reward: -4040.93). Current reward after update: -1191.92, Optimal reward -1009.77
Iteration 179 took 2.98 seconds (mean sampled reward: -4037.80). Current reward after update: -1184.62, Optimal reward -1009.77
Iteration 180 took 2.71 seconds (mean sampled reward: -4044.90). Current reward after update: -1197.84, Optimal reward -1009.77
Iteration 181 took 3.15 seconds (mean sampled reward: -4034.86). Current reward after update: -1180.37, Optimal reward -1009.77
Iteration 182 took 2.87 seconds (mean sampled reward: -4027.23). Current reward after update: -1186.36, Optimal reward -1009.77
Iteration 183 took 2.78 seconds (mean sampled reward: -4060.46). Current reward after update: -1179.94, Optimal reward -1009.77
Iteration 184 took 2.86 seconds (mean sampled reward: -4079.39). Current reward after update: -1195.19, Optimal reward -1009.77
Iteration 185 took 2.80 seconds (mean sampled reward: -4146.80). Current reward after update: -1189.71, Optimal reward -1009.77
Iteration 186 took 2.95 seconds (mean sampled reward: -4082.95). Current reward after update: -1220.24, Optimal reward -1009.77
Iteration 187 took 2.72 seconds (mean sampled reward: -4175.88). Current reward after update: -1234.87, Optimal reward -1009.77
Iteration 188 took 2.91 seconds (mean sampled reward: -4107.19). Current reward after update: -1184.66, Optimal reward -1009.77
Iteration 189 took 2.87 seconds (mean sampled reward: -4041.46). Current reward after update: -1194.55, Optimal reward -1009.77
Iteration 190 took 2.78 seconds (mean sampled reward: -4116.45). Current reward after update: -1218.85, Optimal reward -1009.77
Iteration 191 took 2.72 seconds (mean sampled reward: -4003.59). Current reward after update: -1241.77, Optimal reward -1009.77
Iteration 192 took 2.83 seconds (mean sampled reward: -4003.90). Current reward after update: -1184.60, Optimal reward -1009.77
Iteration 193 took 2.79 seconds (mean sampled reward: -4048.88). Current reward after update: -1163.22, Optimal reward -1009.77
Iteration 194 took 2.95 seconds (mean sampled reward: -4084.38). Current reward after update: -1197.39, Optimal reward -1009.77
Iteration 195 took 2.81 seconds (mean sampled reward: -4114.23). Current reward after update: -1198.23, Optimal reward -1009.77
Iteration 196 took 2.67 seconds (mean sampled reward: -4137.20). Current reward after update: -1196.80, Optimal reward -1009.77
Iteration 197 took 2.74 seconds (mean sampled reward: -4223.16). Current reward after update: -1188.05, Optimal reward -1009.77
Iteration 198 took 2.81 seconds (mean sampled reward: -4072.11). Current reward after update: -1219.34, Optimal reward -1009.77
Iteration 199 took 2.88 seconds (mean sampled reward: -4118.88). Current reward after update: -1179.01, Optimal reward -1009.77
Iteration 200 took 2.93 seconds (mean sampled reward: -4079.51). Current reward after update: -1211.24, Optimal reward -1009.77
Iteration 201 took 2.80 seconds (mean sampled reward: -4040.71). Current reward after update: -1226.26, Optimal reward -1009.77
Iteration 202 took 2.81 seconds (mean sampled reward: -4007.05). Current reward after update: -1209.22, Optimal reward -1009.77
Iteration 203 took 2.87 seconds (mean sampled reward: -3923.63). Current reward after update: -1205.08, Optimal reward -1009.77
Iteration 204 took 2.73 seconds (mean sampled reward: -3974.16). Current reward after update: -1181.70, Optimal reward -1009.77
Iteration 205 took 2.66 seconds (mean sampled reward: -3923.03). Current reward after update: -1197.10, Optimal reward -1009.77
Iteration 206 took 2.78 seconds (mean sampled reward: -3936.14). Current reward after update: -1194.79, Optimal reward -1009.77
Iteration 207 took 2.94 seconds (mean sampled reward: -3903.59). Current reward after update: -1204.65, Optimal reward -1009.77
Iteration 208 took 3.02 seconds (mean sampled reward: -3906.44). Current reward after update: -1199.89, Optimal reward -1009.77
Iteration 209 took 2.68 seconds (mean sampled reward: -3967.43). Current reward after update: -1280.78, Optimal reward -1009.77
Iteration 210 took 2.78 seconds (mean sampled reward: -3975.31). Current reward after update: -1189.30, Optimal reward -1009.77
Iteration 211 took 2.65 seconds (mean sampled reward: -3923.50). Current reward after update: -1182.05, Optimal reward -1009.77
Iteration 212 took 2.78 seconds (mean sampled reward: -3985.96). Current reward after update: -1191.20, Optimal reward -1009.77
Iteration 213 took 2.74 seconds (mean sampled reward: -4050.20). Current reward after update: -1174.80, Optimal reward -1009.77
Iteration 214 took 2.65 seconds (mean sampled reward: -4002.49). Current reward after update: -1158.08, Optimal reward -1009.77
Iteration 215 took 2.84 seconds (mean sampled reward: -4019.17). Current reward after update: -1186.79, Optimal reward -1009.77
Iteration 216 took 2.63 seconds (mean sampled reward: -4043.89). Current reward after update: -1207.50, Optimal reward -1009.77
Iteration 217 took 2.81 seconds (mean sampled reward: -4082.10). Current reward after update: -1209.52, Optimal reward -1009.77
Iteration 218 took 3.03 seconds (mean sampled reward: -4122.02). Current reward after update: -1293.59, Optimal reward -1009.77
Iteration 219 took 2.90 seconds (mean sampled reward: -4202.29). Current reward after update: -1864.85, Optimal reward -1009.77
Iteration 220 took 2.88 seconds (mean sampled reward: -4197.58). Current reward after update: -1201.93, Optimal reward -1009.77
Iteration 221 took 3.09 seconds (mean sampled reward: -4085.17). Current reward after update: -1202.50, Optimal reward -1009.77
Iteration 222 took 2.71 seconds (mean sampled reward: -4048.97). Current reward after update: -1179.61, Optimal reward -1009.77
Iteration 223 took 2.82 seconds (mean sampled reward: -4089.94). Current reward after update: -1170.25, Optimal reward -1009.77
Iteration 224 took 2.86 seconds (mean sampled reward: -4036.09). Current reward after update: -1189.83, Optimal reward -1009.77
Iteration 225 took 2.86 seconds (mean sampled reward: -3961.40). Current reward after update: -1210.46, Optimal reward -1009.77
Iteration 226 took 2.66 seconds (mean sampled reward: -3970.77). Current reward after update: -1179.10, Optimal reward -1009.77
Iteration 227 took 2.95 seconds (mean sampled reward: -3966.52). Current reward after update: -1179.21, Optimal reward -1009.77
Iteration 228 took 2.80 seconds (mean sampled reward: -4013.74). Current reward after update: -1179.14, Optimal reward -1009.77
Iteration 229 took 2.65 seconds (mean sampled reward: -3988.36). Current reward after update: -1213.99, Optimal reward -1009.77
Iteration 230 took 2.80 seconds (mean sampled reward: -3980.21). Current reward after update: -1185.86, Optimal reward -1009.77
Iteration 231 took 2.68 seconds (mean sampled reward: -3955.70). Current reward after update: -1024.79, Optimal reward -1009.77
Iteration 232 took 2.85 seconds (mean sampled reward: -3953.65). Current reward after update: -1259.37, Optimal reward -1009.77
Iteration 233 took 2.94 seconds (mean sampled reward: -4032.94). Current reward after update: -1233.62, Optimal reward -1009.77
Iteration 234 took 3.05 seconds (mean sampled reward: -3958.34). Current reward after update: -1185.20, Optimal reward -1009.77
Iteration 235 took 2.71 seconds (mean sampled reward: -3955.62). Current reward after update: -1226.03, Optimal reward -1009.77
Iteration 236 took 2.99 seconds (mean sampled reward: -4101.50). Current reward after update: -1205.15, Optimal reward -1009.77
Iteration 237 took 2.88 seconds (mean sampled reward: -3947.03). Current reward after update: -1214.62, Optimal reward -1009.77
Iteration 238 took 2.82 seconds (mean sampled reward: -3945.28). Current reward after update: -1225.32, Optimal reward -1009.77
Iteration 239 took 2.69 seconds (mean sampled reward: -4013.83). Current reward after update: -1180.87, Optimal reward -1009.77
Iteration 240 took 2.78 seconds (mean sampled reward: -4018.92). Current reward after update: -1194.87, Optimal reward -1009.77
Iteration 241 took 2.67 seconds (mean sampled reward: -4008.33). Current reward after update: -1176.67, Optimal reward -1009.77
Iteration 242 took 2.99 seconds (mean sampled reward: -4055.55). Current reward after update: -1199.61, Optimal reward -1009.77
Iteration 243 took 2.73 seconds (mean sampled reward: -4087.74). Current reward after update: -1237.58, Optimal reward -1009.77
Iteration 244 took 2.80 seconds (mean sampled reward: -4057.82). Current reward after update: -1194.24, Optimal reward -1009.77
Iteration 245 took 2.90 seconds (mean sampled reward: -4110.21). Current reward after update: -1430.09, Optimal reward -1009.77
Iteration 246 took 2.84 seconds (mean sampled reward: -4088.87). Current reward after update: -1193.28, Optimal reward -1009.77
Iteration 247 took 2.99 seconds (mean sampled reward: -4038.41). Current reward after update: -1180.84, Optimal reward -1009.77
Iteration 248 took 2.86 seconds (mean sampled reward: -4079.64). Current reward after update: -1204.45, Optimal reward -1009.77
Iteration 249 took 2.72 seconds (mean sampled reward: -4039.19). Current reward after update: -1223.42, Optimal reward -1009.77
Iteration 250 took 2.85 seconds (mean sampled reward: -4019.25). Current reward after update: -1182.27, Optimal reward -1009.77
Iteration 251 took 2.90 seconds (mean sampled reward: -4024.05). Current reward after update: -1185.64, Optimal reward -1009.77
Iteration 252 took 2.67 seconds (mean sampled reward: -4041.52). Current reward after update: -1226.03, Optimal reward -1009.77
Iteration 253 took 2.80 seconds (mean sampled reward: -4018.20). Current reward after update: -1225.98, Optimal reward -1009.77
Iteration 254 took 2.68 seconds (mean sampled reward: -4129.72). Current reward after update: -1219.84, Optimal reward -1009.77
Iteration 255 took 2.71 seconds (mean sampled reward: -4169.71). Current reward after update: -1202.43, Optimal reward -1009.77
Iteration 256 took 2.81 seconds (mean sampled reward: -4240.95). Current reward after update: -1354.98, Optimal reward -1009.77
Iteration 257 took 2.93 seconds (mean sampled reward: -4306.16). Current reward after update: -1192.59, Optimal reward -1009.77
Iteration 258 took 2.70 seconds (mean sampled reward: -4169.43). Current reward after update: -1172.67, Optimal reward -1009.77
Iteration 259 took 2.71 seconds (mean sampled reward: -4137.67). Current reward after update: -1178.89, Optimal reward -1009.77
Iteration 260 took 2.96 seconds (mean sampled reward: -4048.30). Current reward after update: -1218.68, Optimal reward -1009.77
Iteration 261 took 2.73 seconds (mean sampled reward: -4152.09). Current reward after update: -1216.29, Optimal reward -1009.77
Iteration 262 took 3.02 seconds (mean sampled reward: -4142.31). Current reward after update: -1179.92, Optimal reward -1009.77
Iteration 263 took 2.77 seconds (mean sampled reward: -4148.38). Current reward after update: -1209.71, Optimal reward -1009.77
Iteration 264 took 2.77 seconds (mean sampled reward: -4171.68). Current reward after update: -1280.77, Optimal reward -1009.77
Iteration 265 took 2.93 seconds (mean sampled reward: -4124.02). Current reward after update: -1206.02, Optimal reward -1009.77
Iteration 266 took 2.96 seconds (mean sampled reward: -4124.73). Current reward after update: -1203.64, Optimal reward -1009.77
Iteration 267 took 2.71 seconds (mean sampled reward: -4089.69). Current reward after update: -1213.72, Optimal reward -1009.77
Iteration 268 took 2.81 seconds (mean sampled reward: -4131.25). Current reward after update: -1236.70, Optimal reward -1009.77
Iteration 269 took 2.92 seconds (mean sampled reward: -4133.57). Current reward after update: -1231.23, Optimal reward -1009.77
Iteration 270 took 2.71 seconds (mean sampled reward: -4125.94). Current reward after update: -1221.81, Optimal reward -1009.77
Iteration 271 took 2.88 seconds (mean sampled reward: -4034.37). Current reward after update: -1168.04, Optimal reward -1009.77
Iteration 272 took 2.93 seconds (mean sampled reward: -4101.08). Current reward after update: -1222.51, Optimal reward -1009.77
Iteration 273 took 2.91 seconds (mean sampled reward: -4086.66). Current reward after update: -1277.09, Optimal reward -1009.77
Iteration 274 took 2.83 seconds (mean sampled reward: -4024.42). Current reward after update: -1206.93, Optimal reward -1009.77
Iteration 275 took 2.62 seconds (mean sampled reward: -4086.94). Current reward after update: -1273.60, Optimal reward -1009.77
Iteration 276 took 2.75 seconds (mean sampled reward: -4115.39). Current reward after update: -1225.05, Optimal reward -1009.77
Iteration 277 took 2.78 seconds (mean sampled reward: -4089.92). Current reward after update: -1208.97, Optimal reward -1009.77
Iteration 278 took 2.66 seconds (mean sampled reward: -4033.96). Current reward after update: -1191.28, Optimal reward -1009.77
Iteration 279 took 2.69 seconds (mean sampled reward: -4101.92). Current reward after update: -1263.98, Optimal reward -1009.77
Iteration 280 took 2.85 seconds (mean sampled reward: -3984.84). Current reward after update: -1212.06, Optimal reward -1009.77
Iteration 281 took 2.77 seconds (mean sampled reward: -4005.47). Current reward after update: -1210.27, Optimal reward -1009.77
Iteration 282 took 2.71 seconds (mean sampled reward: -3965.49). Current reward after update: -1195.32, Optimal reward -1009.77
Iteration 283 took 2.80 seconds (mean sampled reward: -3960.61). Current reward after update: -1189.64, Optimal reward -1009.77
Iteration 284 took 2.78 seconds (mean sampled reward: -3973.03). Current reward after update: -1226.36, Optimal reward -1009.77
Iteration 285 took 2.97 seconds (mean sampled reward: -3985.37). Current reward after update: -1198.37, Optimal reward -1009.77
Iteration 286 took 2.84 seconds (mean sampled reward: -4042.47). Current reward after update: -1236.90, Optimal reward -1009.77
Iteration 287 took 2.80 seconds (mean sampled reward: -4006.51). Current reward after update: -1231.68, Optimal reward -1009.77
Iteration 288 took 2.70 seconds (mean sampled reward: -3976.38). Current reward after update: -1204.81, Optimal reward -1009.77
Iteration 289 took 3.11 seconds (mean sampled reward: -3988.08). Current reward after update: -1222.69, Optimal reward -1009.77
Iteration 290 took 2.74 seconds (mean sampled reward: -3990.98). Current reward after update: -1199.16, Optimal reward -1009.77
Iteration 291 took 2.91 seconds (mean sampled reward: -4008.47). Current reward after update: -1187.92, Optimal reward -1009.77
Iteration 292 took 2.92 seconds (mean sampled reward: -4126.91). Current reward after update: -1196.37, Optimal reward -1009.77
Iteration 293 took 2.92 seconds (mean sampled reward: -4008.03). Current reward after update: -1199.18, Optimal reward -1009.77
Iteration 294 took 2.66 seconds (mean sampled reward: -4001.26). Current reward after update: -1203.84, Optimal reward -1009.77
Iteration 295 took 2.70 seconds (mean sampled reward: -3976.32). Current reward after update: -1203.58, Optimal reward -1009.77
Iteration 296 took 3.03 seconds (mean sampled reward: -3968.86). Current reward after update: -1588.07, Optimal reward -1009.77
Iteration 297 took 2.76 seconds (mean sampled reward: -4029.97). Current reward after update: -1224.26, Optimal reward -1009.77
Iteration 298 took 2.76 seconds (mean sampled reward: -4073.17). Current reward after update: -1229.10, Optimal reward -1009.77
Iteration 299 took 2.88 seconds (mean sampled reward: -3972.70). Current reward after update: -1176.88, Optimal reward -1009.77
Iteration 300 took 2.90 seconds (mean sampled reward: -3994.23). Current reward after update: -1232.96, Optimal reward -1009.77
Iteration 1 took 3.01 seconds (mean sampled reward: -4421.63). Current reward after update: -2047.40, Optimal reward -2047.40
Iteration 2 took 2.84 seconds (mean sampled reward: -4410.40). Current reward after update: -1556.90, Optimal reward -1556.90
Iteration 3 took 3.03 seconds (mean sampled reward: -4402.94). Current reward after update: -1449.35, Optimal reward -1449.35
Iteration 4 took 2.98 seconds (mean sampled reward: -4446.92). Current reward after update: -1367.61, Optimal reward -1367.61
Iteration 5 took 3.00 seconds (mean sampled reward: -4391.85). Current reward after update: -1396.12, Optimal reward -1367.61
Iteration 6 took 2.73 seconds (mean sampled reward: -4397.12). Current reward after update: -1396.33, Optimal reward -1367.61
Iteration 7 took 2.62 seconds (mean sampled reward: -4423.25). Current reward after update: -1295.03, Optimal reward -1295.03
Iteration 8 took 2.90 seconds (mean sampled reward: -4287.31). Current reward after update: -1272.80, Optimal reward -1272.80
Iteration 9 took 2.95 seconds (mean sampled reward: -4216.36). Current reward after update: -1099.61, Optimal reward -1099.61
Iteration 10 took 2.72 seconds (mean sampled reward: -4132.30). Current reward after update: -1221.75, Optimal reward -1099.61
Iteration 11 took 2.79 seconds (mean sampled reward: -4152.62). Current reward after update: -1099.70, Optimal reward -1099.61
Iteration 12 took 2.71 seconds (mean sampled reward: -4196.36). Current reward after update: -1156.83, Optimal reward -1099.61
Iteration 13 took 2.60 seconds (mean sampled reward: -4271.24). Current reward after update: -1070.14, Optimal reward -1070.14
Iteration 14 took 2.53 seconds (mean sampled reward: -3918.82). Current reward after update: -976.68, Optimal reward -976.68
Iteration 15 took 2.78 seconds (mean sampled reward: -3945.57). Current reward after update: -1149.05, Optimal reward -976.68
Iteration 16 took 2.76 seconds (mean sampled reward: -4102.57). Current reward after update: -1202.61, Optimal reward -976.68
Iteration 17 took 2.84 seconds (mean sampled reward: -3785.58). Current reward after update: -1069.07, Optimal reward -976.68
Iteration 18 took 2.65 seconds (mean sampled reward: -3566.96). Current reward after update: -712.79, Optimal reward -712.79
Iteration 19 took 2.67 seconds (mean sampled reward: -3644.14). Current reward after update: -1114.07, Optimal reward -712.79
Iteration 20 took 2.64 seconds (mean sampled reward: -3709.86). Current reward after update: -950.77, Optimal reward -712.79
Iteration 21 took 2.75 seconds (mean sampled reward: -3715.27). Current reward after update: -1223.71, Optimal reward -712.79
Iteration 22 took 2.68 seconds (mean sampled reward: -3589.10). Current reward after update: -1042.12, Optimal reward -712.79
Iteration 23 took 2.55 seconds (mean sampled reward: -3408.80). Current reward after update: -878.37, Optimal reward -712.79
Iteration 24 took 2.67 seconds (mean sampled reward: -3439.65). Current reward after update: -989.33, Optimal reward -712.79
Iteration 25 took 2.43 seconds (mean sampled reward: -3456.65). Current reward after update: -735.91, Optimal reward -712.79
Iteration 26 took 2.65 seconds (mean sampled reward: -3868.34). Current reward after update: -912.43, Optimal reward -712.79
Iteration 27 took 2.71 seconds (mean sampled reward: -3839.62). Current reward after update: -1241.37, Optimal reward -712.79
Iteration 28 took 2.55 seconds (mean sampled reward: -3985.67). Current reward after update: -1152.24, Optimal reward -712.79
Iteration 29 took 2.82 seconds (mean sampled reward: -3864.54). Current reward after update: -1267.95, Optimal reward -712.79
Iteration 30 took 2.51 seconds (mean sampled reward: -3763.40). Current reward after update: -972.44, Optimal reward -712.79
Iteration 31 took 2.76 seconds (mean sampled reward: -3744.97). Current reward after update: -1303.18, Optimal reward -712.79
Iteration 32 took 2.72 seconds (mean sampled reward: -3443.86). Current reward after update: -847.45, Optimal reward -712.79
Iteration 33 took 2.53 seconds (mean sampled reward: -3603.79). Current reward after update: -890.71, Optimal reward -712.79
Iteration 34 took 2.48 seconds (mean sampled reward: -3345.53). Current reward after update: -859.50, Optimal reward -712.79
Iteration 35 took 2.49 seconds (mean sampled reward: -3596.97). Current reward after update: -824.20, Optimal reward -712.79
Iteration 36 took 2.75 seconds (mean sampled reward: -3825.02). Current reward after update: -785.05, Optimal reward -712.79
Iteration 37 took 2.39 seconds (mean sampled reward: -3752.30). Current reward after update: -829.76, Optimal reward -712.79
Iteration 38 took 2.81 seconds (mean sampled reward: -3563.44). Current reward after update: -826.58, Optimal reward -712.79
Iteration 39 took 2.42 seconds (mean sampled reward: -3445.83). Current reward after update: -771.85, Optimal reward -712.79
Iteration 40 took 2.56 seconds (mean sampled reward: -3458.07). Current reward after update: -863.10, Optimal reward -712.79
Iteration 41 took 2.45 seconds (mean sampled reward: -3608.98). Current reward after update: -875.76, Optimal reward -712.79
Iteration 42 took 2.65 seconds (mean sampled reward: -3550.31). Current reward after update: -903.04, Optimal reward -712.79
Iteration 43 took 2.46 seconds (mean sampled reward: -3272.70). Current reward after update: -612.82, Optimal reward -612.82
Iteration 44 took 2.53 seconds (mean sampled reward: -3619.50). Current reward after update: -750.56, Optimal reward -612.82
Iteration 45 took 2.39 seconds (mean sampled reward: -3443.71). Current reward after update: -677.47, Optimal reward -612.82
Iteration 46 took 2.49 seconds (mean sampled reward: -3522.37). Current reward after update: -849.50, Optimal reward -612.82
Iteration 47 took 2.51 seconds (mean sampled reward: -3562.56). Current reward after update: -811.23, Optimal reward -612.82
Iteration 48 took 2.81 seconds (mean sampled reward: -3318.12). Current reward after update: -708.89, Optimal reward -612.82
Iteration 49 took 2.69 seconds (mean sampled reward: -3561.24). Current reward after update: -632.59, Optimal reward -612.82
Iteration 50 took 2.83 seconds (mean sampled reward: -3257.20). Current reward after update: -653.64, Optimal reward -612.82
Iteration 51 took 2.55 seconds (mean sampled reward: -3708.04). Current reward after update: -1078.14, Optimal reward -612.82
Iteration 52 took 2.48 seconds (mean sampled reward: -3735.49). Current reward after update: -789.10, Optimal reward -612.82
Iteration 53 took 2.79 seconds (mean sampled reward: -3337.18). Current reward after update: -761.31, Optimal reward -612.82
Iteration 54 took 2.44 seconds (mean sampled reward: -3550.43). Current reward after update: -754.04, Optimal reward -612.82
Iteration 55 took 2.34 seconds (mean sampled reward: -3579.45). Current reward after update: -702.26, Optimal reward -612.82
Iteration 56 took 2.65 seconds (mean sampled reward: -3530.38). Current reward after update: -701.90, Optimal reward -612.82
Iteration 57 took 2.60 seconds (mean sampled reward: -3413.29). Current reward after update: -691.60, Optimal reward -612.82
Iteration 58 took 2.54 seconds (mean sampled reward: -3170.02). Current reward after update: -707.96, Optimal reward -612.82
Iteration 59 took 2.69 seconds (mean sampled reward: -3271.11). Current reward after update: -733.58, Optimal reward -612.82
Iteration 60 took 2.56 seconds (mean sampled reward: -3427.22). Current reward after update: -683.64, Optimal reward -612.82
Iteration 61 took 2.55 seconds (mean sampled reward: -3407.20). Current reward after update: -656.16, Optimal reward -612.82
Iteration 62 took 2.76 seconds (mean sampled reward: -3523.63). Current reward after update: -675.85, Optimal reward -612.82
Iteration 63 took 2.74 seconds (mean sampled reward: -3242.75). Current reward after update: -643.63, Optimal reward -612.82
Iteration 64 took 2.44 seconds (mean sampled reward: -3318.47). Current reward after update: -664.46, Optimal reward -612.82
Iteration 65 took 2.49 seconds (mean sampled reward: -3428.32). Current reward after update: -661.77, Optimal reward -612.82
Iteration 66 took 2.63 seconds (mean sampled reward: -3279.67). Current reward after update: -648.22, Optimal reward -612.82
Iteration 67 took 2.58 seconds (mean sampled reward: -3383.77). Current reward after update: -648.72, Optimal reward -612.82
Iteration 68 took 2.61 seconds (mean sampled reward: -3377.64). Current reward after update: -742.39, Optimal reward -612.82
Iteration 69 took 2.55 seconds (mean sampled reward: -3409.21). Current reward after update: -688.55, Optimal reward -612.82
Iteration 70 took 2.37 seconds (mean sampled reward: -3453.99). Current reward after update: -737.42, Optimal reward -612.82
Iteration 71 took 2.48 seconds (mean sampled reward: -3369.43). Current reward after update: -675.61, Optimal reward -612.82
Iteration 72 took 2.43 seconds (mean sampled reward: -3438.37). Current reward after update: -692.08, Optimal reward -612.82
Iteration 73 took 2.44 seconds (mean sampled reward: -3572.44). Current reward after update: -717.37, Optimal reward -612.82
Iteration 74 took 2.37 seconds (mean sampled reward: -3476.90). Current reward after update: -670.81, Optimal reward -612.82
Iteration 75 took 2.39 seconds (mean sampled reward: -3396.93). Current reward after update: -675.40, Optimal reward -612.82
Iteration 76 took 2.70 seconds (mean sampled reward: -3503.88). Current reward after update: -803.77, Optimal reward -612.82
Iteration 77 took 2.85 seconds (mean sampled reward: -3562.65). Current reward after update: -886.41, Optimal reward -612.82
Iteration 78 took 2.54 seconds (mean sampled reward: -3541.83). Current reward after update: -697.43, Optimal reward -612.82
Iteration 79 took 2.36 seconds (mean sampled reward: -3621.14). Current reward after update: -733.36, Optimal reward -612.82
Iteration 80 took 2.49 seconds (mean sampled reward: -3707.23). Current reward after update: -652.30, Optimal reward -612.82
Iteration 81 took 2.25 seconds (mean sampled reward: -3870.60). Current reward after update: -727.89, Optimal reward -612.82
Iteration 82 took 2.33 seconds (mean sampled reward: -3764.39). Current reward after update: -728.63, Optimal reward -612.82
Iteration 83 took 2.46 seconds (mean sampled reward: -3810.01). Current reward after update: -614.90, Optimal reward -612.82
Iteration 84 took 2.53 seconds (mean sampled reward: -3691.33). Current reward after update: -670.51, Optimal reward -612.82
Iteration 85 took 2.46 seconds (mean sampled reward: -3662.98). Current reward after update: -729.22, Optimal reward -612.82
Iteration 86 took 2.26 seconds (mean sampled reward: -3657.82). Current reward after update: -637.86, Optimal reward -612.82
Iteration 87 took 2.33 seconds (mean sampled reward: -3679.41). Current reward after update: -744.87, Optimal reward -612.82
Iteration 88 took 2.73 seconds (mean sampled reward: -3434.93). Current reward after update: -696.80, Optimal reward -612.82
Iteration 89 took 2.70 seconds (mean sampled reward: -3473.41). Current reward after update: -704.94, Optimal reward -612.82
Iteration 90 took 2.56 seconds (mean sampled reward: -3485.62). Current reward after update: -615.29, Optimal reward -612.82
Iteration 91 took 2.61 seconds (mean sampled reward: -3365.44). Current reward after update: -688.05, Optimal reward -612.82
Iteration 92 took 2.61 seconds (mean sampled reward: -3360.93). Current reward after update: -610.55, Optimal reward -610.55
Iteration 93 took 2.64 seconds (mean sampled reward: -3314.17). Current reward after update: -821.51, Optimal reward -610.55
Iteration 94 took 2.55 seconds (mean sampled reward: -3189.92). Current reward after update: -674.30, Optimal reward -610.55
Iteration 95 took 2.62 seconds (mean sampled reward: -3193.92). Current reward after update: -590.69, Optimal reward -590.69
Iteration 96 took 2.46 seconds (mean sampled reward: -3261.12). Current reward after update: -673.33, Optimal reward -590.69
Iteration 97 took 2.82 seconds (mean sampled reward: -3310.78). Current reward after update: -810.26, Optimal reward -590.69
Iteration 98 took 2.59 seconds (mean sampled reward: -3306.23). Current reward after update: -685.21, Optimal reward -590.69
Iteration 99 took 2.75 seconds (mean sampled reward: -3268.28). Current reward after update: -687.18, Optimal reward -590.69
Iteration 100 took 2.44 seconds (mean sampled reward: -3290.24). Current reward after update: -669.42, Optimal reward -590.69
Iteration 101 took 2.81 seconds (mean sampled reward: -3367.64). Current reward after update: -739.75, Optimal reward -590.69
Iteration 102 took 2.79 seconds (mean sampled reward: -3518.54). Current reward after update: -660.97, Optimal reward -590.69
Iteration 103 took 2.57 seconds (mean sampled reward: -3788.43). Current reward after update: -795.39, Optimal reward -590.69
Iteration 104 took 2.85 seconds (mean sampled reward: -3660.39). Current reward after update: -933.60, Optimal reward -590.69
Iteration 105 took 2.69 seconds (mean sampled reward: -3573.60). Current reward after update: -628.55, Optimal reward -590.69
Iteration 106 took 2.42 seconds (mean sampled reward: -3552.99). Current reward after update: -1038.13, Optimal reward -590.69
Iteration 107 took 2.41 seconds (mean sampled reward: -3367.53). Current reward after update: -624.96, Optimal reward -590.69
Iteration 108 took 2.69 seconds (mean sampled reward: -3238.42). Current reward after update: -669.82, Optimal reward -590.69
Iteration 109 took 2.64 seconds (mean sampled reward: -3168.68). Current reward after update: -723.89, Optimal reward -590.69
Iteration 110 took 2.46 seconds (mean sampled reward: -3312.02). Current reward after update: -642.98, Optimal reward -590.69
Iteration 111 took 2.84 seconds (mean sampled reward: -3458.98). Current reward after update: -729.56, Optimal reward -590.69
Iteration 112 took 2.69 seconds (mean sampled reward: -3423.46). Current reward after update: -666.95, Optimal reward -590.69
Iteration 113 took 2.43 seconds (mean sampled reward: -3456.85). Current reward after update: -618.36, Optimal reward -590.69
Iteration 114 took 2.76 seconds (mean sampled reward: -3427.69). Current reward after update: -655.07, Optimal reward -590.69
Iteration 115 took 2.43 seconds (mean sampled reward: -3373.11). Current reward after update: -466.38, Optimal reward -466.38
Iteration 116 took 2.65 seconds (mean sampled reward: -3535.15). Current reward after update: -471.31, Optimal reward -466.38
Iteration 117 took 2.50 seconds (mean sampled reward: -3408.96). Current reward after update: -472.39, Optimal reward -466.38
Iteration 118 took 2.82 seconds (mean sampled reward: -3468.77). Current reward after update: -551.08, Optimal reward -466.38
Iteration 119 took 2.60 seconds (mean sampled reward: -3496.66). Current reward after update: -535.12, Optimal reward -466.38
Iteration 120 took 2.59 seconds (mean sampled reward: -3475.76). Current reward after update: -589.45, Optimal reward -466.38
Iteration 121 took 2.74 seconds (mean sampled reward: -3486.77). Current reward after update: -520.51, Optimal reward -466.38
Iteration 122 took 2.46 seconds (mean sampled reward: -3584.53). Current reward after update: -553.30, Optimal reward -466.38
Iteration 123 took 2.41 seconds (mean sampled reward: -3622.01). Current reward after update: -515.33, Optimal reward -466.38
Iteration 124 took 2.64 seconds (mean sampled reward: -3690.40). Current reward after update: -472.34, Optimal reward -466.38
Iteration 125 took 2.44 seconds (mean sampled reward: -3797.67). Current reward after update: -442.61, Optimal reward -442.61
Iteration 126 took 2.49 seconds (mean sampled reward: -3773.76). Current reward after update: -561.24, Optimal reward -442.61
Iteration 127 took 2.47 seconds (mean sampled reward: -3691.99). Current reward after update: -524.35, Optimal reward -442.61
Iteration 128 took 2.57 seconds (mean sampled reward: -3644.72). Current reward after update: -413.99, Optimal reward -413.99
Iteration 129 took 2.44 seconds (mean sampled reward: -3607.50). Current reward after update: -436.78, Optimal reward -413.99
Iteration 130 took 2.67 seconds (mean sampled reward: -3834.91). Current reward after update: -526.51, Optimal reward -413.99
Iteration 131 took 2.74 seconds (mean sampled reward: -3706.23). Current reward after update: -513.77, Optimal reward -413.99
Iteration 132 took 2.59 seconds (mean sampled reward: -3502.75). Current reward after update: -470.04, Optimal reward -413.99
Iteration 133 took 2.48 seconds (mean sampled reward: -3655.40). Current reward after update: -489.93, Optimal reward -413.99
Iteration 134 took 2.69 seconds (mean sampled reward: -3710.84). Current reward after update: -480.63, Optimal reward -413.99
Iteration 135 took 2.51 seconds (mean sampled reward: -3415.09). Current reward after update: -471.40, Optimal reward -413.99
Iteration 136 took 2.59 seconds (mean sampled reward: -3414.18). Current reward after update: -437.57, Optimal reward -413.99
Iteration 137 took 2.52 seconds (mean sampled reward: -3635.48). Current reward after update: -770.70, Optimal reward -413.99
Iteration 138 took 2.55 seconds (mean sampled reward: -3850.65). Current reward after update: -808.19, Optimal reward -413.99
Iteration 139 took 2.86 seconds (mean sampled reward: -4135.05). Current reward after update: -491.05, Optimal reward -413.99
Iteration 140 took 2.48 seconds (mean sampled reward: -3763.10). Current reward after update: -502.44, Optimal reward -413.99
Iteration 141 took 2.61 seconds (mean sampled reward: -3932.55). Current reward after update: -495.66, Optimal reward -413.99
Iteration 142 took 2.55 seconds (mean sampled reward: -3656.69). Current reward after update: -499.84, Optimal reward -413.99
Iteration 143 took 2.70 seconds (mean sampled reward: -3552.55). Current reward after update: -463.37, Optimal reward -413.99
Iteration 144 took 2.52 seconds (mean sampled reward: -3686.84). Current reward after update: -886.59, Optimal reward -413.99
Iteration 145 took 2.51 seconds (mean sampled reward: -3600.94). Current reward after update: -443.39, Optimal reward -413.99
Iteration 146 took 2.67 seconds (mean sampled reward: -3693.07). Current reward after update: -1021.79, Optimal reward -413.99
Iteration 147 took 2.54 seconds (mean sampled reward: -3731.32). Current reward after update: -471.40, Optimal reward -413.99
Iteration 148 took 2.45 seconds (mean sampled reward: -3516.55). Current reward after update: -477.13, Optimal reward -413.99
Iteration 149 took 2.64 seconds (mean sampled reward: -3470.51). Current reward after update: -461.53, Optimal reward -413.99
Iteration 150 took 2.78 seconds (mean sampled reward: -3697.36). Current reward after update: -356.10, Optimal reward -356.10
Iteration 151 took 2.54 seconds (mean sampled reward: -3730.36). Current reward after update: -473.32, Optimal reward -356.10
Iteration 152 took 2.42 seconds (mean sampled reward: -3526.94). Current reward after update: -368.10, Optimal reward -356.10
Iteration 153 took 2.74 seconds (mean sampled reward: -3587.23). Current reward after update: -352.65, Optimal reward -352.65
Iteration 154 took 2.65 seconds (mean sampled reward: -3704.75). Current reward after update: -614.96, Optimal reward -352.65
Iteration 155 took 2.73 seconds (mean sampled reward: -3861.68). Current reward after update: -547.40, Optimal reward -352.65
Iteration 156 took 2.72 seconds (mean sampled reward: -3990.51). Current reward after update: -804.29, Optimal reward -352.65
Iteration 157 took 2.57 seconds (mean sampled reward: -4030.99). Current reward after update: -1049.69, Optimal reward -352.65
Iteration 158 took 2.79 seconds (mean sampled reward: -4097.49). Current reward after update: -692.12, Optimal reward -352.65
Iteration 159 took 2.54 seconds (mean sampled reward: -4173.01). Current reward after update: -947.70, Optimal reward -352.65
Iteration 160 took 2.73 seconds (mean sampled reward: -4189.67). Current reward after update: -610.59, Optimal reward -352.65
Iteration 161 took 2.45 seconds (mean sampled reward: -3906.25). Current reward after update: -990.87, Optimal reward -352.65
Iteration 162 took 2.50 seconds (mean sampled reward: -3747.57). Current reward after update: -1017.92, Optimal reward -352.65
Iteration 163 took 2.48 seconds (mean sampled reward: -3913.74). Current reward after update: -684.04, Optimal reward -352.65
Iteration 164 took 2.81 seconds (mean sampled reward: -3903.83). Current reward after update: -850.20, Optimal reward -352.65
Iteration 165 took 2.49 seconds (mean sampled reward: -3881.47). Current reward after update: -349.16, Optimal reward -349.16
Iteration 166 took 2.42 seconds (mean sampled reward: -3716.23). Current reward after update: -781.72, Optimal reward -349.16
Iteration 167 took 2.59 seconds (mean sampled reward: -3627.78). Current reward after update: -552.46, Optimal reward -349.16
Iteration 168 took 2.67 seconds (mean sampled reward: -3621.69). Current reward after update: -444.33, Optimal reward -349.16
Iteration 169 took 2.81 seconds (mean sampled reward: -3626.05). Current reward after update: -377.21, Optimal reward -349.16
Iteration 170 took 2.49 seconds (mean sampled reward: -3797.74). Current reward after update: -600.52, Optimal reward -349.16
Iteration 171 took 2.47 seconds (mean sampled reward: -3638.87). Current reward after update: -993.21, Optimal reward -349.16
Iteration 172 took 2.51 seconds (mean sampled reward: -3582.20). Current reward after update: -597.95, Optimal reward -349.16
Iteration 173 took 2.44 seconds (mean sampled reward: -3803.24). Current reward after update: -956.23, Optimal reward -349.16
Iteration 174 took 2.60 seconds (mean sampled reward: -3543.65). Current reward after update: -1230.57, Optimal reward -349.16
Iteration 175 took 2.46 seconds (mean sampled reward: -3411.70). Current reward after update: -699.00, Optimal reward -349.16
Iteration 176 took 2.81 seconds (mean sampled reward: -3501.75). Current reward after update: -670.69, Optimal reward -349.16
Iteration 177 took 2.61 seconds (mean sampled reward: -3404.91). Current reward after update: -431.35, Optimal reward -349.16
Iteration 178 took 2.48 seconds (mean sampled reward: -3554.48). Current reward after update: -1040.84, Optimal reward -349.16
Iteration 179 took 2.78 seconds (mean sampled reward: -3631.36). Current reward after update: -797.40, Optimal reward -349.16
Iteration 180 took 2.43 seconds (mean sampled reward: -3622.36). Current reward after update: -556.47, Optimal reward -349.16
Iteration 181 took 2.60 seconds (mean sampled reward: -3599.35). Current reward after update: -540.88, Optimal reward -349.16
Iteration 182 took 2.42 seconds (mean sampled reward: -3657.49). Current reward after update: -547.02, Optimal reward -349.16
Iteration 183 took 2.34 seconds (mean sampled reward: -3539.63). Current reward after update: -1326.11, Optimal reward -349.16
Iteration 184 took 2.64 seconds (mean sampled reward: -3689.25). Current reward after update: -691.83, Optimal reward -349.16
Iteration 185 took 2.95 seconds (mean sampled reward: -3649.14). Current reward after update: -489.06, Optimal reward -349.16
Iteration 186 took 2.37 seconds (mean sampled reward: -3900.05). Current reward after update: -962.60, Optimal reward -349.16
Iteration 187 took 2.44 seconds (mean sampled reward: -3968.22). Current reward after update: -1501.00, Optimal reward -349.16
Iteration 188 took 2.53 seconds (mean sampled reward: -3966.35). Current reward after update: -1028.91, Optimal reward -349.16
Iteration 189 took 2.53 seconds (mean sampled reward: -3989.52). Current reward after update: -1389.13, Optimal reward -349.16
Iteration 190 took 2.46 seconds (mean sampled reward: -3961.14). Current reward after update: -669.57, Optimal reward -349.16
Iteration 191 took 2.25 seconds (mean sampled reward: -4002.05). Current reward after update: -1405.00, Optimal reward -349.16
Iteration 192 took 2.30 seconds (mean sampled reward: -4036.23). Current reward after update: -1027.32, Optimal reward -349.16
Iteration 193 took 2.42 seconds (mean sampled reward: -3955.01). Current reward after update: -587.68, Optimal reward -349.16
Iteration 194 took 2.25 seconds (mean sampled reward: -3851.81). Current reward after update: -1216.52, Optimal reward -349.16
Iteration 195 took 2.56 seconds (mean sampled reward: -3907.83). Current reward after update: -501.57, Optimal reward -349.16
Iteration 196 took 2.76 seconds (mean sampled reward: -3402.92). Current reward after update: -673.24, Optimal reward -349.16
Iteration 197 took 2.75 seconds (mean sampled reward: -3485.19). Current reward after update: -463.12, Optimal reward -349.16
Iteration 198 took 2.71 seconds (mean sampled reward: -3413.43). Current reward after update: -420.17, Optimal reward -349.16
Iteration 199 took 2.82 seconds (mean sampled reward: -3420.49). Current reward after update: -845.21, Optimal reward -349.16
Iteration 200 took 2.45 seconds (mean sampled reward: -3303.39). Current reward after update: -464.30, Optimal reward -349.16
Iteration 201 took 2.76 seconds (mean sampled reward: -3399.56). Current reward after update: -493.17, Optimal reward -349.16
Iteration 202 took 2.66 seconds (mean sampled reward: -3444.93). Current reward after update: -475.09, Optimal reward -349.16
Iteration 203 took 2.46 seconds (mean sampled reward: -3413.84). Current reward after update: -454.64, Optimal reward -349.16
Iteration 204 took 2.63 seconds (mean sampled reward: -3438.63). Current reward after update: -537.35, Optimal reward -349.16
Iteration 205 took 2.64 seconds (mean sampled reward: -3404.72). Current reward after update: -456.91, Optimal reward -349.16
Iteration 206 took 2.77 seconds (mean sampled reward: -3397.06). Current reward after update: -477.26, Optimal reward -349.16
Iteration 207 took 2.62 seconds (mean sampled reward: -3505.15). Current reward after update: -528.26, Optimal reward -349.16
Iteration 208 took 2.67 seconds (mean sampled reward: -3455.01). Current reward after update: -442.98, Optimal reward -349.16
Iteration 209 took 2.43 seconds (mean sampled reward: -3342.22). Current reward after update: -479.62, Optimal reward -349.16
Iteration 210 took 2.61 seconds (mean sampled reward: -3435.39). Current reward after update: -469.89, Optimal reward -349.16
Iteration 211 took 2.63 seconds (mean sampled reward: -3551.26). Current reward after update: -503.66, Optimal reward -349.16
Iteration 212 took 2.54 seconds (mean sampled reward: -3478.18). Current reward after update: -465.62, Optimal reward -349.16
Iteration 213 took 2.52 seconds (mean sampled reward: -3618.43). Current reward after update: -542.96, Optimal reward -349.16
Iteration 214 took 2.67 seconds (mean sampled reward: -3417.29). Current reward after update: -528.52, Optimal reward -349.16
Iteration 215 took 2.66 seconds (mean sampled reward: -3408.42). Current reward after update: -486.73, Optimal reward -349.16
Iteration 216 took 2.77 seconds (mean sampled reward: -3429.83). Current reward after update: -603.87, Optimal reward -349.16
Iteration 217 took 2.55 seconds (mean sampled reward: -3472.81). Current reward after update: -475.04, Optimal reward -349.16
Iteration 218 took 2.47 seconds (mean sampled reward: -3515.79). Current reward after update: -488.95, Optimal reward -349.16
Iteration 219 took 2.59 seconds (mean sampled reward: -3532.21). Current reward after update: -432.11, Optimal reward -349.16
Iteration 220 took 2.74 seconds (mean sampled reward: -3447.00). Current reward after update: -578.74, Optimal reward -349.16
Iteration 221 took 2.45 seconds (mean sampled reward: -3695.41). Current reward after update: -757.69, Optimal reward -349.16
Iteration 222 took 2.45 seconds (mean sampled reward: -3596.26). Current reward after update: -491.36, Optimal reward -349.16
Iteration 223 took 2.45 seconds (mean sampled reward: -3374.95). Current reward after update: -613.60, Optimal reward -349.16
Iteration 224 took 2.62 seconds (mean sampled reward: -3387.54). Current reward after update: -496.84, Optimal reward -349.16
Iteration 225 took 2.69 seconds (mean sampled reward: -3416.19). Current reward after update: -495.82, Optimal reward -349.16
Iteration 226 took 2.68 seconds (mean sampled reward: -3478.74). Current reward after update: -513.14, Optimal reward -349.16
Iteration 227 took 2.66 seconds (mean sampled reward: -3525.52). Current reward after update: -627.46, Optimal reward -349.16
Iteration 228 took 2.89 seconds (mean sampled reward: -3347.84). Current reward after update: -525.62, Optimal reward -349.16
Iteration 229 took 2.57 seconds (mean sampled reward: -3386.04). Current reward after update: -504.53, Optimal reward -349.16
Iteration 230 took 2.47 seconds (mean sampled reward: -3386.59). Current reward after update: -635.09, Optimal reward -349.16
Iteration 231 took 2.66 seconds (mean sampled reward: -3582.17). Current reward after update: -572.17, Optimal reward -349.16
Iteration 232 took 2.44 seconds (mean sampled reward: -3745.45). Current reward after update: -587.24, Optimal reward -349.16
Iteration 233 took 2.49 seconds (mean sampled reward: -3725.79). Current reward after update: -539.18, Optimal reward -349.16
Iteration 234 took 2.41 seconds (mean sampled reward: -3430.98). Current reward after update: -525.74, Optimal reward -349.16
Iteration 235 took 2.57 seconds (mean sampled reward: -3565.28). Current reward after update: -567.70, Optimal reward -349.16
Iteration 236 took 2.40 seconds (mean sampled reward: -3765.86). Current reward after update: -597.60, Optimal reward -349.16
Iteration 237 took 2.55 seconds (mean sampled reward: -3730.91). Current reward after update: -684.50, Optimal reward -349.16
Iteration 238 took 2.59 seconds (mean sampled reward: -3639.50). Current reward after update: -675.23, Optimal reward -349.16
Iteration 239 took 2.43 seconds (mean sampled reward: -3633.04). Current reward after update: -622.03, Optimal reward -349.16
Iteration 240 took 2.81 seconds (mean sampled reward: -3495.79). Current reward after update: -1024.11, Optimal reward -349.16
Iteration 241 took 2.55 seconds (mean sampled reward: -3731.96). Current reward after update: -438.97, Optimal reward -349.16
Iteration 242 took 2.49 seconds (mean sampled reward: -3741.52). Current reward after update: -1015.35, Optimal reward -349.16
Iteration 243 took 2.66 seconds (mean sampled reward: -3668.53). Current reward after update: -735.88, Optimal reward -349.16
Iteration 244 took 2.45 seconds (mean sampled reward: -3502.51). Current reward after update: -776.27, Optimal reward -349.16
Iteration 245 took 2.60 seconds (mean sampled reward: -3460.95). Current reward after update: -1057.88, Optimal reward -349.16
Iteration 246 took 2.57 seconds (mean sampled reward: -3524.48). Current reward after update: -1188.23, Optimal reward -349.16
Iteration 247 took 2.55 seconds (mean sampled reward: -3484.96). Current reward after update: -985.45, Optimal reward -349.16
Iteration 248 took 2.47 seconds (mean sampled reward: -3552.20). Current reward after update: -966.39, Optimal reward -349.16
Iteration 249 took 2.43 seconds (mean sampled reward: -3567.28). Current reward after update: -910.00, Optimal reward -349.16
Iteration 250 took 2.48 seconds (mean sampled reward: -3585.29). Current reward after update: -828.24, Optimal reward -349.16
Iteration 251 took 2.80 seconds (mean sampled reward: -3608.22). Current reward after update: -1255.95, Optimal reward -349.16
Iteration 252 took 2.53 seconds (mean sampled reward: -3635.18). Current reward after update: -1366.81, Optimal reward -349.16
Iteration 253 took 2.75 seconds (mean sampled reward: -3509.60). Current reward after update: -914.85, Optimal reward -349.16
Iteration 254 took 2.53 seconds (mean sampled reward: -3562.51). Current reward after update: -439.72, Optimal reward -349.16
Iteration 255 took 2.60 seconds (mean sampled reward: -3359.49). Current reward after update: -411.69, Optimal reward -349.16
Iteration 256 took 2.90 seconds (mean sampled reward: -3336.19). Current reward after update: -420.85, Optimal reward -349.16
Iteration 257 took 2.37 seconds (mean sampled reward: -3456.84). Current reward after update: -764.68, Optimal reward -349.16
Iteration 258 took 2.65 seconds (mean sampled reward: -3393.55). Current reward after update: -424.83, Optimal reward -349.16
Iteration 259 took 2.57 seconds (mean sampled reward: -3272.34). Current reward after update: -467.19, Optimal reward -349.16
Iteration 260 took 2.61 seconds (mean sampled reward: -3285.40). Current reward after update: -1471.73, Optimal reward -349.16
Iteration 261 took 2.42 seconds (mean sampled reward: -3384.91). Current reward after update: -422.93, Optimal reward -349.16
Iteration 262 took 2.51 seconds (mean sampled reward: -3470.98). Current reward after update: -425.40, Optimal reward -349.16
Iteration 263 took 2.67 seconds (mean sampled reward: -3413.40). Current reward after update: -1367.21, Optimal reward -349.16
Iteration 264 took 2.40 seconds (mean sampled reward: -3332.68). Current reward after update: -425.56, Optimal reward -349.16
Iteration 265 took 2.45 seconds (mean sampled reward: -3394.12). Current reward after update: -432.34, Optimal reward -349.16
Iteration 266 took 2.54 seconds (mean sampled reward: -3334.98). Current reward after update: -453.88, Optimal reward -349.16
Iteration 267 took 2.54 seconds (mean sampled reward: -3263.30). Current reward after update: -413.45, Optimal reward -349.16
Iteration 268 took 2.43 seconds (mean sampled reward: -3491.67). Current reward after update: -420.08, Optimal reward -349.16
Iteration 269 took 2.38 seconds (mean sampled reward: -3322.88). Current reward after update: -413.12, Optimal reward -349.16
Iteration 270 took 2.35 seconds (mean sampled reward: -3470.89). Current reward after update: -432.82, Optimal reward -349.16
Iteration 271 took 2.51 seconds (mean sampled reward: -3423.83). Current reward after update: -450.00, Optimal reward -349.16
Iteration 272 took 2.35 seconds (mean sampled reward: -3302.70). Current reward after update: -397.55, Optimal reward -349.16
Iteration 273 took 2.55 seconds (mean sampled reward: -3390.19). Current reward after update: -408.72, Optimal reward -349.16
Iteration 274 took 2.43 seconds (mean sampled reward: -3317.77). Current reward after update: -408.31, Optimal reward -349.16
Iteration 275 took 2.77 seconds (mean sampled reward: -3242.95). Current reward after update: -402.41, Optimal reward -349.16
Iteration 276 took 2.49 seconds (mean sampled reward: -3202.89). Current reward after update: -416.74, Optimal reward -349.16
Iteration 277 took 2.38 seconds (mean sampled reward: -3359.61). Current reward after update: -406.74, Optimal reward -349.16
Iteration 278 took 2.59 seconds (mean sampled reward: -3279.26). Current reward after update: -422.84, Optimal reward -349.16
Iteration 279 took 2.50 seconds (mean sampled reward: -3270.28). Current reward after update: -392.03, Optimal reward -349.16
Iteration 280 took 2.39 seconds (mean sampled reward: -3293.84). Current reward after update: -418.64, Optimal reward -349.16
Iteration 281 took 2.60 seconds (mean sampled reward: -3395.97). Current reward after update: -416.92, Optimal reward -349.16
Iteration 282 took 2.62 seconds (mean sampled reward: -3370.68). Current reward after update: -475.49, Optimal reward -349.16
Iteration 283 took 2.44 seconds (mean sampled reward: -3286.35). Current reward after update: -489.56, Optimal reward -349.16
Iteration 284 took 2.52 seconds (mean sampled reward: -3376.45). Current reward after update: -453.18, Optimal reward -349.16
Iteration 285 took 2.40 seconds (mean sampled reward: -3431.76). Current reward after update: -522.69, Optimal reward -349.16
Iteration 286 took 2.63 seconds (mean sampled reward: -3559.91). Current reward after update: -445.48, Optimal reward -349.16
Iteration 287 took 2.54 seconds (mean sampled reward: -3550.63). Current reward after update: -405.67, Optimal reward -349.16
Iteration 288 took 2.59 seconds (mean sampled reward: -3472.62). Current reward after update: -402.72, Optimal reward -349.16
Iteration 289 took 2.69 seconds (mean sampled reward: -3435.62). Current reward after update: -418.11, Optimal reward -349.16
Iteration 290 took 2.63 seconds (mean sampled reward: -3478.80). Current reward after update: -404.37, Optimal reward -349.16
Iteration 291 took 2.51 seconds (mean sampled reward: -3412.16). Current reward after update: -1391.19, Optimal reward -349.16
Iteration 292 took 2.44 seconds (mean sampled reward: -3521.14). Current reward after update: -423.86, Optimal reward -349.16
Iteration 293 took 2.34 seconds (mean sampled reward: -3491.84). Current reward after update: -406.80, Optimal reward -349.16
Iteration 294 took 2.61 seconds (mean sampled reward: -3365.84). Current reward after update: -446.00, Optimal reward -349.16
Iteration 295 took 2.59 seconds (mean sampled reward: -3420.73). Current reward after update: -418.91, Optimal reward -349.16
Iteration 296 took 2.32 seconds (mean sampled reward: -3516.81). Current reward after update: -370.52, Optimal reward -349.16
Iteration 297 took 2.47 seconds (mean sampled reward: -3440.16). Current reward after update: -414.27, Optimal reward -349.16
Iteration 298 took 2.48 seconds (mean sampled reward: -3294.88). Current reward after update: -553.66, Optimal reward -349.16
Iteration 299 took 2.56 seconds (mean sampled reward: -3215.94). Current reward after update: -396.00, Optimal reward -349.16
Iteration 300 took 2.45 seconds (mean sampled reward: -3147.45). Current reward after update: -567.19, Optimal reward -349.16
Iteration 1 took 3.02 seconds (mean sampled reward: -4412.61). Current reward after update: -1964.55, Optimal reward -1964.55
Iteration 2 took 2.94 seconds (mean sampled reward: -4314.07). Current reward after update: -1406.11, Optimal reward -1406.11
Iteration 3 took 2.89 seconds (mean sampled reward: -4278.22). Current reward after update: -1338.95, Optimal reward -1338.95
Iteration 4 took 2.78 seconds (mean sampled reward: -4333.89). Current reward after update: -1258.05, Optimal reward -1258.05
Iteration 5 took 2.98 seconds (mean sampled reward: -4244.68). Current reward after update: -1518.65, Optimal reward -1258.05
Iteration 6 took 3.05 seconds (mean sampled reward: -4285.79). Current reward after update: -1201.92, Optimal reward -1201.92
Iteration 7 took 3.07 seconds (mean sampled reward: -4276.46). Current reward after update: -1727.42, Optimal reward -1201.92
Iteration 8 took 2.90 seconds (mean sampled reward: -4304.64). Current reward after update: -1618.92, Optimal reward -1201.92
Iteration 9 took 2.92 seconds (mean sampled reward: -4339.40). Current reward after update: -1626.36, Optimal reward -1201.92
Iteration 10 took 2.93 seconds (mean sampled reward: -4262.89). Current reward after update: -1096.97, Optimal reward -1096.97
Iteration 11 took 2.98 seconds (mean sampled reward: -4175.47). Current reward after update: -1286.87, Optimal reward -1096.97
Iteration 12 took 2.85 seconds (mean sampled reward: -4209.15). Current reward after update: -1332.66, Optimal reward -1096.97
Iteration 13 took 2.86 seconds (mean sampled reward: -4256.79). Current reward after update: -1248.39, Optimal reward -1096.97
Iteration 14 took 2.96 seconds (mean sampled reward: -4244.97). Current reward after update: -1513.20, Optimal reward -1096.97
Iteration 15 took 2.83 seconds (mean sampled reward: -4194.87). Current reward after update: -1481.26, Optimal reward -1096.97
Iteration 16 took 2.77 seconds (mean sampled reward: -4174.31). Current reward after update: -1312.05, Optimal reward -1096.97
Iteration 17 took 2.72 seconds (mean sampled reward: -4210.06). Current reward after update: -1382.92, Optimal reward -1096.97
Iteration 18 took 3.00 seconds (mean sampled reward: -4220.21). Current reward after update: -1389.06, Optimal reward -1096.97
Iteration 19 took 2.72 seconds (mean sampled reward: -4151.62). Current reward after update: -1376.52, Optimal reward -1096.97
Iteration 20 took 2.77 seconds (mean sampled reward: -4235.09). Current reward after update: -1344.08, Optimal reward -1096.97
Iteration 21 took 2.68 seconds (mean sampled reward: -4272.58). Current reward after update: -1454.72, Optimal reward -1096.97
Iteration 22 took 2.82 seconds (mean sampled reward: -4250.24). Current reward after update: -1480.09, Optimal reward -1096.97
Iteration 23 took 3.08 seconds (mean sampled reward: -4237.51). Current reward after update: -1416.19, Optimal reward -1096.97
Iteration 24 took 2.91 seconds (mean sampled reward: -4293.60). Current reward after update: -1650.63, Optimal reward -1096.97
Iteration 25 took 2.93 seconds (mean sampled reward: -4040.37). Current reward after update: -1475.13, Optimal reward -1096.97
Iteration 26 took 2.97 seconds (mean sampled reward: -4037.61). Current reward after update: -1465.86, Optimal reward -1096.97
Iteration 27 took 2.86 seconds (mean sampled reward: -4081.77). Current reward after update: -1627.43, Optimal reward -1096.97
Iteration 28 took 2.77 seconds (mean sampled reward: -3928.08). Current reward after update: -1505.25, Optimal reward -1096.97
Iteration 29 took 2.94 seconds (mean sampled reward: -4058.77). Current reward after update: -1500.31, Optimal reward -1096.97
Iteration 30 took 2.92 seconds (mean sampled reward: -4042.45). Current reward after update: -1408.37, Optimal reward -1096.97
Iteration 31 took 3.03 seconds (mean sampled reward: -3942.74). Current reward after update: -1514.07, Optimal reward -1096.97
Iteration 32 took 2.67 seconds (mean sampled reward: -3743.11). Current reward after update: -1477.97, Optimal reward -1096.97
Iteration 33 took 2.94 seconds (mean sampled reward: -3773.21). Current reward after update: -1523.23, Optimal reward -1096.97
Iteration 34 took 2.77 seconds (mean sampled reward: -3773.61). Current reward after update: -1386.42, Optimal reward -1096.97
Iteration 35 took 2.69 seconds (mean sampled reward: -3629.14). Current reward after update: -1341.24, Optimal reward -1096.97
Iteration 36 took 2.84 seconds (mean sampled reward: -3728.38). Current reward after update: -1387.42, Optimal reward -1096.97
Iteration 37 took 2.78 seconds (mean sampled reward: -3775.27). Current reward after update: -1515.43, Optimal reward -1096.97
Iteration 38 took 2.75 seconds (mean sampled reward: -3683.06). Current reward after update: -1492.21, Optimal reward -1096.97
Iteration 39 took 2.68 seconds (mean sampled reward: -3696.83). Current reward after update: -1480.99, Optimal reward -1096.97
Iteration 40 took 2.89 seconds (mean sampled reward: -3660.17). Current reward after update: -1315.80, Optimal reward -1096.97
Iteration 41 took 2.82 seconds (mean sampled reward: -3569.24). Current reward after update: -1470.57, Optimal reward -1096.97
Iteration 42 took 2.72 seconds (mean sampled reward: -3809.89). Current reward after update: -1462.94, Optimal reward -1096.97
Iteration 43 took 2.69 seconds (mean sampled reward: -3771.09). Current reward after update: -1463.10, Optimal reward -1096.97
Iteration 44 took 2.74 seconds (mean sampled reward: -3834.77). Current reward after update: -1458.55, Optimal reward -1096.97
Iteration 45 took 2.68 seconds (mean sampled reward: -3690.68). Current reward after update: -1088.42, Optimal reward -1088.42
Iteration 46 took 2.80 seconds (mean sampled reward: -3618.16). Current reward after update: -1127.74, Optimal reward -1088.42
Iteration 47 took 2.85 seconds (mean sampled reward: -3329.99). Current reward after update: -1160.25, Optimal reward -1088.42
Iteration 48 took 2.76 seconds (mean sampled reward: -3417.40). Current reward after update: -1248.52, Optimal reward -1088.42
Iteration 49 took 2.71 seconds (mean sampled reward: -3282.55). Current reward after update: -1064.63, Optimal reward -1064.63
Iteration 50 took 2.72 seconds (mean sampled reward: -3496.18). Current reward after update: -1118.55, Optimal reward -1064.63
Iteration 51 took 2.73 seconds (mean sampled reward: -3443.46). Current reward after update: -1108.34, Optimal reward -1064.63
Iteration 52 took 3.00 seconds (mean sampled reward: -3517.87). Current reward after update: -1088.80, Optimal reward -1064.63
Iteration 53 took 2.76 seconds (mean sampled reward: -3706.48). Current reward after update: -1224.01, Optimal reward -1064.63
Iteration 54 took 2.73 seconds (mean sampled reward: -3500.78). Current reward after update: -1237.08, Optimal reward -1064.63
Iteration 55 took 2.73 seconds (mean sampled reward: -3299.85). Current reward after update: -1014.01, Optimal reward -1014.01
Iteration 56 took 2.68 seconds (mean sampled reward: -3168.62). Current reward after update: -1069.83, Optimal reward -1014.01
Iteration 57 took 2.73 seconds (mean sampled reward: -3480.30). Current reward after update: -1187.13, Optimal reward -1014.01
Iteration 58 took 2.66 seconds (mean sampled reward: -3342.00). Current reward after update: -1285.85, Optimal reward -1014.01
Iteration 59 took 2.86 seconds (mean sampled reward: -3311.92). Current reward after update: -1111.08, Optimal reward -1014.01
Iteration 60 took 2.84 seconds (mean sampled reward: -3332.02). Current reward after update: -1318.44, Optimal reward -1014.01
Iteration 61 took 2.68 seconds (mean sampled reward: -3676.66). Current reward after update: -1190.60, Optimal reward -1014.01
Iteration 62 took 2.71 seconds (mean sampled reward: -3622.96). Current reward after update: -1202.81, Optimal reward -1014.01
Iteration 63 took 2.92 seconds (mean sampled reward: -3672.17). Current reward after update: -1144.09, Optimal reward -1014.01
Iteration 64 took 2.85 seconds (mean sampled reward: -3626.62). Current reward after update: -1079.23, Optimal reward -1014.01
Iteration 65 took 2.81 seconds (mean sampled reward: -3678.17). Current reward after update: -1151.73, Optimal reward -1014.01
Iteration 66 took 2.71 seconds (mean sampled reward: -3806.45). Current reward after update: -1185.06, Optimal reward -1014.01
Iteration 67 took 3.06 seconds (mean sampled reward: -3647.73). Current reward after update: -1159.60, Optimal reward -1014.01
Iteration 68 took 2.93 seconds (mean sampled reward: -3620.10). Current reward after update: -1157.00, Optimal reward -1014.01
Iteration 69 took 2.72 seconds (mean sampled reward: -3434.32). Current reward after update: -995.28, Optimal reward -995.28
Iteration 70 took 2.89 seconds (mean sampled reward: -3563.88). Current reward after update: -1126.24, Optimal reward -995.28
Iteration 71 took 3.10 seconds (mean sampled reward: -3727.32). Current reward after update: -1797.65, Optimal reward -995.28
Iteration 72 took 3.09 seconds (mean sampled reward: -3745.65). Current reward after update: -1046.83, Optimal reward -995.28
Iteration 73 took 2.85 seconds (mean sampled reward: -3870.75). Current reward after update: -1285.40, Optimal reward -995.28
Iteration 74 took 2.75 seconds (mean sampled reward: -3806.97). Current reward after update: -1204.77, Optimal reward -995.28
Iteration 75 took 2.82 seconds (mean sampled reward: -3864.77). Current reward after update: -1094.42, Optimal reward -995.28
Iteration 76 took 2.70 seconds (mean sampled reward: -3773.73). Current reward after update: -1357.03, Optimal reward -995.28
Iteration 77 took 2.75 seconds (mean sampled reward: -3890.29). Current reward after update: -932.13, Optimal reward -932.13
Iteration 78 took 2.75 seconds (mean sampled reward: -3944.98). Current reward after update: -1274.74, Optimal reward -932.13
Iteration 79 took 2.80 seconds (mean sampled reward: -3698.95). Current reward after update: -1309.76, Optimal reward -932.13
Iteration 80 took 2.95 seconds (mean sampled reward: -3607.24). Current reward after update: -1197.80, Optimal reward -932.13
Iteration 81 took 2.80 seconds (mean sampled reward: -3834.52). Current reward after update: -1261.82, Optimal reward -932.13
Iteration 82 took 2.83 seconds (mean sampled reward: -3836.90). Current reward after update: -1256.55, Optimal reward -932.13
Iteration 83 took 2.75 seconds (mean sampled reward: -3994.85). Current reward after update: -1251.27, Optimal reward -932.13
Iteration 84 took 2.76 seconds (mean sampled reward: -3786.25). Current reward after update: -1097.52, Optimal reward -932.13
Iteration 85 took 3.02 seconds (mean sampled reward: -3538.22). Current reward after update: -1226.10, Optimal reward -932.13
Iteration 86 took 2.70 seconds (mean sampled reward: -3347.15). Current reward after update: -1279.75, Optimal reward -932.13
Iteration 87 took 2.79 seconds (mean sampled reward: -3717.50). Current reward after update: -1211.74, Optimal reward -932.13
Iteration 88 took 2.74 seconds (mean sampled reward: -3664.48). Current reward after update: -1311.63, Optimal reward -932.13
Iteration 89 took 2.65 seconds (mean sampled reward: -3468.75). Current reward after update: -1152.87, Optimal reward -932.13
Iteration 90 took 2.68 seconds (mean sampled reward: -3267.60). Current reward after update: -1200.26, Optimal reward -932.13
Iteration 91 took 2.70 seconds (mean sampled reward: -3418.51). Current reward after update: -1158.43, Optimal reward -932.13
Iteration 92 took 2.72 seconds (mean sampled reward: -3389.24). Current reward after update: -1132.40, Optimal reward -932.13
Iteration 93 took 2.77 seconds (mean sampled reward: -3395.17). Current reward after update: -1151.59, Optimal reward -932.13
Iteration 94 took 2.78 seconds (mean sampled reward: -3488.28). Current reward after update: -1195.09, Optimal reward -932.13
Iteration 95 took 2.72 seconds (mean sampled reward: -3324.41). Current reward after update: -1185.65, Optimal reward -932.13
Iteration 96 took 2.69 seconds (mean sampled reward: -3498.49). Current reward after update: -1026.48, Optimal reward -932.13
Iteration 97 took 2.81 seconds (mean sampled reward: -3607.19). Current reward after update: -965.08, Optimal reward -932.13
Iteration 98 took 2.85 seconds (mean sampled reward: -3692.15). Current reward after update: -1056.81, Optimal reward -932.13
Iteration 99 took 2.74 seconds (mean sampled reward: -3726.38). Current reward after update: -1022.22, Optimal reward -932.13
Iteration 100 took 2.88 seconds (mean sampled reward: -3892.30). Current reward after update: -1172.93, Optimal reward -932.13
Iteration 101 took 2.95 seconds (mean sampled reward: -3676.82). Current reward after update: -1078.53, Optimal reward -932.13
Iteration 102 took 2.79 seconds (mean sampled reward: -3763.75). Current reward after update: -986.70, Optimal reward -932.13
Iteration 103 took 2.79 seconds (mean sampled reward: -3604.87). Current reward after update: -1237.22, Optimal reward -932.13
Iteration 104 took 2.93 seconds (mean sampled reward: -3697.65). Current reward after update: -1225.19, Optimal reward -932.13
Iteration 105 took 2.97 seconds (mean sampled reward: -3529.23). Current reward after update: -1252.73, Optimal reward -932.13
Iteration 106 took 2.90 seconds (mean sampled reward: -3750.34). Current reward after update: -1172.23, Optimal reward -932.13
Iteration 107 took 3.00 seconds (mean sampled reward: -3695.24). Current reward after update: -1031.26, Optimal reward -932.13
Iteration 108 took 3.03 seconds (mean sampled reward: -3662.69). Current reward after update: -1075.33, Optimal reward -932.13
Iteration 109 took 3.23 seconds (mean sampled reward: -3668.86). Current reward after update: -1082.70, Optimal reward -932.13
Iteration 110 took 2.74 seconds (mean sampled reward: -3701.64). Current reward after update: -1032.96, Optimal reward -932.13
Iteration 111 took 2.93 seconds (mean sampled reward: -3673.77). Current reward after update: -1001.49, Optimal reward -932.13
Iteration 112 took 3.09 seconds (mean sampled reward: -3823.30). Current reward after update: -1208.75, Optimal reward -932.13
Iteration 113 took 3.11 seconds (mean sampled reward: -3780.13). Current reward after update: -1149.70, Optimal reward -932.13
Iteration 114 took 2.95 seconds (mean sampled reward: -3671.31). Current reward after update: -1233.58, Optimal reward -932.13
Iteration 115 took 2.83 seconds (mean sampled reward: -3814.06). Current reward after update: -1033.52, Optimal reward -932.13
Iteration 116 took 2.96 seconds (mean sampled reward: -3899.10). Current reward after update: -1206.90, Optimal reward -932.13
Iteration 117 took 2.70 seconds (mean sampled reward: -3603.40). Current reward after update: -1166.86, Optimal reward -932.13
Iteration 118 took 2.70 seconds (mean sampled reward: -3654.11). Current reward after update: -1100.44, Optimal reward -932.13
Iteration 119 took 2.98 seconds (mean sampled reward: -3599.21). Current reward after update: -1526.21, Optimal reward -932.13
Iteration 120 took 2.93 seconds (mean sampled reward: -3565.37). Current reward after update: -1056.73, Optimal reward -932.13
Iteration 121 took 2.97 seconds (mean sampled reward: -3736.51). Current reward after update: -1178.36, Optimal reward -932.13
Iteration 122 took 2.88 seconds (mean sampled reward: -3789.80). Current reward after update: -1183.57, Optimal reward -932.13
Iteration 123 took 2.90 seconds (mean sampled reward: -3831.99). Current reward after update: -1286.93, Optimal reward -932.13
Iteration 124 took 2.86 seconds (mean sampled reward: -3832.30). Current reward after update: -1524.60, Optimal reward -932.13
Iteration 125 took 2.79 seconds (mean sampled reward: -3890.99). Current reward after update: -1426.56, Optimal reward -932.13
Iteration 126 took 2.98 seconds (mean sampled reward: -3946.34). Current reward after update: -1458.70, Optimal reward -932.13
Iteration 127 took 2.98 seconds (mean sampled reward: -4021.41). Current reward after update: -1254.75, Optimal reward -932.13
Iteration 128 took 2.94 seconds (mean sampled reward: -4075.58). Current reward after update: -1252.00, Optimal reward -932.13
Iteration 129 took 2.81 seconds (mean sampled reward: -4095.72). Current reward after update: -1443.23, Optimal reward -932.13
Iteration 130 took 2.75 seconds (mean sampled reward: -4145.08). Current reward after update: -1295.53, Optimal reward -932.13
Iteration 131 took 2.92 seconds (mean sampled reward: -4212.85). Current reward after update: -1216.08, Optimal reward -932.13
Iteration 132 took 2.95 seconds (mean sampled reward: -3985.63). Current reward after update: -1308.09, Optimal reward -932.13
Iteration 133 took 2.77 seconds (mean sampled reward: -4055.62). Current reward after update: -1155.69, Optimal reward -932.13
Iteration 134 took 3.09 seconds (mean sampled reward: -3934.79). Current reward after update: -1136.86, Optimal reward -932.13
Iteration 135 took 2.93 seconds (mean sampled reward: -3839.58). Current reward after update: -1319.32, Optimal reward -932.13
Iteration 136 took 2.84 seconds (mean sampled reward: -3844.17). Current reward after update: -1266.87, Optimal reward -932.13
Iteration 137 took 2.86 seconds (mean sampled reward: -3877.15). Current reward after update: -1193.47, Optimal reward -932.13
Iteration 138 took 2.90 seconds (mean sampled reward: -3880.38). Current reward after update: -1179.84, Optimal reward -932.13
Iteration 139 took 2.75 seconds (mean sampled reward: -3793.48). Current reward after update: -1242.64, Optimal reward -932.13
Iteration 140 took 2.98 seconds (mean sampled reward: -3960.66). Current reward after update: -1091.43, Optimal reward -932.13
Iteration 141 took 2.82 seconds (mean sampled reward: -3779.52). Current reward after update: -1124.00, Optimal reward -932.13
Iteration 142 took 2.94 seconds (mean sampled reward: -3959.97). Current reward after update: -1378.45, Optimal reward -932.13
Iteration 143 took 3.13 seconds (mean sampled reward: -4000.48). Current reward after update: -1255.87, Optimal reward -932.13
Iteration 144 took 2.98 seconds (mean sampled reward: -3795.87). Current reward after update: -1790.46, Optimal reward -932.13
Iteration 145 took 2.83 seconds (mean sampled reward: -3904.04). Current reward after update: -1188.54, Optimal reward -932.13
Iteration 146 took 2.88 seconds (mean sampled reward: -3767.36). Current reward after update: -1195.12, Optimal reward -932.13
Iteration 147 took 2.79 seconds (mean sampled reward: -3783.57). Current reward after update: -1168.31, Optimal reward -932.13
Iteration 148 took 2.75 seconds (mean sampled reward: -3772.30). Current reward after update: -1257.82, Optimal reward -932.13
Iteration 149 took 2.78 seconds (mean sampled reward: -3743.22). Current reward after update: -1470.76, Optimal reward -932.13
Iteration 150 took 2.91 seconds (mean sampled reward: -3645.09). Current reward after update: -1405.14, Optimal reward -932.13
Iteration 151 took 2.83 seconds (mean sampled reward: -3446.08). Current reward after update: -1169.46, Optimal reward -932.13
Iteration 152 took 2.75 seconds (mean sampled reward: -3569.42). Current reward after update: -1389.06, Optimal reward -932.13
Iteration 153 took 3.06 seconds (mean sampled reward: -3508.54). Current reward after update: -1192.56, Optimal reward -932.13
Iteration 154 took 3.03 seconds (mean sampled reward: -3685.69). Current reward after update: -1471.88, Optimal reward -932.13
Iteration 155 took 2.94 seconds (mean sampled reward: -3867.85). Current reward after update: -1193.26, Optimal reward -932.13
Iteration 156 took 3.13 seconds (mean sampled reward: -3890.11). Current reward after update: -1212.93, Optimal reward -932.13
Iteration 157 took 3.10 seconds (mean sampled reward: -3865.98). Current reward after update: -1368.93, Optimal reward -932.13
Iteration 158 took 2.95 seconds (mean sampled reward: -3905.91). Current reward after update: -1313.15, Optimal reward -932.13
Iteration 159 took 2.72 seconds (mean sampled reward: -4009.33). Current reward after update: -1211.73, Optimal reward -932.13
Iteration 160 took 2.98 seconds (mean sampled reward: -3921.25). Current reward after update: -1356.36, Optimal reward -932.13
Iteration 161 took 2.92 seconds (mean sampled reward: -3931.50). Current reward after update: -1325.83, Optimal reward -932.13
Iteration 162 took 2.88 seconds (mean sampled reward: -3967.57). Current reward after update: -1400.80, Optimal reward -932.13
Iteration 163 took 3.06 seconds (mean sampled reward: -4003.50). Current reward after update: -1378.86, Optimal reward -932.13
Iteration 164 took 2.75 seconds (mean sampled reward: -4144.55). Current reward after update: -1404.56, Optimal reward -932.13
Iteration 165 took 2.73 seconds (mean sampled reward: -3979.11). Current reward after update: -1318.01, Optimal reward -932.13
Iteration 166 took 2.88 seconds (mean sampled reward: -3980.56). Current reward after update: -1162.56, Optimal reward -932.13
Iteration 167 took 2.77 seconds (mean sampled reward: -3886.70). Current reward after update: -1271.21, Optimal reward -932.13
Iteration 168 took 2.88 seconds (mean sampled reward: -3956.51). Current reward after update: -1420.19, Optimal reward -932.13
Iteration 169 took 2.70 seconds (mean sampled reward: -4021.67). Current reward after update: -979.20, Optimal reward -932.13
Iteration 170 took 2.87 seconds (mean sampled reward: -3884.50). Current reward after update: -1379.46, Optimal reward -932.13
Iteration 171 took 2.69 seconds (mean sampled reward: -3947.74). Current reward after update: -1313.65, Optimal reward -932.13
Iteration 172 took 3.22 seconds (mean sampled reward: -3914.76). Current reward after update: -1244.68, Optimal reward -932.13
Iteration 173 took 2.78 seconds (mean sampled reward: -3832.58). Current reward after update: -1386.96, Optimal reward -932.13
Iteration 174 took 2.95 seconds (mean sampled reward: -3887.17). Current reward after update: -1435.57, Optimal reward -932.13
Iteration 175 took 2.80 seconds (mean sampled reward: -3694.80). Current reward after update: -1328.42, Optimal reward -932.13
Iteration 176 took 2.72 seconds (mean sampled reward: -3853.37). Current reward after update: -1325.53, Optimal reward -932.13
Iteration 177 took 2.93 seconds (mean sampled reward: -3753.86). Current reward after update: -1293.71, Optimal reward -932.13
Iteration 178 took 2.91 seconds (mean sampled reward: -3652.86). Current reward after update: -1079.83, Optimal reward -932.13
Iteration 179 took 2.78 seconds (mean sampled reward: -3596.66). Current reward after update: -1136.24, Optimal reward -932.13
Iteration 180 took 2.86 seconds (mean sampled reward: -3694.01). Current reward after update: -1146.36, Optimal reward -932.13
Iteration 181 took 2.87 seconds (mean sampled reward: -3770.40). Current reward after update: -1273.30, Optimal reward -932.13
Iteration 182 took 3.02 seconds (mean sampled reward: -3753.59). Current reward after update: -1137.12, Optimal reward -932.13
Iteration 183 took 2.86 seconds (mean sampled reward: -3843.37). Current reward after update: -1252.35, Optimal reward -932.13
Iteration 184 took 2.95 seconds (mean sampled reward: -3772.00). Current reward after update: -1035.27, Optimal reward -932.13
Iteration 185 took 2.75 seconds (mean sampled reward: -3759.63). Current reward after update: -1097.88, Optimal reward -932.13
Iteration 186 took 2.89 seconds (mean sampled reward: -3764.19). Current reward after update: -1233.47, Optimal reward -932.13
Iteration 187 took 2.73 seconds (mean sampled reward: -3919.58). Current reward after update: -1281.89, Optimal reward -932.13
Iteration 188 took 2.78 seconds (mean sampled reward: -3724.34). Current reward after update: -1196.72, Optimal reward -932.13
Iteration 189 took 2.82 seconds (mean sampled reward: -3751.96). Current reward after update: -945.61, Optimal reward -932.13
Iteration 190 took 2.98 seconds (mean sampled reward: -3826.07). Current reward after update: -991.01, Optimal reward -932.13
Iteration 191 took 2.98 seconds (mean sampled reward: -3869.39). Current reward after update: -1335.64, Optimal reward -932.13
Iteration 192 took 2.77 seconds (mean sampled reward: -3972.19). Current reward after update: -1235.99, Optimal reward -932.13
Iteration 193 took 2.83 seconds (mean sampled reward: -3866.51). Current reward after update: -1144.19, Optimal reward -932.13
Iteration 194 took 2.99 seconds (mean sampled reward: -3834.21). Current reward after update: -1165.10, Optimal reward -932.13
Iteration 195 took 2.74 seconds (mean sampled reward: -3729.54). Current reward after update: -964.63, Optimal reward -932.13
Iteration 196 took 2.95 seconds (mean sampled reward: -3826.94). Current reward after update: -1129.71, Optimal reward -932.13
Iteration 197 took 2.80 seconds (mean sampled reward: -3817.03). Current reward after update: -1104.68, Optimal reward -932.13
Iteration 198 took 2.80 seconds (mean sampled reward: -3739.66). Current reward after update: -1873.98, Optimal reward -932.13
Iteration 199 took 2.86 seconds (mean sampled reward: -3699.23). Current reward after update: -1175.37, Optimal reward -932.13
Iteration 200 took 2.72 seconds (mean sampled reward: -3747.18). Current reward after update: -1179.13, Optimal reward -932.13
Iteration 201 took 3.02 seconds (mean sampled reward: -3870.42). Current reward after update: -1259.79, Optimal reward -932.13
Iteration 202 took 2.93 seconds (mean sampled reward: -3866.99). Current reward after update: -1189.91, Optimal reward -932.13
Iteration 203 took 2.70 seconds (mean sampled reward: -3786.78). Current reward after update: -1142.71, Optimal reward -932.13
Iteration 204 took 2.75 seconds (mean sampled reward: -3737.12). Current reward after update: -1385.90, Optimal reward -932.13
Iteration 205 took 2.95 seconds (mean sampled reward: -3772.09). Current reward after update: -1192.72, Optimal reward -932.13
Iteration 206 took 2.87 seconds (mean sampled reward: -3839.75). Current reward after update: -1178.70, Optimal reward -932.13
Iteration 207 took 2.86 seconds (mean sampled reward: -3759.56). Current reward after update: -1258.63, Optimal reward -932.13
Iteration 208 took 3.01 seconds (mean sampled reward: -3828.04). Current reward after update: -1116.63, Optimal reward -932.13
Iteration 209 took 2.73 seconds (mean sampled reward: -3877.72). Current reward after update: -1204.06, Optimal reward -932.13
Iteration 210 took 3.13 seconds (mean sampled reward: -3812.00). Current reward after update: -1236.78, Optimal reward -932.13
Iteration 211 took 3.06 seconds (mean sampled reward: -3722.77). Current reward after update: -1186.78, Optimal reward -932.13
Iteration 212 took 2.92 seconds (mean sampled reward: -3762.43). Current reward after update: -1224.24, Optimal reward -932.13
Iteration 213 took 3.13 seconds (mean sampled reward: -3699.14). Current reward after update: -1182.20, Optimal reward -932.13
Iteration 214 took 2.86 seconds (mean sampled reward: -3708.73). Current reward after update: -1184.56, Optimal reward -932.13
Iteration 215 took 3.01 seconds (mean sampled reward: -3787.29). Current reward after update: -1298.64, Optimal reward -932.13
Iteration 216 took 2.90 seconds (mean sampled reward: -3540.56). Current reward after update: -1220.86, Optimal reward -932.13
Iteration 217 took 2.99 seconds (mean sampled reward: -3858.50). Current reward after update: -1247.82, Optimal reward -932.13
Iteration 218 took 2.79 seconds (mean sampled reward: -3842.42). Current reward after update: -1215.29, Optimal reward -932.13
Iteration 219 took 2.74 seconds (mean sampled reward: -4086.85). Current reward after update: -1320.02, Optimal reward -932.13
Iteration 220 took 2.93 seconds (mean sampled reward: -3920.35). Current reward after update: -1282.26, Optimal reward -932.13
Iteration 221 took 2.75 seconds (mean sampled reward: -3744.27). Current reward after update: -1126.41, Optimal reward -932.13
Iteration 222 took 2.91 seconds (mean sampled reward: -3857.65). Current reward after update: -1165.30, Optimal reward -932.13
Iteration 223 took 3.08 seconds (mean sampled reward: -3870.66). Current reward after update: -1223.60, Optimal reward -932.13
Iteration 224 took 2.84 seconds (mean sampled reward: -3830.96). Current reward after update: -1216.80, Optimal reward -932.13
Iteration 225 took 3.09 seconds (mean sampled reward: -3708.65). Current reward after update: -1219.36, Optimal reward -932.13
Iteration 226 took 2.99 seconds (mean sampled reward: -3895.85). Current reward after update: -1251.91, Optimal reward -932.13
Iteration 227 took 2.94 seconds (mean sampled reward: -3792.90). Current reward after update: -1199.11, Optimal reward -932.13
Iteration 228 took 2.70 seconds (mean sampled reward: -3796.47). Current reward after update: -1165.76, Optimal reward -932.13
Iteration 229 took 2.86 seconds (mean sampled reward: -3878.44). Current reward after update: -1147.10, Optimal reward -932.13
Iteration 230 took 2.89 seconds (mean sampled reward: -3848.80). Current reward after update: -1137.21, Optimal reward -932.13
Iteration 231 took 2.80 seconds (mean sampled reward: -3801.29). Current reward after update: -1148.80, Optimal reward -932.13
Iteration 232 took 2.83 seconds (mean sampled reward: -3735.93). Current reward after update: -1163.09, Optimal reward -932.13
Iteration 233 took 2.98 seconds (mean sampled reward: -3762.08). Current reward after update: -1153.08, Optimal reward -932.13
Iteration 234 took 2.85 seconds (mean sampled reward: -3670.37). Current reward after update: -1132.53, Optimal reward -932.13
Iteration 235 took 2.76 seconds (mean sampled reward: -3714.41). Current reward after update: -1190.58, Optimal reward -932.13
Iteration 236 took 3.00 seconds (mean sampled reward: -3623.87). Current reward after update: -1181.66, Optimal reward -932.13
Iteration 237 took 2.86 seconds (mean sampled reward: -3701.01). Current reward after update: -1247.98, Optimal reward -932.13
Iteration 238 took 2.96 seconds (mean sampled reward: -3779.78). Current reward after update: -1255.41, Optimal reward -932.13
Iteration 239 took 2.82 seconds (mean sampled reward: -3786.79). Current reward after update: -1171.11, Optimal reward -932.13
Iteration 240 took 2.93 seconds (mean sampled reward: -3751.69). Current reward after update: -1125.00, Optimal reward -932.13
Iteration 241 took 2.74 seconds (mean sampled reward: -3831.93). Current reward after update: -1273.49, Optimal reward -932.13
Iteration 242 took 3.03 seconds (mean sampled reward: -3874.21). Current reward after update: -1187.77, Optimal reward -932.13
Iteration 243 took 3.15 seconds (mean sampled reward: -3717.18). Current reward after update: -1186.65, Optimal reward -932.13
Iteration 244 took 3.19 seconds (mean sampled reward: -3929.34). Current reward after update: -1225.13, Optimal reward -932.13
Iteration 245 took 2.77 seconds (mean sampled reward: -3864.54). Current reward after update: -1377.31, Optimal reward -932.13
Iteration 246 took 3.12 seconds (mean sampled reward: -3880.31). Current reward after update: -1529.05, Optimal reward -932.13
Iteration 247 took 2.87 seconds (mean sampled reward: -3795.13). Current reward after update: -1186.30, Optimal reward -932.13
Iteration 248 took 2.91 seconds (mean sampled reward: -3939.16). Current reward after update: -1159.11, Optimal reward -932.13
Iteration 249 took 3.12 seconds (mean sampled reward: -4023.08). Current reward after update: -1436.25, Optimal reward -932.13
Iteration 250 took 2.98 seconds (mean sampled reward: -4039.97). Current reward after update: -1166.26, Optimal reward -932.13
Iteration 251 took 3.07 seconds (mean sampled reward: -3965.05). Current reward after update: -1164.94, Optimal reward -932.13
Iteration 252 took 2.96 seconds (mean sampled reward: -4003.87). Current reward after update: -1276.44, Optimal reward -932.13
Iteration 253 took 2.83 seconds (mean sampled reward: -4008.75). Current reward after update: -1208.56, Optimal reward -932.13
Iteration 254 took 2.84 seconds (mean sampled reward: -3966.65). Current reward after update: -1435.73, Optimal reward -932.13
Iteration 255 took 2.99 seconds (mean sampled reward: -3952.48). Current reward after update: -1539.80, Optimal reward -932.13
Iteration 256 took 2.97 seconds (mean sampled reward: -3936.98). Current reward after update: -1211.17, Optimal reward -932.13
Iteration 257 took 3.05 seconds (mean sampled reward: -3984.78). Current reward after update: -1239.49, Optimal reward -932.13
Iteration 258 took 2.85 seconds (mean sampled reward: -3911.03). Current reward after update: -1279.25, Optimal reward -932.13
Iteration 259 took 2.86 seconds (mean sampled reward: -4000.29). Current reward after update: -1198.48, Optimal reward -932.13
Iteration 260 took 2.93 seconds (mean sampled reward: -3939.94). Current reward after update: -1388.64, Optimal reward -932.13
Iteration 261 took 3.07 seconds (mean sampled reward: -3939.18). Current reward after update: -1124.64, Optimal reward -932.13
Iteration 262 took 3.12 seconds (mean sampled reward: -3942.00). Current reward after update: -1298.32, Optimal reward -932.13
Iteration 263 took 3.00 seconds (mean sampled reward: -3964.01). Current reward after update: -1323.51, Optimal reward -932.13
Iteration 264 took 2.96 seconds (mean sampled reward: -4039.64). Current reward after update: -1499.82, Optimal reward -932.13
Iteration 265 took 2.94 seconds (mean sampled reward: -3792.29). Current reward after update: -1262.83, Optimal reward -932.13
Iteration 266 took 2.93 seconds (mean sampled reward: -3904.05). Current reward after update: -1166.20, Optimal reward -932.13
Iteration 267 took 2.91 seconds (mean sampled reward: -3925.54). Current reward after update: -1193.14, Optimal reward -932.13
Iteration 268 took 2.78 seconds (mean sampled reward: -3981.26). Current reward after update: -1475.13, Optimal reward -932.13
Iteration 269 took 2.86 seconds (mean sampled reward: -3915.85). Current reward after update: -1396.47, Optimal reward -932.13
Iteration 270 took 2.95 seconds (mean sampled reward: -3915.26). Current reward after update: -1341.42, Optimal reward -932.13
Iteration 271 took 2.87 seconds (mean sampled reward: -3924.39). Current reward after update: -1584.11, Optimal reward -932.13
Iteration 272 took 3.00 seconds (mean sampled reward: -4090.56). Current reward after update: -1640.81, Optimal reward -932.13
Iteration 273 took 2.82 seconds (mean sampled reward: -4248.57). Current reward after update: -1694.75, Optimal reward -932.13
Iteration 274 took 2.77 seconds (mean sampled reward: -4156.40). Current reward after update: -1579.14, Optimal reward -932.13
Iteration 275 took 2.90 seconds (mean sampled reward: -4062.10). Current reward after update: -1514.88, Optimal reward -932.13
Iteration 276 took 3.05 seconds (mean sampled reward: -4184.25). Current reward after update: -1371.63, Optimal reward -932.13
Iteration 277 took 3.01 seconds (mean sampled reward: -3974.23). Current reward after update: -1557.47, Optimal reward -932.13
Iteration 278 took 2.98 seconds (mean sampled reward: -4146.50). Current reward after update: -1546.13, Optimal reward -932.13
Iteration 279 took 2.86 seconds (mean sampled reward: -4137.15). Current reward after update: -1510.59, Optimal reward -932.13
Iteration 280 took 2.91 seconds (mean sampled reward: -4196.60). Current reward after update: -2133.06, Optimal reward -932.13
Iteration 281 took 2.77 seconds (mean sampled reward: -4052.94). Current reward after update: -1801.58, Optimal reward -932.13
Iteration 282 took 2.86 seconds (mean sampled reward: -3947.85). Current reward after update: -1742.50, Optimal reward -932.13
Iteration 283 took 2.89 seconds (mean sampled reward: -3840.99). Current reward after update: -1301.93, Optimal reward -932.13
Iteration 284 took 3.04 seconds (mean sampled reward: -3688.89). Current reward after update: -1108.20, Optimal reward -932.13
Iteration 285 took 2.98 seconds (mean sampled reward: -3547.87). Current reward after update: -1175.79, Optimal reward -932.13
Iteration 286 took 3.10 seconds (mean sampled reward: -3784.51). Current reward after update: -1182.83, Optimal reward -932.13
Iteration 287 took 2.82 seconds (mean sampled reward: -3820.07). Current reward after update: -1351.43, Optimal reward -932.13
Iteration 288 took 3.16 seconds (mean sampled reward: -3985.12). Current reward after update: -1268.65, Optimal reward -932.13
Iteration 289 took 2.88 seconds (mean sampled reward: -4038.27). Current reward after update: -1312.97, Optimal reward -932.13
Iteration 290 took 2.77 seconds (mean sampled reward: -3962.83). Current reward after update: -1543.49, Optimal reward -932.13
Iteration 291 took 2.85 seconds (mean sampled reward: -4022.31). Current reward after update: -1427.64, Optimal reward -932.13
Iteration 292 took 3.10 seconds (mean sampled reward: -4157.20). Current reward after update: -1706.20, Optimal reward -932.13
Iteration 293 took 2.86 seconds (mean sampled reward: -3984.42). Current reward after update: -1435.45, Optimal reward -932.13
Iteration 294 took 3.01 seconds (mean sampled reward: -3950.99). Current reward after update: -1456.64, Optimal reward -932.13
Iteration 295 took 2.84 seconds (mean sampled reward: -3948.59). Current reward after update: -1603.10, Optimal reward -932.13
Iteration 296 took 3.07 seconds (mean sampled reward: -3934.75). Current reward after update: -1326.73, Optimal reward -932.13
Iteration 297 took 3.09 seconds (mean sampled reward: -3927.04). Current reward after update: -1664.14, Optimal reward -932.13
Iteration 298 took 2.82 seconds (mean sampled reward: -3939.47). Current reward after update: -1419.04, Optimal reward -932.13
Iteration 299 took 2.95 seconds (mean sampled reward: -3979.42). Current reward after update: -1518.84, Optimal reward -932.13
Iteration 300 took 2.90 seconds (mean sampled reward: -3904.32). Current reward after update: -1435.40, Optimal reward -932.13
Iteration 1 took 2.96 seconds (mean sampled reward: -4432.04). Current reward after update: -1884.87, Optimal reward -1884.87
Iteration 2 took 2.84 seconds (mean sampled reward: -4327.21). Current reward after update: -1440.47, Optimal reward -1440.47
Iteration 3 took 2.82 seconds (mean sampled reward: -4374.89). Current reward after update: -1651.52, Optimal reward -1440.47
Iteration 4 took 2.98 seconds (mean sampled reward: -4177.13). Current reward after update: -1449.13, Optimal reward -1440.47
Iteration 5 took 2.77 seconds (mean sampled reward: -4230.71). Current reward after update: -1349.07, Optimal reward -1349.07
Iteration 6 took 2.86 seconds (mean sampled reward: -4184.41). Current reward after update: -1214.22, Optimal reward -1214.22
Iteration 7 took 2.76 seconds (mean sampled reward: -4178.55). Current reward after update: -1219.45, Optimal reward -1214.22
Iteration 8 took 2.96 seconds (mean sampled reward: -4114.08). Current reward after update: -1107.71, Optimal reward -1107.71
Iteration 9 took 2.89 seconds (mean sampled reward: -4202.80). Current reward after update: -880.70, Optimal reward -880.70
Iteration 10 took 2.83 seconds (mean sampled reward: -4211.97). Current reward after update: -1076.07, Optimal reward -880.70
Iteration 11 took 2.78 seconds (mean sampled reward: -4158.18). Current reward after update: -1390.26, Optimal reward -880.70
Iteration 12 took 3.03 seconds (mean sampled reward: -4246.74). Current reward after update: -1234.86, Optimal reward -880.70
Iteration 13 took 2.85 seconds (mean sampled reward: -4227.25). Current reward after update: -1163.61, Optimal reward -880.70
Iteration 14 took 2.77 seconds (mean sampled reward: -4028.79). Current reward after update: -1026.40, Optimal reward -880.70
Iteration 15 took 2.76 seconds (mean sampled reward: -3935.36). Current reward after update: -1045.47, Optimal reward -880.70
Iteration 16 took 2.78 seconds (mean sampled reward: -4078.59). Current reward after update: -882.71, Optimal reward -880.70
Iteration 17 took 2.72 seconds (mean sampled reward: -4064.92). Current reward after update: -367.24, Optimal reward -367.24
Iteration 18 took 2.94 seconds (mean sampled reward: -4029.79). Current reward after update: -376.76, Optimal reward -367.24
Iteration 19 took 2.89 seconds (mean sampled reward: -3903.30). Current reward after update: -272.88, Optimal reward -272.88
Iteration 20 took 2.94 seconds (mean sampled reward: -3621.21). Current reward after update: -890.90, Optimal reward -272.88
Iteration 21 took 3.06 seconds (mean sampled reward: -3483.83). Current reward after update: -894.13, Optimal reward -272.88
Iteration 22 took 2.89 seconds (mean sampled reward: -3259.08). Current reward after update: -1141.01, Optimal reward -272.88
Iteration 23 took 2.91 seconds (mean sampled reward: -3073.24). Current reward after update: -479.51, Optimal reward -272.88
Iteration 24 took 2.70 seconds (mean sampled reward: -3247.59). Current reward after update: -522.28, Optimal reward -272.88
Iteration 25 took 2.90 seconds (mean sampled reward: -3248.12). Current reward after update: -305.59, Optimal reward -272.88
Iteration 26 took 2.76 seconds (mean sampled reward: -3579.66). Current reward after update: -907.36, Optimal reward -272.88
Iteration 27 took 2.58 seconds (mean sampled reward: -3381.94). Current reward after update: -251.99, Optimal reward -251.99
Iteration 28 took 2.52 seconds (mean sampled reward: -3361.79). Current reward after update: -542.15, Optimal reward -251.99
Iteration 29 took 2.60 seconds (mean sampled reward: -3255.14). Current reward after update: -478.29, Optimal reward -251.99
Iteration 30 took 2.94 seconds (mean sampled reward: -3259.09). Current reward after update: -386.21, Optimal reward -251.99
Iteration 31 took 2.72 seconds (mean sampled reward: -3215.68). Current reward after update: -411.86, Optimal reward -251.99
Iteration 32 took 2.81 seconds (mean sampled reward: -3303.70). Current reward after update: -470.81, Optimal reward -251.99
Iteration 33 took 2.76 seconds (mean sampled reward: -3558.79). Current reward after update: -807.18, Optimal reward -251.99
Iteration 34 took 3.01 seconds (mean sampled reward: -3612.38). Current reward after update: -770.89, Optimal reward -251.99
Iteration 35 took 2.80 seconds (mean sampled reward: -3752.27). Current reward after update: -956.02, Optimal reward -251.99
Iteration 36 took 2.72 seconds (mean sampled reward: -3451.13). Current reward after update: -1175.10, Optimal reward -251.99
Iteration 37 took 2.89 seconds (mean sampled reward: -3519.14). Current reward after update: -1126.82, Optimal reward -251.99
Iteration 38 took 2.89 seconds (mean sampled reward: -3453.47). Current reward after update: -1276.17, Optimal reward -251.99
Iteration 39 took 2.66 seconds (mean sampled reward: -3370.58). Current reward after update: -1149.70, Optimal reward -251.99
Iteration 40 took 2.91 seconds (mean sampled reward: -3475.23). Current reward after update: -712.07, Optimal reward -251.99
Iteration 41 took 2.81 seconds (mean sampled reward: -3420.66). Current reward after update: -1130.24, Optimal reward -251.99
Iteration 42 took 3.02 seconds (mean sampled reward: -3475.87). Current reward after update: -1202.87, Optimal reward -251.99
Iteration 43 took 2.71 seconds (mean sampled reward: -3664.87). Current reward after update: -1066.42, Optimal reward -251.99
Iteration 44 took 2.81 seconds (mean sampled reward: -3683.28). Current reward after update: -763.61, Optimal reward -251.99
Iteration 45 took 2.72 seconds (mean sampled reward: -3537.55). Current reward after update: -1094.99, Optimal reward -251.99
Iteration 46 took 2.74 seconds (mean sampled reward: -3368.06). Current reward after update: -1250.92, Optimal reward -251.99
Iteration 47 took 2.62 seconds (mean sampled reward: -3541.27). Current reward after update: -930.82, Optimal reward -251.99
Iteration 48 took 2.79 seconds (mean sampled reward: -3584.03). Current reward after update: -1048.14, Optimal reward -251.99
Iteration 49 took 2.75 seconds (mean sampled reward: -3498.83). Current reward after update: -918.06, Optimal reward -251.99
Iteration 50 took 2.92 seconds (mean sampled reward: -3398.04). Current reward after update: -853.88, Optimal reward -251.99
Iteration 51 took 2.63 seconds (mean sampled reward: -3521.95). Current reward after update: -946.31, Optimal reward -251.99
Iteration 52 took 2.76 seconds (mean sampled reward: -3478.95). Current reward after update: -712.15, Optimal reward -251.99
Iteration 53 took 2.92 seconds (mean sampled reward: -3542.45). Current reward after update: -756.42, Optimal reward -251.99
Iteration 54 took 2.66 seconds (mean sampled reward: -3708.36). Current reward after update: -796.51, Optimal reward -251.99
Iteration 55 took 2.66 seconds (mean sampled reward: -3409.13). Current reward after update: -987.55, Optimal reward -251.99
Iteration 56 took 2.67 seconds (mean sampled reward: -3457.21). Current reward after update: -754.43, Optimal reward -251.99
Iteration 57 took 2.64 seconds (mean sampled reward: -3307.27). Current reward after update: -679.31, Optimal reward -251.99
Iteration 58 took 2.73 seconds (mean sampled reward: -3425.79). Current reward after update: -768.54, Optimal reward -251.99
Iteration 59 took 2.67 seconds (mean sampled reward: -3334.83). Current reward after update: -818.18, Optimal reward -251.99
Iteration 60 took 2.63 seconds (mean sampled reward: -3474.42). Current reward after update: -756.12, Optimal reward -251.99
Iteration 61 took 2.63 seconds (mean sampled reward: -3505.86). Current reward after update: -677.96, Optimal reward -251.99
Iteration 62 took 2.67 seconds (mean sampled reward: -3507.97). Current reward after update: -716.74, Optimal reward -251.99
Iteration 63 took 2.75 seconds (mean sampled reward: -3416.49). Current reward after update: -716.01, Optimal reward -251.99
Iteration 64 took 2.69 seconds (mean sampled reward: -3227.47). Current reward after update: -797.07, Optimal reward -251.99
Iteration 65 took 2.71 seconds (mean sampled reward: -3236.28). Current reward after update: -654.85, Optimal reward -251.99
Iteration 66 took 2.84 seconds (mean sampled reward: -3210.59). Current reward after update: -612.96, Optimal reward -251.99
Iteration 67 took 2.88 seconds (mean sampled reward: -3262.21). Current reward after update: -585.07, Optimal reward -251.99
Iteration 68 took 2.92 seconds (mean sampled reward: -3171.31). Current reward after update: -643.13, Optimal reward -251.99
Iteration 69 took 2.77 seconds (mean sampled reward: -3179.37). Current reward after update: -666.23, Optimal reward -251.99
Iteration 70 took 2.77 seconds (mean sampled reward: -3222.48). Current reward after update: -571.94, Optimal reward -251.99
Iteration 71 took 2.70 seconds (mean sampled reward: -3210.47). Current reward after update: -512.94, Optimal reward -251.99
Iteration 72 took 2.71 seconds (mean sampled reward: -3179.03). Current reward after update: -612.25, Optimal reward -251.99
Iteration 73 took 2.70 seconds (mean sampled reward: -3211.53). Current reward after update: -806.61, Optimal reward -251.99
Iteration 74 took 2.69 seconds (mean sampled reward: -3158.26). Current reward after update: -566.73, Optimal reward -251.99
Iteration 75 took 2.86 seconds (mean sampled reward: -3227.73). Current reward after update: -578.23, Optimal reward -251.99
Iteration 76 took 2.89 seconds (mean sampled reward: -3270.41). Current reward after update: -1985.03, Optimal reward -251.99
Iteration 77 took 2.73 seconds (mean sampled reward: -3353.55). Current reward after update: -732.18, Optimal reward -251.99
Iteration 78 took 2.86 seconds (mean sampled reward: -3270.77). Current reward after update: -710.44, Optimal reward -251.99
Iteration 79 took 2.70 seconds (mean sampled reward: -3180.59). Current reward after update: -539.16, Optimal reward -251.99
Iteration 80 took 2.91 seconds (mean sampled reward: -3205.97). Current reward after update: -601.95, Optimal reward -251.99
Iteration 81 took 2.53 seconds (mean sampled reward: -3233.97). Current reward after update: -709.71, Optimal reward -251.99
Iteration 82 took 2.63 seconds (mean sampled reward: -3305.67). Current reward after update: -699.21, Optimal reward -251.99
Iteration 83 took 2.51 seconds (mean sampled reward: -3319.70). Current reward after update: -545.08, Optimal reward -251.99
Iteration 84 took 2.75 seconds (mean sampled reward: -3424.94). Current reward after update: -577.04, Optimal reward -251.99
Iteration 85 took 2.71 seconds (mean sampled reward: -3567.77). Current reward after update: -499.33, Optimal reward -251.99
Iteration 86 took 2.82 seconds (mean sampled reward: -3640.62). Current reward after update: -781.29, Optimal reward -251.99
Iteration 87 took 2.67 seconds (mean sampled reward: -3617.19). Current reward after update: -926.54, Optimal reward -251.99
Iteration 88 took 2.49 seconds (mean sampled reward: -3456.72). Current reward after update: -444.19, Optimal reward -251.99
Iteration 89 took 2.50 seconds (mean sampled reward: -3600.77). Current reward after update: -599.75, Optimal reward -251.99
Iteration 90 took 2.50 seconds (mean sampled reward: -3663.64). Current reward after update: -614.75, Optimal reward -251.99
Iteration 91 took 2.60 seconds (mean sampled reward: -3556.79). Current reward after update: -539.03, Optimal reward -251.99
Iteration 92 took 2.53 seconds (mean sampled reward: -3721.78). Current reward after update: -545.93, Optimal reward -251.99
Iteration 93 took 2.65 seconds (mean sampled reward: -3764.95). Current reward after update: -794.53, Optimal reward -251.99
Iteration 94 took 2.61 seconds (mean sampled reward: -3824.40). Current reward after update: -468.44, Optimal reward -251.99
Iteration 95 took 2.63 seconds (mean sampled reward: -3756.18). Current reward after update: -419.07, Optimal reward -251.99
Iteration 96 took 2.52 seconds (mean sampled reward: -3871.82). Current reward after update: -860.27, Optimal reward -251.99
Iteration 97 took 2.67 seconds (mean sampled reward: -3854.85). Current reward after update: -541.18, Optimal reward -251.99
Iteration 98 took 2.47 seconds (mean sampled reward: -3417.26). Current reward after update: -554.76, Optimal reward -251.99
Iteration 99 took 2.50 seconds (mean sampled reward: -3613.25). Current reward after update: -393.10, Optimal reward -251.99
Iteration 100 took 2.49 seconds (mean sampled reward: -3468.50). Current reward after update: -389.60, Optimal reward -251.99
Iteration 101 took 2.53 seconds (mean sampled reward: -3478.53). Current reward after update: -809.95, Optimal reward -251.99
Iteration 102 took 2.46 seconds (mean sampled reward: -3441.99). Current reward after update: -869.12, Optimal reward -251.99
Iteration 103 took 2.53 seconds (mean sampled reward: -3638.87). Current reward after update: -451.01, Optimal reward -251.99
Iteration 104 took 2.67 seconds (mean sampled reward: -3741.53). Current reward after update: -579.87, Optimal reward -251.99
Iteration 105 took 2.57 seconds (mean sampled reward: -3546.37). Current reward after update: -404.59, Optimal reward -251.99
Iteration 106 took 2.48 seconds (mean sampled reward: -3654.47). Current reward after update: -579.77, Optimal reward -251.99
Iteration 107 took 2.44 seconds (mean sampled reward: -3711.04). Current reward after update: -670.25, Optimal reward -251.99
Iteration 108 took 2.46 seconds (mean sampled reward: -3568.58). Current reward after update: -407.27, Optimal reward -251.99
Iteration 109 took 2.45 seconds (mean sampled reward: -3781.57). Current reward after update: -677.52, Optimal reward -251.99
Iteration 110 took 2.50 seconds (mean sampled reward: -3498.42). Current reward after update: -669.29, Optimal reward -251.99
Iteration 111 took 2.39 seconds (mean sampled reward: -3728.47). Current reward after update: -718.08, Optimal reward -251.99
Iteration 112 took 2.58 seconds (mean sampled reward: -3299.14). Current reward after update: -403.12, Optimal reward -251.99
Iteration 113 took 2.46 seconds (mean sampled reward: -3189.65). Current reward after update: -491.76, Optimal reward -251.99
Iteration 114 took 2.44 seconds (mean sampled reward: -3138.76). Current reward after update: -396.21, Optimal reward -251.99
Iteration 115 took 2.65 seconds (mean sampled reward: -3325.59). Current reward after update: -517.02, Optimal reward -251.99
Iteration 116 took 2.72 seconds (mean sampled reward: -3326.53). Current reward after update: -372.82, Optimal reward -251.99
Iteration 117 took 2.58 seconds (mean sampled reward: -3443.43). Current reward after update: -397.38, Optimal reward -251.99
Iteration 118 took 2.47 seconds (mean sampled reward: -3392.09). Current reward after update: -498.73, Optimal reward -251.99
Iteration 119 took 2.60 seconds (mean sampled reward: -3618.71). Current reward after update: -398.26, Optimal reward -251.99
Iteration 120 took 2.80 seconds (mean sampled reward: -3372.33). Current reward after update: -422.10, Optimal reward -251.99
Iteration 121 took 2.48 seconds (mean sampled reward: -3237.59). Current reward after update: -385.72, Optimal reward -251.99
Iteration 122 took 2.49 seconds (mean sampled reward: -3388.19). Current reward after update: -431.46, Optimal reward -251.99
Iteration 123 took 2.57 seconds (mean sampled reward: -3713.69). Current reward after update: -397.59, Optimal reward -251.99
Iteration 124 took 2.68 seconds (mean sampled reward: -3608.31). Current reward after update: -420.07, Optimal reward -251.99
Iteration 125 took 2.72 seconds (mean sampled reward: -3490.91). Current reward after update: -351.13, Optimal reward -251.99
Iteration 126 took 2.52 seconds (mean sampled reward: -3335.53). Current reward after update: -317.33, Optimal reward -251.99
Iteration 127 took 2.48 seconds (mean sampled reward: -3484.72). Current reward after update: -604.17, Optimal reward -251.99
Iteration 128 took 2.60 seconds (mean sampled reward: -3262.11). Current reward after update: -341.95, Optimal reward -251.99
Iteration 129 took 2.61 seconds (mean sampled reward: -3363.38). Current reward after update: -1455.42, Optimal reward -251.99
Iteration 130 took 2.67 seconds (mean sampled reward: -3480.64). Current reward after update: -508.87, Optimal reward -251.99
Iteration 131 took 2.60 seconds (mean sampled reward: -3397.32). Current reward after update: -396.59, Optimal reward -251.99
Iteration 132 took 2.59 seconds (mean sampled reward: -3317.27). Current reward after update: -352.97, Optimal reward -251.99
Iteration 133 took 2.52 seconds (mean sampled reward: -3411.56). Current reward after update: -378.88, Optimal reward -251.99
Iteration 134 took 2.78 seconds (mean sampled reward: -3402.94). Current reward after update: -347.41, Optimal reward -251.99
Iteration 135 took 2.44 seconds (mean sampled reward: -3571.64). Current reward after update: -341.91, Optimal reward -251.99
Iteration 136 took 2.49 seconds (mean sampled reward: -3532.84). Current reward after update: -448.26, Optimal reward -251.99
Iteration 137 took 2.44 seconds (mean sampled reward: -3223.74). Current reward after update: -461.53, Optimal reward -251.99
Iteration 138 took 2.55 seconds (mean sampled reward: -3340.21). Current reward after update: -340.64, Optimal reward -251.99
Iteration 139 took 2.48 seconds (mean sampled reward: -3397.33). Current reward after update: -452.90, Optimal reward -251.99
Iteration 140 took 2.50 seconds (mean sampled reward: -3343.45). Current reward after update: -385.19, Optimal reward -251.99
Iteration 141 took 2.49 seconds (mean sampled reward: -3222.69). Current reward after update: -367.74, Optimal reward -251.99
Iteration 142 took 2.56 seconds (mean sampled reward: -3289.94). Current reward after update: -302.73, Optimal reward -251.99
Iteration 143 took 2.52 seconds (mean sampled reward: -3444.19). Current reward after update: -838.38, Optimal reward -251.99
Iteration 144 took 2.42 seconds (mean sampled reward: -3548.21). Current reward after update: -336.35, Optimal reward -251.99
Iteration 145 took 2.45 seconds (mean sampled reward: -3478.60). Current reward after update: -333.00, Optimal reward -251.99
Iteration 146 took 2.39 seconds (mean sampled reward: -3539.77). Current reward after update: -357.19, Optimal reward -251.99
Iteration 147 took 2.49 seconds (mean sampled reward: -3434.72). Current reward after update: -315.60, Optimal reward -251.99
Iteration 148 took 2.62 seconds (mean sampled reward: -3352.31). Current reward after update: -306.59, Optimal reward -251.99
Iteration 149 took 2.67 seconds (mean sampled reward: -3499.29). Current reward after update: -367.12, Optimal reward -251.99
Iteration 150 took 2.51 seconds (mean sampled reward: -3496.38). Current reward after update: -466.26, Optimal reward -251.99
Iteration 151 took 2.50 seconds (mean sampled reward: -3462.22). Current reward after update: -335.51, Optimal reward -251.99
Iteration 152 took 2.46 seconds (mean sampled reward: -3671.93). Current reward after update: -374.41, Optimal reward -251.99
Iteration 153 took 2.42 seconds (mean sampled reward: -3541.11). Current reward after update: -377.89, Optimal reward -251.99
Iteration 154 took 2.39 seconds (mean sampled reward: -3556.94). Current reward after update: -346.47, Optimal reward -251.99
Iteration 155 took 2.51 seconds (mean sampled reward: -3505.09). Current reward after update: -274.47, Optimal reward -251.99
Iteration 156 took 2.67 seconds (mean sampled reward: -3548.97). Current reward after update: -366.00, Optimal reward -251.99
Iteration 157 took 2.44 seconds (mean sampled reward: -3574.46). Current reward after update: -363.84, Optimal reward -251.99
Iteration 158 took 2.39 seconds (mean sampled reward: -3553.78). Current reward after update: -367.41, Optimal reward -251.99
Iteration 159 took 2.44 seconds (mean sampled reward: -3546.24). Current reward after update: -348.90, Optimal reward -251.99
Iteration 160 took 2.44 seconds (mean sampled reward: -3624.23). Current reward after update: -354.21, Optimal reward -251.99
Iteration 161 took 2.61 seconds (mean sampled reward: -3538.65). Current reward after update: -437.71, Optimal reward -251.99
Iteration 162 took 2.49 seconds (mean sampled reward: -3557.54). Current reward after update: -289.59, Optimal reward -251.99
Iteration 163 took 2.43 seconds (mean sampled reward: -3573.71). Current reward after update: -344.60, Optimal reward -251.99
Iteration 164 took 2.49 seconds (mean sampled reward: -3598.27). Current reward after update: -266.44, Optimal reward -251.99
Iteration 165 took 2.45 seconds (mean sampled reward: -3524.53). Current reward after update: -316.89, Optimal reward -251.99
Iteration 166 took 2.41 seconds (mean sampled reward: -3585.64). Current reward after update: -331.21, Optimal reward -251.99
Iteration 167 took 2.46 seconds (mean sampled reward: -3598.46). Current reward after update: -281.31, Optimal reward -251.99
Iteration 168 took 2.43 seconds (mean sampled reward: -3516.16). Current reward after update: -337.54, Optimal reward -251.99
Iteration 169 took 2.36 seconds (mean sampled reward: -3557.03). Current reward after update: -314.29, Optimal reward -251.99
Iteration 170 took 2.72 seconds (mean sampled reward: -3522.96). Current reward after update: -991.37, Optimal reward -251.99
Iteration 171 took 2.42 seconds (mean sampled reward: -3566.42). Current reward after update: -331.60, Optimal reward -251.99
Iteration 172 took 2.42 seconds (mean sampled reward: -3519.35). Current reward after update: -2297.30, Optimal reward -251.99
Iteration 173 took 2.43 seconds (mean sampled reward: -3587.79). Current reward after update: -579.47, Optimal reward -251.99
Iteration 174 took 2.70 seconds (mean sampled reward: -3639.90). Current reward after update: -328.51, Optimal reward -251.99
Iteration 175 took 2.54 seconds (mean sampled reward: -3608.04). Current reward after update: -319.92, Optimal reward -251.99
Iteration 176 took 2.40 seconds (mean sampled reward: -3583.45). Current reward after update: -329.42, Optimal reward -251.99
Iteration 177 took 2.50 seconds (mean sampled reward: -3689.52). Current reward after update: -340.59, Optimal reward -251.99
Iteration 178 took 2.41 seconds (mean sampled reward: -3571.46). Current reward after update: -387.54, Optimal reward -251.99
Iteration 179 took 2.44 seconds (mean sampled reward: -3610.22). Current reward after update: -356.13, Optimal reward -251.99
Iteration 180 took 2.50 seconds (mean sampled reward: -3330.21). Current reward after update: -362.18, Optimal reward -251.99
Iteration 181 took 2.47 seconds (mean sampled reward: -3577.91). Current reward after update: -340.44, Optimal reward -251.99
Iteration 182 took 2.42 seconds (mean sampled reward: -3537.85). Current reward after update: -338.76, Optimal reward -251.99
Iteration 183 took 2.48 seconds (mean sampled reward: -3552.13). Current reward after update: -299.59, Optimal reward -251.99
Iteration 184 took 2.60 seconds (mean sampled reward: -3712.04). Current reward after update: -316.58, Optimal reward -251.99
Iteration 185 took 2.42 seconds (mean sampled reward: -3680.41). Current reward after update: -940.14, Optimal reward -251.99
Iteration 186 took 2.47 seconds (mean sampled reward: -3801.92). Current reward after update: -330.46, Optimal reward -251.99
Iteration 187 took 2.49 seconds (mean sampled reward: -3575.84). Current reward after update: -301.92, Optimal reward -251.99
Iteration 188 took 2.62 seconds (mean sampled reward: -3581.33). Current reward after update: -362.74, Optimal reward -251.99
Iteration 189 took 2.55 seconds (mean sampled reward: -3562.58). Current reward after update: -356.86, Optimal reward -251.99
Iteration 190 took 2.50 seconds (mean sampled reward: -3555.24). Current reward after update: -380.83, Optimal reward -251.99
Iteration 191 took 2.41 seconds (mean sampled reward: -3457.95). Current reward after update: -401.35, Optimal reward -251.99
Iteration 192 took 2.43 seconds (mean sampled reward: -3492.83). Current reward after update: -352.16, Optimal reward -251.99
Iteration 193 took 2.41 seconds (mean sampled reward: -3498.52). Current reward after update: -315.98, Optimal reward -251.99
Iteration 194 took 2.61 seconds (mean sampled reward: -3603.00). Current reward after update: -338.61, Optimal reward -251.99
Iteration 195 took 2.48 seconds (mean sampled reward: -3634.79). Current reward after update: -402.90, Optimal reward -251.99
Iteration 196 took 2.47 seconds (mean sampled reward: -3620.53). Current reward after update: -325.33, Optimal reward -251.99
Iteration 197 took 2.41 seconds (mean sampled reward: -3719.53). Current reward after update: -341.66, Optimal reward -251.99
Iteration 198 took 2.46 seconds (mean sampled reward: -3734.10). Current reward after update: -337.79, Optimal reward -251.99
Iteration 199 took 2.42 seconds (mean sampled reward: -3755.30). Current reward after update: -320.13, Optimal reward -251.99
Iteration 200 took 2.51 seconds (mean sampled reward: -3768.95). Current reward after update: -365.81, Optimal reward -251.99
Iteration 201 took 2.61 seconds (mean sampled reward: -3790.72). Current reward after update: -358.53, Optimal reward -251.99
Iteration 202 took 2.52 seconds (mean sampled reward: -3713.55). Current reward after update: -330.19, Optimal reward -251.99
Iteration 203 took 2.53 seconds (mean sampled reward: -3738.29). Current reward after update: -385.16, Optimal reward -251.99
Iteration 204 took 2.55 seconds (mean sampled reward: -3583.89). Current reward after update: -300.88, Optimal reward -251.99
Iteration 205 took 2.53 seconds (mean sampled reward: -3403.86). Current reward after update: -394.43, Optimal reward -251.99
Iteration 206 took 2.64 seconds (mean sampled reward: -3449.67). Current reward after update: -390.83, Optimal reward -251.99
Iteration 207 took 2.85 seconds (mean sampled reward: -3538.92). Current reward after update: -394.80, Optimal reward -251.99
Iteration 208 took 2.74 seconds (mean sampled reward: -3723.32). Current reward after update: -575.47, Optimal reward -251.99
Iteration 209 took 2.66 seconds (mean sampled reward: -3563.44). Current reward after update: -409.77, Optimal reward -251.99
Iteration 210 took 2.61 seconds (mean sampled reward: -3668.95). Current reward after update: -415.53, Optimal reward -251.99
Iteration 211 took 2.73 seconds (mean sampled reward: -3499.18). Current reward after update: -388.90, Optimal reward -251.99
Iteration 212 took 2.43 seconds (mean sampled reward: -3423.45). Current reward after update: -356.94, Optimal reward -251.99
Iteration 213 took 2.47 seconds (mean sampled reward: -3314.73). Current reward after update: -361.55, Optimal reward -251.99
Iteration 214 took 2.60 seconds (mean sampled reward: -3469.04). Current reward after update: -508.68, Optimal reward -251.99
Iteration 215 took 2.73 seconds (mean sampled reward: -3354.23). Current reward after update: -414.72, Optimal reward -251.99
Iteration 216 took 2.56 seconds (mean sampled reward: -3450.37). Current reward after update: -348.47, Optimal reward -251.99
Iteration 217 took 2.50 seconds (mean sampled reward: -3443.56). Current reward after update: -412.30, Optimal reward -251.99
Iteration 218 took 2.40 seconds (mean sampled reward: -3420.73). Current reward after update: -379.24, Optimal reward -251.99
Iteration 219 took 2.42 seconds (mean sampled reward: -3483.96). Current reward after update: -341.70, Optimal reward -251.99
Iteration 220 took 2.42 seconds (mean sampled reward: -3461.98). Current reward after update: -396.15, Optimal reward -251.99
Iteration 221 took 2.52 seconds (mean sampled reward: -3425.55). Current reward after update: -420.28, Optimal reward -251.99
Iteration 222 took 2.48 seconds (mean sampled reward: -3345.72). Current reward after update: -366.23, Optimal reward -251.99
Iteration 223 took 2.42 seconds (mean sampled reward: -3436.36). Current reward after update: -428.83, Optimal reward -251.99
Iteration 224 took 2.66 seconds (mean sampled reward: -3355.53). Current reward after update: -441.48, Optimal reward -251.99
Iteration 225 took 2.59 seconds (mean sampled reward: -3294.76). Current reward after update: -405.13, Optimal reward -251.99
Iteration 226 took 2.55 seconds (mean sampled reward: -3398.15). Current reward after update: -392.73, Optimal reward -251.99
Iteration 227 took 2.67 seconds (mean sampled reward: -3392.12). Current reward after update: -357.69, Optimal reward -251.99
Iteration 228 took 2.56 seconds (mean sampled reward: -3447.95). Current reward after update: -399.51, Optimal reward -251.99
Iteration 229 took 2.61 seconds (mean sampled reward: -3625.40). Current reward after update: -319.33, Optimal reward -251.99
Iteration 230 took 2.68 seconds (mean sampled reward: -3398.31). Current reward after update: -348.18, Optimal reward -251.99
Iteration 231 took 2.65 seconds (mean sampled reward: -3650.29). Current reward after update: -340.01, Optimal reward -251.99
Iteration 232 took 2.62 seconds (mean sampled reward: -3606.47). Current reward after update: -600.38, Optimal reward -251.99
Iteration 233 took 2.59 seconds (mean sampled reward: -3426.37). Current reward after update: -663.25, Optimal reward -251.99
Iteration 234 took 2.77 seconds (mean sampled reward: -3479.13). Current reward after update: -365.54, Optimal reward -251.99
Iteration 235 took 2.54 seconds (mean sampled reward: -3397.44). Current reward after update: -327.10, Optimal reward -251.99
Iteration 236 took 2.51 seconds (mean sampled reward: -3356.65). Current reward after update: -425.64, Optimal reward -251.99
Iteration 237 took 2.63 seconds (mean sampled reward: -3557.15). Current reward after update: -514.28, Optimal reward -251.99
Iteration 238 took 2.66 seconds (mean sampled reward: -3813.14). Current reward after update: -880.44, Optimal reward -251.99
Iteration 239 took 2.57 seconds (mean sampled reward: -3540.07). Current reward after update: -326.84, Optimal reward -251.99
Iteration 240 took 2.51 seconds (mean sampled reward: -3583.78). Current reward after update: -493.46, Optimal reward -251.99
Iteration 241 took 2.52 seconds (mean sampled reward: -3587.18). Current reward after update: -419.41, Optimal reward -251.99
Iteration 242 took 2.50 seconds (mean sampled reward: -3521.65). Current reward after update: -332.59, Optimal reward -251.99
Iteration 243 took 2.94 seconds (mean sampled reward: -3823.34). Current reward after update: -487.42, Optimal reward -251.99
Iteration 244 took 2.77 seconds (mean sampled reward: -3772.85). Current reward after update: -356.95, Optimal reward -251.99
Iteration 245 took 2.59 seconds (mean sampled reward: -3913.58). Current reward after update: -435.05, Optimal reward -251.99
Iteration 246 took 2.88 seconds (mean sampled reward: -3770.78). Current reward after update: -391.01, Optimal reward -251.99
Iteration 247 took 2.58 seconds (mean sampled reward: -3705.07). Current reward after update: -410.23, Optimal reward -251.99
Iteration 248 took 2.61 seconds (mean sampled reward: -3619.67). Current reward after update: -429.03, Optimal reward -251.99
Iteration 249 took 2.74 seconds (mean sampled reward: -3725.18). Current reward after update: -359.25, Optimal reward -251.99
Iteration 250 took 2.62 seconds (mean sampled reward: -3678.28). Current reward after update: -398.08, Optimal reward -251.99
Iteration 251 took 2.62 seconds (mean sampled reward: -3680.35). Current reward after update: -1406.54, Optimal reward -251.99
Iteration 252 took 2.71 seconds (mean sampled reward: -3695.14). Current reward after update: -401.59, Optimal reward -251.99
Iteration 253 took 2.52 seconds (mean sampled reward: -3579.34). Current reward after update: -433.70, Optimal reward -251.99
Iteration 254 took 2.53 seconds (mean sampled reward: -3606.05). Current reward after update: -397.25, Optimal reward -251.99
Iteration 255 took 2.69 seconds (mean sampled reward: -3609.82). Current reward after update: -412.79, Optimal reward -251.99
Iteration 256 took 2.50 seconds (mean sampled reward: -3574.55). Current reward after update: -322.58, Optimal reward -251.99
Iteration 257 took 2.59 seconds (mean sampled reward: -3542.35). Current reward after update: -589.42, Optimal reward -251.99
Iteration 258 took 2.55 seconds (mean sampled reward: -3507.88). Current reward after update: -340.04, Optimal reward -251.99
Iteration 259 took 2.58 seconds (mean sampled reward: -3479.32). Current reward after update: -1639.27, Optimal reward -251.99
Iteration 260 took 2.45 seconds (mean sampled reward: -3459.04). Current reward after update: -377.23, Optimal reward -251.99
Iteration 261 took 2.72 seconds (mean sampled reward: -3450.88). Current reward after update: -347.11, Optimal reward -251.99
Iteration 262 took 2.49 seconds (mean sampled reward: -3434.94). Current reward after update: -389.95, Optimal reward -251.99
Iteration 263 took 2.55 seconds (mean sampled reward: -3364.88). Current reward after update: -354.22, Optimal reward -251.99
Iteration 264 took 2.45 seconds (mean sampled reward: -3484.35). Current reward after update: -371.27, Optimal reward -251.99
Iteration 265 took 2.53 seconds (mean sampled reward: -3487.59). Current reward after update: -355.43, Optimal reward -251.99
Iteration 266 took 2.64 seconds (mean sampled reward: -3487.32). Current reward after update: -297.25, Optimal reward -251.99
Iteration 267 took 2.46 seconds (mean sampled reward: -3500.05). Current reward after update: -390.83, Optimal reward -251.99
Iteration 268 took 2.47 seconds (mean sampled reward: -3413.22). Current reward after update: -356.72, Optimal reward -251.99
Iteration 269 took 2.45 seconds (mean sampled reward: -3447.06). Current reward after update: -363.02, Optimal reward -251.99
Iteration 270 took 2.48 seconds (mean sampled reward: -3366.41). Current reward after update: -511.24, Optimal reward -251.99
Iteration 271 took 2.48 seconds (mean sampled reward: -3331.15). Current reward after update: -408.14, Optimal reward -251.99
Iteration 272 took 2.48 seconds (mean sampled reward: -3340.65). Current reward after update: -485.38, Optimal reward -251.99
Iteration 273 took 2.79 seconds (mean sampled reward: -3307.26). Current reward after update: -495.84, Optimal reward -251.99
Iteration 274 took 2.49 seconds (mean sampled reward: -3374.67). Current reward after update: -497.48, Optimal reward -251.99
Iteration 275 took 2.49 seconds (mean sampled reward: -3388.67). Current reward after update: -387.97, Optimal reward -251.99
Iteration 276 took 2.62 seconds (mean sampled reward: -3363.58). Current reward after update: -391.87, Optimal reward -251.99
Iteration 277 took 2.66 seconds (mean sampled reward: -3294.00). Current reward after update: -373.64, Optimal reward -251.99
Iteration 278 took 2.74 seconds (mean sampled reward: -3251.18). Current reward after update: -441.10, Optimal reward -251.99
Iteration 279 took 2.46 seconds (mean sampled reward: -3371.09). Current reward after update: -333.89, Optimal reward -251.99
Iteration 280 took 2.48 seconds (mean sampled reward: -3371.96). Current reward after update: -472.52, Optimal reward -251.99
Iteration 281 took 2.59 seconds (mean sampled reward: -3295.47). Current reward after update: -462.13, Optimal reward -251.99
Iteration 282 took 2.52 seconds (mean sampled reward: -3388.95). Current reward after update: -425.99, Optimal reward -251.99
Iteration 283 took 2.45 seconds (mean sampled reward: -3375.17). Current reward after update: -415.31, Optimal reward -251.99
Iteration 284 took 2.45 seconds (mean sampled reward: -3427.17). Current reward after update: -486.92, Optimal reward -251.99
Iteration 285 took 2.64 seconds (mean sampled reward: -3411.12). Current reward after update: -705.75, Optimal reward -251.99
Iteration 286 took 2.71 seconds (mean sampled reward: -3420.50). Current reward after update: -380.75, Optimal reward -251.99
Iteration 287 took 2.73 seconds (mean sampled reward: -3461.46). Current reward after update: -342.82, Optimal reward -251.99
Iteration 288 took 2.49 seconds (mean sampled reward: -3471.58). Current reward after update: -410.97, Optimal reward -251.99
Iteration 289 took 2.45 seconds (mean sampled reward: -3389.33). Current reward after update: -423.43, Optimal reward -251.99
Iteration 290 took 2.61 seconds (mean sampled reward: -3458.42). Current reward after update: -435.55, Optimal reward -251.99
Iteration 291 took 2.78 seconds (mean sampled reward: -3460.78). Current reward after update: -393.25, Optimal reward -251.99
Iteration 292 took 2.81 seconds (mean sampled reward: -3449.56). Current reward after update: -403.62, Optimal reward -251.99
Iteration 293 took 2.59 seconds (mean sampled reward: -3426.43). Current reward after update: -355.21, Optimal reward -251.99
Iteration 294 took 2.50 seconds (mean sampled reward: -3426.87). Current reward after update: -1036.87, Optimal reward -251.99
Iteration 295 took 2.79 seconds (mean sampled reward: -3386.51). Current reward after update: -423.57, Optimal reward -251.99
Iteration 296 took 2.59 seconds (mean sampled reward: -3394.51). Current reward after update: -382.04, Optimal reward -251.99
Iteration 297 took 2.53 seconds (mean sampled reward: -3363.45). Current reward after update: -405.40, Optimal reward -251.99
Iteration 298 took 2.60 seconds (mean sampled reward: -3298.43). Current reward after update: -411.37, Optimal reward -251.99
Iteration 299 took 2.75 seconds (mean sampled reward: -3311.34). Current reward after update: -403.75, Optimal reward -251.99
Iteration 300 took 2.64 seconds (mean sampled reward: -3228.18). Current reward after update: -418.37, Optimal reward -251.99
Sigma: 0.8 mean rewards: -595.9419868952676, best rewards:-251.98774969218493

argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
